Pattern Recognition and Classiﬁcation for Multivariate Time Series  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee.

life multivariate sensor signals which give information about the progression of motor parameters during car drive. In case of motor signals, a time series segment could represent a critical situation that appeared during acceleration or loss of speed. Figure 1 illustrates a test drive in which speed and accelerator angle were recorded.
Our main interest is the recognition of patterns, or rather situations, in sensor data recorded during a car drive. To identify recurring situations we need to group the identied segments according their similarity. For the grouping of time series segments we propose agglomerative hierarchical clustering, which has the distinct advantage that any valid measure of distance or similarity can be used. Although the proposed time series segmentation and segment clustering is discussed in terms of vehicular sensors, it is also applicable to time series data of other domains.
The rest of the paper is organized as follows. Section 2 gives a general introduction to time series segmentation and furthermore discusses several di erent segmentation techniques. Section 3 describes the automatic classi cation of identi ed time series segments. The experimental results of both time series segmentation and clustering are presented in Section 4. Related work can be found in Section 5, and nally Section 6 concludes this paper.

2. TIME SERIES SEGMENTATION Segmentation is the most frequently used subroutine in both clustering and classi cation of time series. It is often used to locate stable periods of time or alternatively to identify change points [2]. There already exist di erent heuristic approaches to extract internally homogeneous segments. Some primitive algorithms search for in ection points to locate episodes [13]. Other algorithms determine segment borders by the Sliding Window technique, were a segment is grown until it exceeds some error bound [6]. The contributed approach uses the bottom-up method, which begins creating a ne approximation of the time series, and iteratively merges the lowest cost pair of segments until some stopping criteria is met [6]. The cost of merging two adjacent segments is eval uated by the Singular Value Decomposition (SVD) model of the new segment. SVD-based algorithms are able to detect changes in the mean, variance and correlation structure among several variables. The proposed approach can be considered as an extension of the sensor fusion algorithm developed by Abonyi [2]. In the following, we describe the mathematical foundations of time series segmentation in more detail.
For further problem discussion we de ne a multivariate time series T = fxk = [x1;k; x2;k; : : : ; xm;k]j1 k N g as a nite set of N samples with m measurements labelled by time points t1; : : : ; tN [2]. Consequently, a segment of T is dened as a set of consecutive time points S(a; b) = a k b, xa; xa+1; : : : ; xb. The segmentation of time series T into c non-overlapping time intervals can thus be formulated as STc = fSe(ae; be)j1 e cg, were a1 = 1, bc = N and ae = be 1 + 1.
For the purpose of nding internally homogeneous segments from a given time series, we need to formalize the cost function for the individual time intervals. In most cases, the cost function cost(S(a; b)) is based on the distance between the actual values of the time series and a simple function (linear function, or polynome of higher degree) tted to the data of each segment [2].
Since our approach aims at detecting changes in the correlation structure among several variables, the cost function of the segmentation is based on the Singular Value Decomposition of the segment matrices, where each row is an observation, and each column is a variable. Before applying SVD, the observed variables need to be centred and scaled, in such a way as to make them comparable. Hence, we compute the z-scores using mean and standard deviation along each individual variable of the time series.
The singular value decomposition of a segment matrix X can be described as an factorization X U V T into a matrix which includes the singular values of X in its diagonal in decreasing order, and into the matrices U and V which include the corresponding left-singular and right-singular vectors. With the use of the rst few non-zero singular val ues and the corresponding singular vectors, the SVD model projects the correlated high-dimensional data onto a hyperplane which is useful for the analysis of multivariate data.
When the SVD model has an adequate number of dimensions, the distance of the original data from the hyperplane is a signi cant indicator for anomalies or unsteadiness in the progression of the observed variables. Hence, it is useful to analyse the reconstruction error of the SVD model to get information about the homogeneity of the factorized time series segments. In the proposed approach the reconstruction error is determined by the Q-measure [2], which is commonly used for the monitoring of multivariate systems and for the exploration of the errors. The Q-measure is dened as the mean squared deviation of projection P , which transfers our original segment data X into the p-dimensional subspace given by the Singular Value Decomposition. Due to the fact that we are mainly interested in the correlation structure among the observed variables, the projection P just considers the singular-vectors given by factor matrix V and neglect the hidden structure among the observations  held in factor matrix U . The crux of the proposed time series segmentation is to use the Q-measure as an indicator for the homogeneity of individual or rather merged segments.
Equation 1 to 3 formulate the derivation of the cost function:  The introduced bottom-up segmentation approach iteratively merges the cheapest pair of adjacent segments until a predetermined number of segments is reached or a speci c thresh old is exceeded. By the time the proposed bottom-up algorithm terminates we expect that an optimal time series segmentation was found. Figure 2 illustrates the growth of the reconstruction error, when two contiguous segments are merged. One possible way to determine the merge threshold can be inferred from Figure 5(a).
The accuracy of the SVD model strongly depends on the rank of the decomposition. However, in most cases it is hard to determine the optimal rank in terms of model accuracy and computational complexity. Abonyi et al. [2] suggest to infer an appropriate rank from the scree-plot of the eigenvalues. Since this method is very imprecise and only works for variables with the same scale, we suggest to choose the model rank according the desired accuracy. In other words, we consider the k-largest singular values of matrix n n that hold the required energy [Ek = (sumik=1 i;i)=(sumin=1 i;i)].
Note, that the calculation of the required model rank is based on the size of the nal coarse segmentation. This derives from the fact that the approximation of large segments usually requires a high model rank, which most certainly also ts smaller segments.
The sensor fusion algorithm developed by Abonyi [2] merges segments bottom-up, whereas the initial ne-grain segmentation of the time series is determined manually. In case of a periodic signal and an odd initial segmentation it is highly probable that the bottom-up algorithm will not nd reappearing segments, because the time series was partitioned into internal inhomogeneous time intervals. Therefore, we propose to perform the initial ne-grain segmentation of a time series according to the critical points of the individual  signals. Critical points of great interest are extrema as well as in ection and saddle points. In order to calculate the critical points of a curve or signal we need to calculate the rst, second or third derivative respectively.
Due to the fact that sensors often produce extremely noisy and uctuate signals, it is very likely that the examined time series exhibit dozens of critical points. Hence, we need to smooth the signal with a low-pass or base-band-pass lter that reduces the number of critical points, but this does not substantially change the coordinates of the retrieved segmentation borders (refer to Figure 3).
In our critical point (CP) approach, we employ the SavitzkyGolay lter, which is typically used to smooth out a noisy signal whose frequency span is large. Savitzky-Golay smoothing lters perform much better than standard averaging FIR (Finite Impulse Respond) lters, which tend to lter out a signi cant portion of the signal's high frequency content along with the noise. Although Savitzky-Golay lters are more e ective at preserving the pertinent high frequency components of the signal, they are less successful than standard averaging FIR lters at rejecting noise. Savitzky-Golay lters are optimal in the sense that they minimize the leastsquares error in tting a higher-order polynomial to frames of noisy data [9].
The introduced critical point (CP) approach can be employed as a preprocessing step for the proposed bottom-up segmentation (BU), but can also be considered as a segmentation technique by its own. In Section 4 we evaluate the mixed approach (BUCP) as well as both segmentation techniques separately.

3. PATTERN CLASSIFICATION One major advantage of the proposed segmentation algorithm is that it allows one to reuse the SVD model to de ne a distance measure to compare multivariate time series segments [17,18]. Consider two segments Si and Sj with data matrices Xi and Xj that hold the same n variables, and whose singular value decomposition transforms the original data into a k-dimensional subspace, denoted by Ui;k i;kViT;k and Uj;k j;kVjT;k respectively. Then the similarity between these subspaces can be de ned as the sum of the squares of the cosines of the angles between each vector of matrix Wi;k and Wj;k, which are composed by multiplying the respective singular values and right-singular vectors (Wi;k = i;kVi;k and Wj;k = j;kVj;k). This similarity measure is formalized by the following equation:  Due to the fact that subspaces Wi and Wj contain the k most important singular vectors that account for most of the variance in their corresponding data set, SSV D is also a measure of similarity between the segments Si and Sj.
Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense. It is a method of unsupervised learning, and a common technique for statistical data analysis used in elds like machine learning, data mining and information retrieval. Hierarchical methods nd successive clusters using previously established ones. Agglomerative hierarchical clustering algorithms begin with each element as a separate cluster and merge them into successively larger clusters. The merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented as a dendrogram. Figure 4 illustrates the dendrogram for a sample of time series segments.
Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required; all that is used is a matrix of distances. The choice of an appropriate metric does in uence the shape of the clusters, as some elements  may be close to one another according to one distance and farther away according to another. Some commonly used metrics for hierarchical clustering are Euclidean distance, cosine similarity, Mahalanobis distance (covariance matrix) and Pearson's correlation. For text or other non-numeric data, metrics such as the Hamming distance or Levenshtein distance are often used. Given an appropriate metric, an agglomerative hierarchical cluster tree can be created by several di erent methods, which di er from one another in how they measure the distance between the clusters. Available methods are single and complete linkage, also known as shortest and furthest distance, as well as average, centroid and inner-squared distance. In our proposed approach we employ cosine distance (refer to Equation 5) to calculate the similarities between feature vectors and average linkage to compute average distance between all pairs of objects in any two clusters (see Equation 4).

Although distance metric and cluster measure directly in uence the grouping of items or rather segments, meaningful clustering always depends on the selection of expressive features. In machine learning and statistics, feature selection is the technique of selecting a subset of relevant characteristics for building robust learning models. From a theoretical perspective, it can be shown that optimal feature selection for (un)-supervised learning problems requires an exhaustive search of all possible subsets of features. For practical learning algorithms, the search is for a satisfactory set of features instead of an optimal set. Beside the previously introduced feature extraction via SVD, we furthermore examine common signal processing features like standard deviation, local maxima and minima as well as the increase or decrease of the individual signals within a segment. The evaluation of the proposed time series segmentation and segment clustering is discussed in the following section.

(b) Performance of Algorithms (lower Q-measure is better)  4. EVALUATION In supervised learning, one usually has a training data set of input and target values which is used to adjust the parameters of the learning algorithm until it is able to model the underlying structure of the presented data. The trained model can then be used to predict target values for previously unseen input. However, in unsupervised learning one aims to discover the underlying structure of the presented data without having any target values for validation.
In case of our time series segmentation, we do not know what makes up a \good" segment and at which point of our bottom-up algorithm we should stop to merge adjacent segments. Nonetheless we are able to determine a merge threshold by means of the applied cost function. Figure 5(a) illustrates the growth of the Q-measure (sum over all segments) for each iteration of our bottom-up algorithm. A possible termination criteria for merging time series segments could be a steep ascent of the cost function. Beside automatically determining the number of segments (merge threshold), we furthermore compare di erent approaches of time series seg mentation by evaluating the overall cost of the established segments. Figure 5(b) shows a comparison of our proposed bottom-up segmentation (BU), the introduced critical point (CP) approach and a mixed BUCP segmentation for several di erent car drives.
Clustering falls into the category of unsupervised learning algorithm as well, and therefore exhibits the common problem of validation. Similar to the segmentation task we do not know for which threshold we should stop to merge single items or rather clusters. However, we believe that it is possible to determine the number of clusters based on the internal homogeneity and external heterogeneity. In other words, similar to community detection in complex networks [7], we want to nd groups of nodes or items that are more densely connected internally then with the rest of the network. Due to the fact that we employ average linkage as cluster measure, both internal and external item distance grow monotonically with an increasing number of clusters.
For the evaluation we take account of the external cluster distance, which can be retrieved from the hierarchical clus ter tree generated during segment grouping (also refer to the dendrogram in Figure 4). The cost function for several different sample datasets and the determination of a reasonable number of clusters can be inferred from Figure 6. However, cluster quantity always depends on the underlying structure of the time series and might vary from dataset to dataset.
For a better understanding of time series segmentation and clustering, we present an evaluated time series dataset in Figure 7. As mentioned previously, we are mainly interested in the recognition of complex drive manoeuvres in sensor data recorded from vehicles. To identify complex drive manoeuvres, we rst of all employ time series segmentation and clustering as introduced before, and subsequently retrieve recurring patterns of segment sequences. In our case, sequences are regarded as sets of grouped segments, which exhibit a distinct order of cluster labels. If the determined time series segments are considered as individual situations in car driving, then segment sequences can be viewed as complex drive manoeuvres, like stop and go at a tra c light.
Figure 7 illustrates several recurring sequences, which denote complex drive manoeuvres of a prede ned length.
Drive manoeuvres might di er from person to person, and also depend on the motorization of the vehicles. Therefore it is interesting to analyse the patterns that characterize individuals or vehicles respectively. In our future work, we plan to identify sequences that describe normal drives or rather drivers, as well as sequences that can be regarded as critical drive manoeuvres or abnormalities. This knowledge is especially valuable for the motor vehicle industry, because it allows for performance optimization of an engine based on the understanding of frequent or abnormal situations. Although this paper discusses context recognition in terms of sensor data recorded from vehicles, the proposed time series analysis is also applicable to datasets from other domains.
The following section gives some examples of related work.

5. RELATED WORK The sensor fusion algorithm developed by Abonyi et al. [2] is able to segment a multivariate time series in a bottom-up manner. Adjacent segments are merged iteratively if their  combined model does not exceed a prede ned reconstruction error, which is computed by means of principal component analysis. The PCA model is used to measure the homogeneity of the segment according the change of correlation among the variables. Since the PCA model de nes linear hyperplanes, the proposed segmentation algorithm can be considered as the multivariate extension of piecewise linear approximation. Our own work can be considered as an extension of the sensor fusion algorithm and has been evaluated on several real-life datasets.
In a following paper [1] Abonyi introduces a modi ed time series segmentation approach, which is able to group overlapping and vague segments by means of fuzzy clustering.
The proposed algorithm favours contiguous clusters in time and is capable to detect changes in the hidden structure of multivariate time-series, where local probabilistic PCA models are used to represent the segments. The evaluation results suggest that the proposed approach can be applied to extract useful information from temporal databases. In our future work we plan to employ fuzzy clustering to model overlapping situation in car driving.
A group of researchers from the University of California (Keogh et al. [6]) reviewed the three major segmentation approaches in the literature (Sliding Windows, Top-Down & Bottom-Up) and provided an extensive empirical evaluation on a heterogeneous collection of datasets. In order to evaluate the segmentation algorithms, all datasets are represented as piecewise linear approximation (PLA), which refers to an approximation of a time series. Among all segmentation algorithms the bottom-up approach performed best, which corresponds to our own evaluation. Furthermore, they introduce a novel on-line algorithm for segmenting time series that scales linearly to the size of the data and produces high quality approximations.
In a survey about time series abstraction methods, Hoeppner [5] discusses data representations that have been used in literature. The author states that an abstracted data representation is necessary to account for the human way of perceiving time series. Moreover, the paper suggests to use multi-scale methods or rather multi-resolution analysis,  which only considers features that persist over a broad range of scale and compensate dislocation e ects. We aim to extend our work to multi-scale methods to capture features from segments or situations of di erent scales.
Most segmentation algorithms belong to the supervised learning family, where a labelled corpus is available to the algorithm in the learning phase. An unsupervised learning algorithm for segmenting sequences of symbols or categorical events is presented by Shani et al. [10], where the algorithm never sees examples of successful segmentation, but still discovers meaningful segments. The proposed algorithm computes a maximum likelihood segmentation, which is most appropriate to hierarchical sequences, where smaller segments are grouped into larger segments. One advantage of this probabilistic approach is that it allows one to suggest conditional entropy as quality measurement of a segmentation in absence of labelled data. The idea of measuring the quality of segmentation inspired our own performance evaluation on the basis of the employed cost function.
In many cases time series segmentation is used to recognize context from sensor data. For instance, recognizing the context of use is important in making mobile devices as simple to use as possible. The awareness of a user's situa tion can help the device and underlying services in providing an adaptive and personalized user interface. Himberg et al. [4] present a randomized variation of dynamic programming that minimizes the intra-segment variances and gives approximately optimal results. Although their approach is interesting, we are more fascinated by the idea of integrating our own segmentation and clustering algorithm into a real-life context-aware application.
The use of context in mobile devices is receiving increasing attention in mobile and ubiquitous computing research.
Gellersen et al. [3] have investigated multi-sensor contextawareness in a series of projects and report experience from development of a number of device prototypes. In order to design context-aware systems, raw sensor data is transformed into `cues' that incorporate various features based on simple statistics (e.g., standard deviation, quartile distance, etc.). The mapping from cues to context may be explicit, for instance when certain cues are known to be relevant indicators of a speci c context, or implicit in the result of a supervised or unsupervised learning technique. Based on several software prototypes, Gellerson et al. [3] have shown that the integration of diverse sensors is a practical approach to obtain context representing real-life situations.

This paper describes an interesting use case of multi-sensor context-awareness and approves the relevance and topicality of our own foundational research.
Another considerable context-aware application is CenceMe [8], which infers the presence of individuals using sensorenabled mobile phones and shares this information through social network applications (such as Facebook and MySpace).
The CenceMe framework contributes to the design of lightweight classi ers, running on mobile phones, which realize a split-level classi cation paradigm. To infer the presence of individuals, audio data is classi ed using the unsupervised learning technique of discriminant analysis, where the feature vectors are composed of the mean and standard deviation of the DFT (Discrete Fourier Transform) power. In case of context-inference from accelerometer readings the mean, standard deviation, and the number of peaks per unit time are accurate feature vector components, providing high classi cation accuracy. Classi cation of accelerometer data is based on a decision tree technique (J48 algorithm of WEKA data mining workbench1).

6. CONCLUSION This paper is concerned with time series segmentation and segment clustering. We proposed a SVD-based bottom-up algorithm that identi es internally homogeneous time series segments. To recognize recurring patterns, the established time series segments were grouped via agglomerative hierarchical clustering. Subsequently we retrieved recurring sequences of grouped segments, which can be considered as complex situations or higher-level context.
The proposed bottom-up segmentation extends the introduced sensor fusion algorithm [2] by several features, such as automatic determination of model rank and merge threshold. Furthermore, we evaluated the extended segmentation algorithm on several real-life datasets which comprise the progression of motor parameters during a car drive.
Our evaluation demonstrated that the proposed bottomup (BU) approach performs better than the straightforward critical point (CP) segmentation. Additionally, we suggested to employ the segmentation cost to determine the merge threshold. Based on further experiments we showed how to choose a suitable number of clusters for grouping established time series segments.
In our future work, we plan to identify segment sequences that describe normal and abnormal drives or rather drivers, and support performance tuning of an engine.
Although this paper discusses time series segmentation and clustering in terms of multivariate sensor data recorded from vehicles, we explained that our approach is suitable for other domains, such as sensor data collected from smart phones.
We described di erent real-life use cases and applications, where time series analysis is successfully applied to recognize higher-level context or complex situations.

7. ACKNOWLEDGEMENTS The proposed pattern recognition and classi cation approach for multivariate time series was developed in cooperation with the Volkswagen AG, Wolfsburg. Thanks to Bernd Werther and Matthias Pries for providing sensor data from vehicles, and for their contribution of expert knowledge.

The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.