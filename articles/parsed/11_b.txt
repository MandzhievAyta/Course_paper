Information overload is a well-known phenomenon of the information age, since due to the progress in computer power and storage capacity over the last decades, data is produced at an incredible rate. Meanwhile, our ability to collect and store data is growing faster than our ability to analyse it. However, the analysis of these massive, typically messy and inconsistent, volumes of data is crucial in many application domains. For decision makers, analysts or emergency response teams it is an essential task to rapidly extract relevant information from the ﬂood of data.
Today, a selected number of software tools is employed to help analysts to organize their data, generate overviews and explore the information space in order to extract potentially useful information. Most of such data analysis systems still rely on interaction metaphors developed over a decade ago  ∗Part of this work has been presented at Dagstuhl Seminar 05231 “Scientiﬁc Visualization: Challenges for the Future” [3]  and it is questionable whether they are able to meet the demands of the information age. In fact, huge investments in terms of time and money are often wasted, because the possibilities to properly interact with the databases are still too limited. Visual analytics aims at bridging this gap by employing more intelligent means in the analysis process. The basic idea of visual analytics is to visually represent information, allowing the human to directly interact with it, to gain insight, to draw conclusions, and to ultimately make better decisions. Visual representation of the information reduces complex cognitive work needed to perform certain tasks. People may use visual analytics tools and techniques to synthesize information and derive insight from massive, dynamic, and often conﬂicting data by providing timely, defensible, and understandable assessments. [10]  The goal of visual analytics research is to turn the information overload into an opportunity. Decision-makers should be enabled to examine massive, multi-dimensional, multi-source, time-varying information stream to make effective decisions in time-critical situations. For informed decisions, it is indispensable to include humans into the data analysis process to combine ﬂexibility, creativity, and background knowledge with the enormous storage capacity and the computational power of today’s computers. The speciﬁc advantage of visual analytics is that decision makers may focus their full cognitive and perceptual capabilities on the analytical process, while allowing them to apply advanced computational capabilities to augment the discovery process. This paper gives an overview on visual analytics and discusses the most important research challenges in this ﬁeld. Technical research challenges are detailed to show how visual analytics can help to turn information overload as generated by today’s applications into a useful asset.

The rest of the paper is organized as follows: section 2 deﬁnes visual analytics and discusses its scope. Technical challenges are described in Section 3. Finally, section 4 presents the visual analytics mantra as a solution.

In general, visual analytics can be described as “the science of analytical reasoning facilitated by interactive visual interfaces” [10]. To be more precise, visual analytics is an iterative process that involves collecting information, data preprocessing, knowledge representation, interaction, and decision making. The ultimate goal is to gain insight into the problem at hand which is described by vast amounts of scientiﬁc, forensic or business data from heterogeneous sources. To achieve this goal, visual analytics combines the advantages of machines with strenghts of humans. While methods from knowledge discovery in databases (KDD), statistics and mathematics are the driving force on the automatic analysis side, capabilities to perceive, relate and conclude turn visual analytics into a very promising ﬁeld of research.
Historically, visual analytics has evolved out of the ﬁelds of information and scientiﬁc visualization. According to Colin Ware, the term visualization is meanwhile understood as “a graphical representation of data or concepts” [14], while the term was formerly applied to form a mental image. Nowadays fast computers and sophisticated output devices create meaningful visualizations and allow us not only to mentally visualize data and concepts, but to actually see and explore the representation of the data under consideration on a computer screen. However, transformation of data into meaningful visualizations is a non-trivial task that can not be automatically improved through steadily growing computational resources. Very often, there are many different ways to represent the data and it is unclear which representation is the best one. State-of-the-art concepts of representation, perception, interaction and decision-making need to be applied and extended to be suitable for visual data analysis.
The ﬁelds of information and scientiﬁc visualization deal with visual representations of data. Scientiﬁc visualization examines potentially huge amounts of scientiﬁc data obtained from sensors, simulations or laboratory tests with typical applications being ﬂow visualization, volume rendering, and slicing techniques for medical illustrations. In most cases, some aspects of the data can be directly mapped onto geographic coordinates or into virtual 3D environments. We deﬁne information visualization more generally as the communication of abstract data relevant in terms of action through the use of interactive visual interfaces. There are three major goals of visualization, namely a) presentation, b) conﬁrmatory analysis, and c) exploratory analysis.
For presentation purposes, the facts to be presented are ﬁxed a priori, and the choice of the appropriate presentation technique depends largely on the user. The aim is to efﬁciently and effectively communicate the results of an analysis. For conﬁrmatory analysis, one or more hypotheses about the  data serve as a starting point. The process can be described as a goal-oriented examination of these hypotheses. As a result, visualization either conﬁrms these hypotheses or rejects them. Exploratory data analysis as the process of searching and analyzing databases to ﬁnd implicit but potentially useful information, is a difﬁcult task. At the beginning, the analyst has no hypothesis about the data. According to John Tuckey, tools as well as understanding are needed [12] for the interactive and usually undirected search for structures and trends.

Visual analytics is more than just visualization and can rather be seen as an integrated approach combining visualization, human factors and data analysis. Figure 1 illustrates the detailed scope of visual analytics. With respect to the ﬁeld of visualization, visual analytics integrates methodology from information analytics, geospatial analytics, and scientiﬁc analytics. Especially human factors (e.g., interaction, cognition, perception, collaboration, presentation, and dissemination) play a key role in the communication between human and computer, as well as in the decisionmaking process. In this context, production is deﬁned as the creation of materials that summarize the results of an analytical effort, presentation as the packaging of those materials in a way that helps the audience understand the analytical results using terms that are meaningful to them, and dissemination as the process of sharing that information with the intended audience [11]. In matters of data analysis, visual analytics further beneﬁts from the methodologies developed in the ﬁelds of data management & knowledge representation, knowledge discovery, and statistical analytics. Note that visual analytics, is not likely to become a separate ﬁeld of study [15], but its inﬂuence will spread over the research areas it comprises.

According to Jarke J. van Wijk, “visualization is not  ’good’ by deﬁnition, developers of new methods have to make clear why the information sought cannot be extracted automatically” [13]. From this statement, one immediately recognizes the need for the visual analytics approach using automatic methods from statistics, mathematics and knowledge discovery in databases (KDD) wherever they are applicable. Visualization is used as a means to efﬁciently communicate and explore the information space when automatic methods fail. In this context, human background knowledge, intuition, and decision-making either cannot be automated or serve as input for the future development of automated processes.
Overlooking a large information space is a typical visual analytics problem. In many cases, the information at hand is conﬂicting and needs to be integrated from heterogeneous data sources. Moreover, the system lacks the knowledge human experts posses. By applying analytical reasoning, hypotheses about the data can be either afﬁrmed or discarded, eventually leading to a better understanding of the data, thus supporting the analyst in his task to gain insight. Contrary to that, a well-deﬁned problem where the optimum or a good estimation can be calculated by non-interactive analytical means should not be classiﬁed as a visual analytics problem. In such a scenario, non-interactive analysis should be clearly preferred due to efﬁciency reasons. Likewise, visualization problems not involving methods for automatic data analysis do not fall into the category of visual analytics problems.
The ﬁelds of visualization and visual analytics both rely on methods from scientiﬁc, geospatial, and information analytics. They both beneﬁt from the knowledge out of the ﬁeld of interaction as well as of cognitive and perceptual science. They do differ in that visual analytics also integrates the methodology from the ﬁelds of statistical analytics, knowledge discovery, data management & knowledge representation and presentation, production & dissemination.

With information technology becoming a standard in most areas in the past years, from medicine to science, from education to business, from astronomy to networking (Internet), more and more digital information is generated and collected. Today, we continuously face rapidly increasing amounts of data: New sensors, faster recording methods and decreasing prices for storage capacities in the previous years allow storing huge amounts of data that used to be unimaginable a decade ago. Applications like ﬂow simulations, molecular dynamics, nuclear science, computer tomography or astronomy generate amounts of data that can easily reach terabytes; the Large Hadron Collider (LHC) at CERN, for example, generates a volume of 1 petabyte of  data per year. Parallel to the growth of datasets, the computational power of the computer systems for processing these amounts of data also evolved: Faster processors, more main memory, faster networks, parallel and distributed computing and larger storage capacities increase the data throughput every year. Having no possibility of adequately exploring the large amounts of data which have been collected due to their potential usefulness, the data becomes useless and the databases become data “dumps” [2].
Commonly, the amount of data (often terabytes) to be visualized exceeds the limited amount of pixels of a display by several orders of magnitude. Filtering, aggregation, compression, principle component analysis or other data reduction techniques are then needed to reduce the amount of data as only a small portion of it can be displayed. Visual scalability is deﬁned as the capability of visualization tools to effectively display large data sets in terms of either the number or the dimension of individual data elements [1].
We thus not only have to compare the absolute data growth and hardware performance in order to cope with a problem, but also the software and the algorithms to bring this data onto the screen in an appropriate way. Scalability in general is a key challenge of visual analytics as it determines the ability to process large datasets by means of computational overhead as well as appropriate rendering techniques.
In the last decade, information visualization has developed numerous techniques to visualize datasets, but only some of them are scalable to the huge data sets used in visual analytics. As the amount of data is continuously growing and the amount of pixels on the display remains rather constant, the rate of compression to visualize the dataset on the display is continuously increasing, and therefore, more and more details are lost. It is the task of visual analytics to create a higher-level view of the dataset to gain insight, while maximizing the amount of details at the same time.
An example of a highly scalable visual analytics technique in the ﬁeld of ﬁnancial data analysis is the Growth Matrix [4]. It visualizes all possible time intervals of a fund  over a time span of 14 years (about 11.000 intervals), and simultaneously compares the performance of each interval with 14.000 funds that are in the database in just one image (see Fig. 2). To generate each image, 154 million values have to be calculated. Using a large display wall, it is possible to compare several hundreds of these Growth Matrices.
Dynamic processes, arising in business, network or telecommunications generate tremendous streams of time related or real time data. Examples are sensor logs, web statistics, network trafﬁc logs or atmospheric and meteorological applications. Analysis of such data streams is an important challenge, since it plays an essential role in many areas of science and technology. As the sheer amount of data often does not allow to record all the data at full detail, effective compression and feature extraction methods are needed to manage the data. Furthermore, it is vital to provide analysis techniques and metaphors that are capable of analyzing large real time data streams in time, and to present the results in a meaningful and intuitive way. This enables quick identiﬁcation of important information and timely reaction on critical process states or alarming incidents.
Real-world applications often access information from a number of heterogeneous information sources and thus require synthesis of heterogeneous types of data in order to perform an effective analysis. These heterogeneous data sources may include collections of vector data, strings and text documents, graphs or sets of objects. A typical application domain is computational biology, where the human genome is accompanied by real-valued gene expression data, functional annotation of genes, genotyping information, a graph of interacting proteins, equations describing the dynamics of a system, localization of proteins in a cell, and natural language text in the form of papers describing experiments, partial models, and numerous other data sources. The problem of integrating these data sources touches upon many fundamental problems in decision theory, information theory, statistics, and machine learning evidently posing a challenge for visual analytics, too. The focus on scalable and robust methods for fusing complex and heterogeneous data sources is thus a key to a more effective analysis process.
Interpretability or the ability to recognize and understand the data is one of the biggest challenges in visual analytics. Generating a visually correct output from raw data and drawing the right conclusions largely depends on the quality of the used data and methods. Many possible quality problems (e.g., data capture errors, noise, outliers, low precision, missing values, coverage errors, double counts) can already be contained in the raw data. Furthermore, preprocessing of data in order to use it for visual analysis bears many potential quality problems (i.e., data migration and parsing, data cleaning, data reduction, data enrichment, up / down-sampling, rounding and weighting, aggregation and combining). Data can be inherently incomplete or simply out of date. The challenges are on the one hand to determine and to minimize these errors on the pre-processing side, and to provide a ﬂexible yet stable design of the visual analytics application to cope with data quality problems on the other hand. From the technical point of view such application can be either designed to be insensitive to data quality issues through employment of data cleaning methods or to explicitly visualize errors and uncertainty in the application to make the analyst aware of the problem. Many domains ranging from terrorism informatics to natural sciences and business intelligence would potentially beneﬁt from the methods for enhancing data quality. Homeland Security applications in general have to deal with many missing values and uncertainty. For instance, consider a screening program in the context of Homeland Security in an airport. The system should identify potential terrorists, but also try to minimize false positives in order to avoid incorrectly targeting innocent travelers. A falsely inserted data record should not inﬂuence the principle way in which the system observes and analyses other persons. Moreover, updated data of a potential terrorist might not be available in the database for many weeks, but the visual monitoring and analysis of patterns should still work even though the records in the database are widely incomplete.

The ﬁeld of problem solving, decision science, and human information discourse constitutes a further visual analytics challenge. Many psychological studies about the process of problem solving have been conducted. In a usual test setup the subjects have to solve a well-deﬁned problem where the optimal solution is known to the researchers. However, real-world problems are manifold. In many cases these problems are intransparent, consist of conﬂicting goals, and are complex in terms of large numbers of items, interrelations, and decisions involved. In addition to these aspects, information exhibits its own dynamics by changing over time and has thus a strong impact on the optimal solution. To date, decision making is aided by so-called Decision Support Systems which try to represent expert knowledge through rules in order to reduce the risk of human errors when too much interrelated information is involved. Decision making in groups makes the human decision-making process even more complicated.
As for human information discourse, it is estimated that human subjects can reliably distinguish between approximately seven categories in the process of absolute judgement [6]. This number has therefore direct consequences for the design of effective user interfaces. To assess further limitations of human capabilities for more complex tasks is challenging as subjects react differently to screen designs depending on their personality and their cultural, educational, and professional background. Summing up,  the process of decision support for problem solving requires understanding of technology on the one hand, but also comprehension of typical human capabilities such as logic, reasoning, and common sense on the other hand. Intuitive displays and interaction devices should be constructed to steer analysis and to communicate analytical results through meaningful visualizations and clear representations.

Another challenge in the context of visual analytics is to provide semantics for future analysis tasks and decisioncentered visualization. Semantic meta data extracted from heterogeneous sources may capture associations and complex relationships. Therefore, providing techniques to analyse and detect this information is crucial to visual analytics applications. Ontology-driven techniques and systems have already started to enable new semantic applications in a wide span of areas such as bioinformatics, ﬁnancial services, web services , business intelligence, and national security. However, further research is necessary in order to increase capabilities for creating and maintaining large domain ontologies and automatic extraction of semantic meta data, since the integration process between different ontologies to link various datasets is hardly automated yet. In order to perform a more powerful analysis of heterogeneous data sources, more advanced methods for the extraction of semantics from heterogenous data are a key requirement. Thereby, research challenges arise from the size of ontologies, content diversity, heterogeneity as well as from computation of complex queries and link analysis over ontology instances and meta data. New techniques are necessary to resolve semantic heterogeneities in order to discover complex relationships.

User acceptability is a further challenge; many novel visualization techniques have been presented, yet their widespread deployment has not taken place, primarily due to the users’ refusal to change their working routines. Therefore, the advantages of visual analytics tools need to be communicated to the audience of future users to overcome usage barriers, and to eventually tap the full potential of the visual analytics approach. One example is the IBM Remail project [8] which tries to enhance human capabilities to cope with email overload. Concepts such as “Thread Arcs”, “Correspondents Map”, and “Message Map” support the user in efﬁciently analysing his personal email communication.
MIT’s project Oxygen [7] even goes one step further, by addressing the challenges of new systems to be pervasive, embedded, nomadic, adaptable, powerful, intentional and eternal.

Another challenge for visual analytics is the integration of the visualization techniques into other applications and systems. Visual analytics tools and techniques should not stand alone, but should integrate seamlessly into the applications of diverse domains, and allow interaction with other already existing systems. Although many visual analytics  tools are very speciﬁc (like in astronomy or nuclear science) and therefore rather unique, in many domains (like business applications or network security) integration into existing systems would make sense. This requires interactive capabilities and input/output software interfaces for communication with other applications. Real-time visual monitoring and analysis require a connection to databases and frequent updates, and can also be used for automated analysis. In addition to that, visual analytics applications can be integrated or connected to new innovative interaction and visualization devices for improved performance.
Evaluation as a systematic determination of merit, worth, and signiﬁcance of a system is crucial to its success. When evaluating a system, different aspects can be considered such as functional testing, performance benchmarks, measurement of the effectiveness of the display, economic success, user studies, assessment of its impact on decision-making to name just a few. Not all of these aspects are orthogonal, they rather commonly represent comparisons with previous systems to assess the novel system’s adequacy. In this context, objective rules of thumbs to facilitate design decisions would be a great contribution to the community.

Visual analytics combines strengths from information analytics, geospatial analytics, scientiﬁc analytics, statistical analytics, knowledge discovery, data management & knowledge representation, presentation, production & dissemination, cognition, perception, and interaction. It is a goal-oriented process to gain insight into heterogeneous, contradictory and incomplete data through the combination of automatic analysis methods with human background knowledge and intuition.
The example shown in Figure 3 helps to illustrate the ratio behind visual analytics. The ﬁgure shows analysis of stock market data using the CircleView [5] approach, visualizing prices of 240 stocks from the S&P 500 over 6 months, starting from January 2004 (periphery of the circle) to June 2004 (center of the circle). Instead of just visualizing the data, it is analysed by applying a relevance function in the ﬁrst step. Since from the analyst’s point of view it may be more interesting to analyse actual stock prizes rather than historic ones, the basic idea is to show actual stock prizes in full detail and to present historic values as aggregated high level views. That is, the relevance value of each data point is determined by its time stamp and the data is presented at different levels of details (i.e., day, week, month). This helps to reduce the amount of data and to provide overviews even on large data sets. In the next step the stocks are clustered to identify groups of stocks with similar performance over time. If the analyst then identiﬁes  stocks of interest or relevant groups of stocks, he can proceed by selecting relevant subsets of data and performing a detailed analysis using drill down operations on items with lower resolution or select interesting parts of the data and investigate them in more detail using chart techniques.
Unlike described in the information seeking mantra (“overview ﬁrst, zoom/ﬁlter, details on demand”) [9], the visual analytics process comprises the application of automatic analysis methods before and after the interactive visual representation is used. This is primarily due to the fact that current and especially future data sets are complex on the one hand and too large to be visualized in a straightforward manner on the other hand. Therefore, we present the visual analytics mantra:  “Analyse First Show the Important Zoom, Filter and Analyse Further Details on Demand”  The visual analytics mantra could be exemplarily applied in the context of data analysis for network security. Visualizing the raw data is unfeasible and rarely reveals any insight. Therefore, the data is ﬁrst analysed (i.e., compute changes, intrusion detection analysis, etc.) and then displayed. The analyst proceeds by choosing a small suspicious subset of the recorded intrusion incidents by applying ﬁlters and zoom operations. Finally, this subset is used for a more careful analysis. Insight is gained in the course of the whole visual analytics process.