{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from nltk.parse import stanford\n",
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tree import ParentedTree\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RemoveShortStubs(paragraphs):\n",
    "    startStub = [u'it', u'there']\n",
    "    endStub = u'that'\n",
    "    for par in paragraphs:\n",
    "        for sent in par:\n",
    "            startidx = []\n",
    "            endidx = []\n",
    "            flag = 0\n",
    "            for i, word in enumerate(sent):\n",
    "                if word[0].lower() in startStub:\n",
    "                    flag = 1\n",
    "                    start_candidate = i\n",
    "                if (word[0] == endStub) and (flag == 1):\n",
    "                    flag = 0\n",
    "                    startidx.append(start_candidate)\n",
    "                    endidx.append(i)\n",
    "            for stubidx in reversed(range(len(startidx))):\n",
    "                del sent[startidx[stubidx]: endidx[stubidx] + 1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcess(text):\n",
    "    #Deleting literature references\n",
    "    #[([] -open bracket ( or [;  [^([]*? - lazy; \\d+ - number; [^])]*?; [])] - close bracket ) or ]\n",
    "    pat = re.compile(r\"[([][^([]*?\\d+[^])]*?[])]\", re.IGNORECASE or re.DOTALL)\n",
    "    text = re.sub(pat, r\"\", text)\n",
    "    text = re.sub(r'([,.:;\"])', r' \\g<1> ', text)\n",
    "    \n",
    "    #Splitting into paragraphs\n",
    "    paragraphs = re.split(u\"\\n\", text)\n",
    "    #Splitting into sentences\n",
    "    for i, par in enumerate(paragraphs):\n",
    "        paragraphs[i] = tokenize.sent_tokenize(par)\n",
    "    #Tagging all words\n",
    "    st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "    for i, par in enumerate(paragraphs):\n",
    "        for sentence in range(len(par)):\n",
    "            paragraphs[i][sentence] = st.tag(paragraphs[i][sentence].split())\n",
    "    RemoveShortStubs(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(u\"text_test\", 'r', 'utf-8') as fin:\n",
    "        text = fin.read()\n",
    "paragraphs = PreProcess(text)\n",
    "#debug print\n",
    "#for par in paragraphs:\n",
    "#    for sentence in par:\n",
    "#        print sentence, '\\n'\n",
    "#    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SourceText:\n",
    "    def __init__(self, paragraphs):\n",
    "        self._paragraphs = [x for x in paragraphs if x != []]\n",
    "        self._par_iter = 0\n",
    "        self._sent_iter = 0\n",
    "        \n",
    "    def nextPar(self):\n",
    "        self._par_iter += 1\n",
    "        if self._par_iter == len(self._paragraphs):\n",
    "            return 1\n",
    "        else:\n",
    "            self._sent_iter = 0\n",
    "            return 0\n",
    "        \n",
    "    def nextSent(self):\n",
    "        if self._sent_iter < len(self._paragraphs[self._par_iter]):\n",
    "            self._sent_iter += 1\n",
    "            return self._paragraphs[self._par_iter][self._sent_iter - 1], 0\n",
    "        else:\n",
    "            return [], 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OffsetTopicsAndStresses:\n",
    "    def __init__(self):\n",
    "        self.topic_strong_words = [[], [], []]\n",
    "        self.topic_weak_words = [[], [], []]\n",
    "        self.stress_strong_words = [[], [], []]\n",
    "        self.stress_weak_words = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NOT_APPLICABLE = 0\n",
    "UNKNOWN = 1\n",
    "FLUID = 2\n",
    "INVERTED_TOPIC_CANDIDATE = 3\n",
    "OUT_OF_SYNC = 4\n",
    "INVERTED_TOPIC = 5\n",
    "DISCONNECTED = 6\n",
    "\n",
    "class SentInfo:\n",
    "    _fluid_words = [u\"admittedly\", u\"all in all\", u\"as a result\", u\"because\", u\"conversely\", u\"equally\", u\"finally\",\\\n",
    "               u\"for example\", u\"in a similar\", u\"in contrast\", u\"in summary\", u\"initially\", u\"last\", u\"nevertheless\",\\\n",
    "               u\"once\", u\"so far\", u\"such\", u\"after\", u\"along these lines\", u\"as expected\", u\"before\", u\"curiously\",\\\n",
    "               u\"even though\", u\"first\", u\"for instance\", u\"in a way\", u\"in other words\", u\"in the first\",\\\n",
    "               u\"interestingly\", u\"lastly\", u\"next\", u\"regardless\", u\"specifically\", u\"surprisingly\", u\"afterward\",\\\n",
    "               u\"although\", u\"as soon as\", u\"but\", u\"despite\", u\"eventually\", u\"firstly\", u\"for this reason\",\\\n",
    "               u\"in comparison\", u\"in particular\", u\"in the same way\", u\"it follows\", u\"likewise\", u\"nonetheless\",\\\n",
    "               u\"similarly\", u\"still\", u\"that is why\", u\"again\", u\"as a consequence\", u\"be that as it may\",\\\n",
    "               u\"consequently\", u\"during\", u\"figure\", u\"following\", u\"in a certain sense\", u\"in conclusion\", u\"in short\",\\\n",
    "               u\"indeed\", u\"it is as if\", u\"meanwhile\", u\"now\", u\"so\", u\"subsequently\", u\"the first\", u\"the last\",\\\n",
    "               u\"this\", u\"to elaborate\", u\"the next\", u\"this is why\", u\"to explain\", u\"the reason\", u\"thus\",\\\n",
    "               u\"to illustrate\", u\"then\", u\"to conclude\", u\"to put it another way\", u\"to put it succinctly\",\\\n",
    "               u\"unexpectedly\", u\"while\", u\"to sum up\", u\"until\", u\"while\", u\"to summarize\", u\"up to now\", u\"yet\",\\\n",
    "               u\"ultimately\", u\"whereas\"]\n",
    "    _parser = stanford.StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "    def __init__(self, sentence, type):\n",
    "        self.sent = sentence\n",
    "        self.type = type\n",
    "        self.topic_strong_words = []\n",
    "        self.topic_weak_words = []\n",
    "        self.stress_strong_words = []\n",
    "        self.stress_weak_words = []\n",
    "        self._begins_fluid_words = -1\n",
    "        self.current_offset = 0\n",
    "        #0 ~ Sn-1; 1 ~ Sn-2; 2 ~ Sn-3 \n",
    "        self.offset_wordset = OffsetTopicsAndStresses()\n",
    "    \n",
    "    def _deleteNonMainClauses(self, tree):\n",
    "        idxs = tree.treepositions()\n",
    "        for s in list(tree.subtrees(lambda t: t.label() == u\"SBAR\" or t.label() == u\"SBARQ\")):\n",
    "            idx = -1\n",
    "            for x in idxs:\n",
    "                try:\n",
    "                    if s == tree[x]:\n",
    "                        idx = x\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            if idx != -1:\n",
    "                del tree[idx]\n",
    "        return tree\n",
    "    \n",
    "    def _recursive_search(self, tree, subj_list):\n",
    "        if tree.height() == 2 or (tree.height() == 3 and len(tree.leaves()) == 1):\n",
    "            subj_list.extend(tree.leaves())\n",
    "        else:\n",
    "            for np_idx in range(len(tree)):\n",
    "                l = tree[np_idx].label()\n",
    "                target_tags = [u\"NP\", u\"NN\", u\"NNS\", u\"PRP\", u\"CD\"]\n",
    "                if  l in target_tags:\n",
    "                    self._recursive_search(tree[np_idx], subj_list)    \n",
    "    \n",
    "    def _findSubjects(self, tree):\n",
    "        res = []\n",
    "        for s in list(tree.subtrees(lambda t: t.label() == u\"S\")):\n",
    "            child_labels_list = []\n",
    "            for i in range(len(s)):\n",
    "                child_labels_list.append(s[i].label())\n",
    "            if u\"NP\" in child_labels_list and u\"VP\" in child_labels_list:\n",
    "                for np_idx in range(len(s)):\n",
    "                    if s[np_idx].label() == u\"NP\":\n",
    "                        self._recursive_search(s[np_idx], res)\n",
    "        return res\n",
    "    \n",
    "    def _appearsBeforeFirstPunct(self, idx):\n",
    "        punctuation_marks = [tuple([u\",\", u\",\"]), tuple([u\":\", u\":\"]), tuple([u\";\", u\";\"]), tuple([r'\"', r'``'])]\n",
    "        if ([x for x in punctuation_marks if x in self.sent[:idx]] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _appearsAfterLastPunctOrConj(self, idx):\n",
    "        punctuation_marks = [tuple([u\",\", u\",\"]), tuple([u\":\", u\":\"]), tuple([u\";\", u\";\"]), tuple([r'\"', r'``'])]\n",
    "        conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "        if ([x for x in punctuation_marks if x in self.sent[idx:]] == []) or\\\n",
    "        ([x for x in self.sent[idx:] if x[1] in conj_tags] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isMainClauseContainsTopic(self, main_clause_words):\n",
    "        topic = list(self.topic_strong_words)\n",
    "        topic.extend(self.topic_weak_words)\n",
    "        if ([x for x in topic if not x in main_clause_words] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isNumber(self, word):\n",
    "        numbers = [\"one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen,\\\n",
    "        fifteen, sixteen, seventeen, eighteen, nineteen, twenty, thirty, fourty, fifty, sixty, seventy, eighty, ninety,\\\n",
    "        hundred, thousand, million, billion\"]\n",
    "        if (word in numbers) or (re.match(r\"^[-+]?[0-9]+$\", word) != None):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _appearsAfterConj(self, idx):\n",
    "        conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "        if ([x for x in self.sent[:idx] if x[1] in conj_tags] != []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isStrongStress(self, stress_word, main_clause_words):\n",
    "        noun_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\"]\n",
    "        if stress_word[1] in noun_tags:\n",
    "            idx = self.sent.index(stress_word)\n",
    "            if self._appearsBeforeFirstPunct(idx) or self._appearsAfterLastPunctOrConj(idx):\n",
    "                return True\n",
    "            if (stress_word in main_clause_words) and self._isMainClauseContainsTopic(main_clause_words) and\\\n",
    "            self._appearsAfterConj(idx):\n",
    "                return True\n",
    "            if self._isNumber(self.sent[idx - 1][0]):\n",
    "                return True\n",
    "        #verb derived; stress word in main clause\n",
    "        elif stress_word[0] in main_clause_words:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _addStressWords(self, stress_words, main_clause_words):\n",
    "        for stress_word in stress_words:\n",
    "            if self._isStrongStress(stress_word, main_clause_words):\n",
    "                self.stress_strong_words.append(stress_word[0])\n",
    "            else:\n",
    "                self.stress_weak_words.append(stress_word[0])\n",
    "    \n",
    "    def setDefaultWordSet(self):\n",
    "        self.topic_weak_words = []\n",
    "        self.topic_strong_words = []\n",
    "        self.stress_weak_words = []\n",
    "        self.stress_strong_words = []\n",
    "        tree = next(self._parser.tagged_parse(self.sent))            \n",
    "        #display(tree)\n",
    "        \n",
    "        #Добавляем все слова с метками NN, NNS, NNP, NNPS\n",
    "        #Отличить verb derived nouns в VBG FIX\n",
    "        target_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\", u\"VBG\"]\n",
    "        nounsAndVerbDerivedNouns = []\n",
    "        for tup in self.sent:\n",
    "            if tup[1] in target_tags:\n",
    "                nounsAndVerbDerivedNouns.append(tup)\n",
    "        #print \"nounsAndVerbDerivedNouns: \", nounsAndVerbDerivedNouns, \"\\n\"\n",
    "        \n",
    "        #Удаляем non main clauses\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #Ищем subjects\n",
    "        self.topic_strong_words = self._findSubjects(tree)\n",
    "        stress_words = [x for x in nounsAndVerbDerivedNouns if x[0] not in self.topic_strong_words]\n",
    "        self._addStressWords(stress_words, tree.leaves())\n",
    "        #print \"topic_strong_words(main clause subj): \", self.topic_strong_words, \"\\n\"\n",
    "        #print \"strong stress: \", self.stress_strong_words, \"\\n\"\n",
    "        #print \"weak stress: \", self.stress_weak_words, \"\\n\"\n",
    "        #print \"current sentence: \", self.sent, \"\\n\"\n",
    "    \n",
    "    def addStressWords(self, stress_words, offset):\n",
    "        tree = next(self._parser.tagged_parse(self.sent)) \n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #print \"stress_words:\"\n",
    "        #print stress_words\n",
    "        for stress_word in stress_words:\n",
    "            if self._isStrongStress(stress_word, tree.leaves()):\n",
    "                self.offset_wordset.stress_strong_words[offset - 1].append(stress_word[0])\n",
    "            else:\n",
    "                self.offset_wordset.stress_weak_words[offset - 1].append(stress_word[0])\n",
    "    \n",
    "    def getMainClauseSubjects(self):\n",
    "        tree = next(self._parser.tagged_parse(self.sent))\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #display(tree)\n",
    "        return self._findSubjects(tree) \n",
    "    \n",
    "    def getMainClauseWords(self):\n",
    "        tree = next(self._parser.tagged_parse(self.sent))\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        return tree.leaves()\n",
    "        \n",
    "    def beginsWithFluidWords(self):\n",
    "        if self._begins_fluid_words != -1:\n",
    "            return self._begins_fluid_words\n",
    "        verb_tags = [u\"VB\", u\"VBD\", u\"VBN\", u\"VBP\", u\"VBZ\"]\n",
    "        ordinal_numbers = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\",\\\n",
    "                           \"tenth\", \"eleventh\", \"twelfth\", \"thirteenth\", \"fourteenth\", \"fifteenth\", \"sixteenth\",\\\n",
    "                           \"seventeenth\", \"eighteenth\", \"nineteenth\", \"twentieth\", \"thirtieth\", \"fortieth\", \"fiftieth\",\\\n",
    "                           \"sixtieth\", \"seventieth\", \"eightieth\", \"ninetieth\", \"hundredth\", \"thousandth\"]\n",
    "        pronouns_tags = [u\"PRP$\", u\"PRP\"]\n",
    "        patt = re.compile(r\"([A-Z]+\\))|(\\([A-Z]+\\))|([1-9]+\\))|(\\([1-9]+\\))|([1-9]*1st)|([1-9]*2nd)|([1-9]*3rd)|([1-9]*[4-9]th)|([1-9]+0th)\")\n",
    "        for word in self.sent:\n",
    "            if word[1] in verb_tags:\n",
    "                break\n",
    "            #FIX некоторые FLUID WORDS состоят из двух и более слов, а я рассматриваю только по одному\n",
    "            if (word[0].lower() in self._fluid_words) or (word[1] in pronouns_tags) or\\\n",
    "            (re.search(patt, word[0]) != None) or (word[0].lower() in ordinal_numbers):\n",
    "                self.beginsWithFluidWords = 1\n",
    "            else:\n",
    "                self.beginsWithFluidWords = 0\n",
    "        return self.beginsWithFluidWords\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#проверка алгоритма(поиск defaultWordSet для каждого предложения)\n",
    "# state = 0\n",
    "# text = SourceText(paragraphs)\n",
    "# sent_list = []\n",
    "\n",
    "# while state == 0:\n",
    "#     print state\n",
    "#     sent, p = text.nextSent()\n",
    "#     while p != 1:\n",
    "#         sent_list.append(SentInfo(sent, UNKNOWN))\n",
    "#         sent_list[-1].setDefaultWordSet()\n",
    "#         sent, p = text.nextSent()\n",
    "#     state = text.nextPar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STRONG_TOPIC = 10\n",
    "WEAK_TOPIC = 11\n",
    "\n",
    "def BetweenFluidOrInverted(sent_list, offset):\n",
    "    #check Sn-1\n",
    "    if sent_list[-2].type != FLUID and sent_list[-2].type != INVERTED_TOPIC_CANDIDATE:\n",
    "        return False\n",
    "    #check Sn-2\n",
    "    if sent_list[-3].type != FLUID and sent_list[-3].type != INVERTED_TOPIC_CANDIDATE:\n",
    "        return False\n",
    "    if offset == 3:\n",
    "        if sent_list[-4].type != FLUID and sent_list[-4].type != INVERTED_TOPIC_CANDIDATE:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def TopicFound(topic_words, topic_type, sentence, previous_sentence, reached_verb, offset, sent_list):\n",
    "#     print \"trying to find topic!\", topic_words, topic_type\n",
    "    if offset == 1:\n",
    "        if not reached_verb:\n",
    "            sentence.type = FLUID\n",
    "        else:\n",
    "            sentence.type = INVERTED_TOPIC_CANDIDATE\n",
    "    else:\n",
    "        if not reached_verb:\n",
    "            if BetweenFluidOrInverted(sent_list, offset):\n",
    "                sentence.type = FLUID\n",
    "            else:\n",
    "                sentence.type = OUT_OF_SYNC\n",
    "    if topic_type == WEAK_TOPIC:\n",
    "        if not set(topic_words).issubset(set(sentence.offset_wordset.topic_strong_words[offset - 1])):\n",
    "            sentence.offset_wordset.topic_weak_words[offset - 1].extend(topic_words)\n",
    "    elif topic_type == STRONG_TOPIC:\n",
    "        sentence.offset_wordset.topic_strong_words[offset - 1].extend(topic_words)\n",
    "                 \n",
    "def CheckSentenceMainClauses(sentence, previous_sentence, sent_list):\n",
    "    offset = sentence.current_offset\n",
    "    main_clause_subjects = sentence.getMainClauseSubjects()\n",
    "    prev_sent_wordset = list(previous_sentence.topic_weak_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.topic_strong_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.stress_strong_words)\n",
    "#     print \"prev_sent_wordset: \", prev_sent_wordset\n",
    "#     print \"main clause subjects: \", main_clause_subjects\n",
    "    matched_words = [x for x in main_clause_subjects if x in prev_sent_wordset]\n",
    "#     print \"found strong topic! Give me sec to check: \", matched_words\n",
    "    if matched_words != []:\n",
    "        TopicFound(matched_words, STRONG_TOPIC, sentence, previous_sentence, False, offset, sent_list)\n",
    "    #FIX какие именно subjects имеются в виду?\n",
    "    stress_words = [x for x in main_clause_subjects if x not in matched_words]\n",
    "    sentence.addStressWords([(x, u\"\") for x in stress_words], offset)\n",
    "    \n",
    "def CheckWholeSentence(sentence, previous_sentence, sent_list):\n",
    "    offset = sentence.current_offset\n",
    "    reached_verb = False\n",
    "    reached_topic_or_main = False\n",
    "    prev_sent_wordset = list(previous_sentence.topic_weak_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.topic_strong_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.stress_strong_words)\n",
    "    main_clause_words = sentence.getMainClauseWords()\n",
    "#     print \"prev_sent_wordset: \", prev_sent_wordset\n",
    "#     print \"main clause words: \", main_clause_words\n",
    "    conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    noun_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\"]\n",
    "    \n",
    "    for word in sentence.sent:\n",
    "        reached_verb = word[1] in conj_tags or reached_verb\n",
    "        reached_topic_or_main = word[0] in main_clause_words or reached_topic_or_main\n",
    "#         print word, reached_verb, reached_topic_or_main\n",
    "        if not reached_verb:\n",
    "            matches = word[0] in prev_sent_wordset\n",
    "            if (matches and word[1] in noun_tags) or (matches and reached_topic_or_main and (word[1] == u\"VBG\")):\n",
    "                TopicFound([word[0]], WEAK_TOPIC, sentence, previous_sentence, False, offset, sent_list)\n",
    "            elif (word[1] == u\"VBG\"):\n",
    "                sentence.addStressWords([word], offset)\n",
    "        else:\n",
    "            if (sentence.offset_wordset.topic_strong_words[offset - 1] != []) or (sentence.offset_wordset.topic_weak_words[offset - 1] != []):\n",
    "                if word[1] in noun_tags:\n",
    "                    sentence.addStressWords([word], offset)\n",
    "            else:\n",
    "                matches = word[0] in prev_sent_wordset\n",
    "                if (word[1] in noun_tags) and matches:\n",
    "#                     print \"special for recognition!!!\", [word[0]]\n",
    "                    TopicFound([word[0]], WEAK_TOPIC, sentence, previous_sentence, True, offset, sent_list)\n",
    "                    reached_topic_or_main = True\n",
    "                elif (word[1] == u\"VBG\") and matches:\n",
    "                    sentence.addStressWords([word], offset)\n",
    "                    \n",
    "def CheckSentenceProgression(sentence, previous_sentence, sent_list):\n",
    "    sentence.current_offset += 1\n",
    "    CheckSentenceMainClauses(sentence, previous_sentence, sent_list)\n",
    "    CheckWholeSentence(sentence, previous_sentence, sent_list)\n",
    "    \n",
    "def DefineResults(sentence, total_amount):\n",
    "    if sentence.type == UNKNOWN:\n",
    "        sentence.type = DISCONNECTED\n",
    "        sentence.setDefaultWordSet()\n",
    "    else:\n",
    "        if sentence.type == INVERTED_TOPIC:\n",
    "            word_set_from_round = 1\n",
    "        elif sentence.type in [FLUID, OUT_OF_SYNC]:\n",
    "            word_set_from_round = sentence.current_offset\n",
    "        sentence.topic_strong_words = sentence.offset_wordset.topic_strong_words[word_set_from_round - 1]\n",
    "        sentence.topic_weak_words = sentence.offset_wordset.topic_weak_words[word_set_from_round - 1]\n",
    "        sentence.stress_strong_words = sentence.offset_wordset.stress_strong_words[word_set_from_round - 1]\n",
    "        sentence.stress_weak_words = sentence.offset_wordset.stress_weak_words[word_set_from_round - 1]\n",
    "    if sentence.type == FLUID:\n",
    "        total_amount[0] += 1\n",
    "    elif sentence.type == INVERTED_TOPIC:\n",
    "        total_amount[1] += 1\n",
    "    elif sentence.type == OUT_OF_SYNC:\n",
    "        total_amount[2] +=1\n",
    "    elif sentence.type == DISCONNECTED:\n",
    "        total_amount[3] += 1\n",
    "        \n",
    "def TypeToString(type):\n",
    "    if type == FLUID:\n",
    "        return \"Fluid\"\n",
    "    elif type == INVERTED_TOPIC:\n",
    "        return \"Inverted topic\"\n",
    "    elif type == OUT_OF_SYNC:\n",
    "        return \"Out if sync\"\n",
    "    elif type == DISCONNECTED:\n",
    "        return \"Disconnected\"\n",
    "    elif type == NOT_APPLICABLE:\n",
    "        return \"Not applicable\"\n",
    "    \n",
    "def PrintSentInfo(sent):\n",
    "    print \"--------------------------------------------------------------------------------------------------------\"\n",
    "    print \"\\nSentence: \", sent.sent\n",
    "    print \"Topic:\"\n",
    "    print \"Strong: \", sent.topic_strong_words\n",
    "    print \"Weak  : \", sent.topic_weak_words\n",
    "    print \"Stress:\"\n",
    "    print \"Strong: \", sent.stress_strong_words\n",
    "    print \"Weak  : \", sent.stress_weak_words\n",
    "    print \"\\nType: \", TypeToString(sent_list[-1].type)\n",
    "    print \"\\nWordsets from checkups with Sn-1, Sn-2, Sn-3:\"\n",
    "    print \"Topic:\"\n",
    "    print \"Strong: \", sent.offset_wordset.topic_strong_words\n",
    "    print \"Weak: \", sent.offset_wordset.topic_weak_words\n",
    "    print \"Stress:\"\n",
    "    print \"Strong: \", sent.offset_wordset.stress_strong_words\n",
    "    print \"Weak: \", sent.offset_wordset.stress_weak_words\n",
    "    print \"--------------------------------------------------------------------------------------------------------\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next Paragraph:\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'A', u'DT'), (u'search', u'NN'), (u'on', u'IN'), (u'\"', u'``'), (u'tools', u'NNS'), (u'to', u'TO'), (u'evaluate', u'VB'), (u'the', u'DT'), (u'quality', u'NN'), (u'of', u'IN'), (u'writing', u'VBG'), (u'\"', u'``'), (u'often', u'RB'), (u'gets', u'VBZ'), (u'you', u'PRP'), (u'to', u'TO'), (u'sites', u'NNS'), (u'assessing', u'VBG'), (u'only', u'RB'), (u'one', u'CD'), (u'of', u'IN'), (u'the', u'DT'), (u'qualities', u'NNS'), (u'of', u'IN'), (u'writing', u'NN'), (u':', u':'), (u'its', u'PRP$'), (u'readability', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'search']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'writing', u'assessing', u'qualities', u'writing', u'readability']\n",
      "Weak  :  [u'tools', u'quality', u'sites']\n",
      "\n",
      "Type:  Not applicable\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Measuring', u'VBG'), (u'ease', u'NN'), (u'of', u'IN'), (u'reading', u'NN'), (u'is', u'VBZ'), (u'indeed', u'RB'), (u'useful', u'JJ'), (u'to', u'TO'), (u'determine', u'VB'), (u'if', u'IN'), (u'your', u'PRP$'), (u'writing', u'NN'), (u'meets', u'VBZ'), (u'the', u'DT'), (u'reading', u'NN'), (u'level', u'NN'), (u'of', u'IN'), (u'your', u'PRP$'), (u'targeted', u'VBN'), (u'reader', u'NN'), (u',', u','), (u'but', u'CC'), (u'with', u'IN'), (u'scientific', u'JJ'), (u'writing', u'NN'), (u',', u','), (u'the', u'DT'), (u'statistical', u'JJ'), (u'formulae', u'NN'), (u'and', u'CC'), (u'readability', u'NN'), (u'indices', u'NNS'), (u'such', u'JJ'), (u'as', u'IN'), (u'Flesch-Kincaid', u'NNP'), (u'lose', u'VBP'), (u'their', u'PRP$'), (u'usefulness', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'readability']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'ease', u'formulae', u'indices', u'ease', u'reading', u'writing', u'reading', u'level', u'reader', u'writing', u'formulae', u'readability', u'indices', u'Flesch-Kincaid', u'usefulness']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[u'readability'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'ease', u'formulae', u'indices', u'ease', u'reading', u'writing', u'reading', u'level', u'reader', u'writing', u'formulae', u'readability', u'indices', u'Flesch-Kincaid', u'usefulness'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'In', u'IN'), (u'a', u'DT'), (u'way', u'NN'), (u',', u','), (u'readability', u'NN'), (u'is', u'VBZ'), (u'subjective', u'JJ'), (u'and', u'CC'), (u'dependent', u'JJ'), (u'on', u'IN'), (u'how', u'WRB'), (u'familiar', u'JJ'), (u'the', u'DT'), (u'reader', u'NN'), (u'is', u'VBZ'), (u'with', u'IN'), (u'the', u'DT'), (u'specific', u'JJ'), (u'vocabulary', u'NN'), (u'and', u'CC'), (u'the', u'DT'), (u'written', u'VBN'), (u'style', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'readability']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'way', u'reader', u'vocabulary', u'style']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Not applicable\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Scientific', u'NNP'), (u'reader', u'NN'), (u'is', u'VBZ'), (u'with', u'IN'), (u'the', u'DT'), (u'specific', u'JJ'), (u'vocabulary', u'NN'), (u'and', u'CC'), (u'the', u'DT'), (u'written', u'VBN'), (u'style', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'reader']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'vocabulary', u'style']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[u'reader'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'vocabulary', u'style'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Scientific', u'NNP'), (u'papers', u'NNS'), (u'are', u'VBP'), (u'targeting', u'VBG'), (u'an', u'DT'), (u'audience', u'NN'), (u'at', u'IN'), (u'ease', u'NN'), (u'with', u'IN'), (u'a', u'DT'), (u'more', u'RBR'), (u'specialized', u'JJ'), (u'vocabulary', u'NN'), (u',', u','), (u'an', u'DT'), (u'audience', u'NN'), (u'expecting', u'VBG'), (u'sentence-lengthening', u'JJ'), (u'precision', u'NN'), (u'in', u'IN'), (u'writing', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  [u'vocabulary']\n",
      "Stress:\n",
      "Strong:  [u'papers', u'audience', u'precision', u'writing']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Inverted topic\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[u'vocabulary'], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'papers', u'audience', u'precision', u'writing'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'The', u'DT'), (u'readability', u'NN'), (u'index', u'NN'), (u'would', u'MD'), (u'require', u'VB'), (u'recalibration', u'NN'), (u'for', u'IN'), (u'such', u'JJ'), (u'a', u'DT'), (u'specific', u'JJ'), (u'audience', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  [u'audience']\n",
      "Stress:\n",
      "Strong:  [u'readability', u'index']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[u'audience'], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'readability', u'index'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'But', u'CC'), (u'the', u'DT'), (u'need', u'NN'), (u'for', u'IN'), (u'readability', u'NN'), (u'indices', u'NNS'), (u'is', u'VBZ'), (u'not', u'RB'), (u'questioned', u'VBN'), (u'here', u'RB'), (u'.', u'.'), (u'\"', u\"''\")]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  [u'readability']\n",
      "Stress:\n",
      "Strong:  [u'need']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[u'readability'], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'need'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Science', u'NNP'), (u'is', u'VBZ'), (u'often', u'RB'), (u'hard', u'JJ'), (u'to', u'TO'), (u'read', u'VB'), (u'\"', u'``'), (u',', u','), (u'even', u'RB'), (u'for', u'IN'), (u'scientists', u'NNS'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'Science']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'scientists']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Disconnected\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'Science'], [u'Science'], [u'Science']]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Science', u'NNP'), (u'is', u'VBZ'), (u'also', u'RB'), (u'hard', u'JJ'), (u'to', u'TO'), (u'write', u'VB'), (u',', u','), (u'and', u'CC'), (u'finding', u'VBG'), (u'fault', u'NN'), (u'with', u'IN'), (u\"one's\", u'JJ'), (u'own', u'JJ'), (u'writing', u'NN'), (u'is', u'VBZ'), (u'even', u'RB'), (u'more', u'RBR'), (u'challenging', u'JJ'), (u'since', u'IN'), (u'we', u'PRP'), (u'understand', u'VBP'), (u'ourselves', u'PRP'), (u'perfectly', u'RB'), (u',', u','), (u'at', u'IN'), (u'least', u'JJS'), (u'most', u'JJS'), (u'of', u'IN'), (u'the', u'DT'), (u'time', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'Science']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'finding', u'time']\n",
      "Weak  :  [u'fault', u'writing']\n",
      "\n",
      "Type:  Not applicable\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'To', u'TO'), (u'gain', u'VB'), (u'objectivity', u'NN'), (u'scientists', u'NNS'), (u'turn', u'VBP'), (u'away', u'RB'), (u'from', u'IN'), (u'silent', u'JJ'), (u'readability', u'NN'), (u'indices', u'NNS'), (u'and', u'CC'), (u'find', u'VB'), (u'more', u'RBR'), (u'direct', u'JJ'), (u'help', u'NN'), (u'in', u'IN'), (u'checklists', u'NNS'), (u'such', u'JJ'), (u'as', u'IN'), (u'the', u'DT'), (u'peer', u'VBP'), (u'review', u'NN'), (u'form', u'NN'), (u'proposed', u'VBN'), (u'by', u'IN'), (u'Bates', u'NNP'), (u'College', u'NNP'), (u',', u','), (u'or', u'CC'), (u'scoring', u'VBG'), (u'sheets', u'NNS'), (u'to', u'TO'), (u'assess', u'VB'), (u'the', u'DT'), (u'quality', u'NN'), (u'of', u'IN'), (u'a', u'DT'), (u'scientific', u'JJ'), (u'paper', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'objectivity', u'scientists', u'readability', u'indices', u'help', u'checklists', u'review', u'form', u'Bates', u'College', u'scoring', u'sheets', u'quality', u'paper']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Disconnected\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'These', u'DT'), (u'organise', u'VBP'), (u'a', u'DT'), (u'systematic', u'JJ'), (u'and', u'CC'), (u'critical', u'JJ'), (u'walk', u'NN'), (u'through', u'IN'), (u'each', u'DT'), (u'part', u'NN'), (u'of', u'IN'), (u'a', u'DT'), (u'paper', u'NN'), (u',', u','), (u'from', u'IN'), (u'its', u'PRP$'), (u'title', u'NN'), (u'to', u'TO'), (u'its', u'PRP$'), (u'references', u'NNS'), (u'in', u'IN'), (u'peer-review', u'JJ'), (u'style', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  [u'paper']\n",
      "Stress:\n",
      "Strong:  [u'These', u'title', u'references', u'style']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Inverted topic\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[u'paper'], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'These', u'title', u'references', u'style'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'They', u'PRP'), (u'integrate', u'VBP'), (u'readability', u'NN'), (u'criteria', u'NNS'), (u'that', u'WDT'), (u'far', u'RB'), (u'exceed', u'VBP'), (u'those', u'DT'), (u'covered', u'VBN'), (u'by', u'IN'), (u'statistical', u'JJ'), (u'lexical', u'JJ'), (u'tools', u'NNS'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'They']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'readability', u'criteria', u'tools']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'For', u'IN'), (u'example', u'NN'), (u',', u','), (u'they', u'PRP'), (u'examine', u'VBP'), (u'how', u'WRB'), (u'the', u'DT'), (u'text', u'NN'), (u'structure', u'NN'), (u'frames', u'NNS'), (u'the', u'DT'), (u'contents', u'NNS'), (u'under', u'IN'), (u'headings', u'NNS'), (u'and', u'CC'), (u'subheadings', u'NNS'), (u'that', u'WDT'), (u'are', u'VBP'), (u'consistent', u'JJ'), (u'with', u'IN'), (u'the', u'DT'), (u'title', u'NN'), (u'and', u'CC'), (u'abstract', u'JJ'), (u'of', u'IN'), (u'the', u'DT'), (u'paper', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'they']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'example', u'text', u'structure', u'frames', u'contents', u'headings', u'subheadings', u'title', u'paper']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'They', u'PRP'), (u'test', u'VBP'), (u'whether', u'IN'), (u'or', u'CC'), (u'not', u'RB'), (u'the', u'DT'), (u'writer', u'NN'), (u'fluidly', u'RB'), (u'meets', u'VBZ'), (u'the', u'DT'), (u'expectations', u'NNS'), (u'of', u'IN'), (u'the', u'DT'), (u'reader', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'They']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'writer', u'expectations', u'reader']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Written', u'VBN'), (u'by', u'IN'), (u'expert', u'NN'), (u'reviewers', u'NNS'), (u'(and', u'FW'), (u'readers)', u'FW'), (u',', u','), (u'they', u'PRP'), (u'represent', u'VBP'), (u'them', u'PRP'), (u',', u','), (u'their', u'PRP$'), (u'needs', u'NNS'), (u'and', u'CC'), (u'concerns', u'NNS'), (u',', u','), (u'and', u'CC'), (u'act', u'VBP'), (u'as', u'IN'), (u'their', u'PRP$'), (u'proxy', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'they']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'expert', u'reviewers', u'proxy']\n",
      "Weak  :  [u'needs', u'concerns']\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Such', u'JJ'), (u'manual', u'JJ'), (u'tools', u'NNS'), (u'effectively', u'RB'), (u'improve', u'VBP'), (u'writing', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'tools']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'writing']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Disconnected\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'tools'], [u'tools'], [u'tools']]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Just', u'RB'), (u'for', u'IN'), (u'test', u'NN'), (u':', u':'), (u'bla-bla', u'NN'), (u',', u','), (u'Gopen', u'NNP'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'test', u'bla-bla', u'Gopen']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Disconnected\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'One', u'CD'), (u'of', u'IN'), (u'the', u'DT'), (u'biggest', u'JJS'), (u'challenges', u'NNS'), (u'in', u'IN'), (u'automatic', u'JJ'), (u'speaker', u'NN'), (u'recognition', u'NN'), (u'is', u'VBZ'), (u'obtaining', u'VBG'), (u'invariance', u'NN'), (u'across', u'IN'), (u'varying', u'VBG'), (u'operating', u'NN'), (u'conditions', u'NNS'), (u',', u','), (u'while', u'IN'), (u'retaining', u'VBG'), (u'maximum', u'NN'), (u'speaker', u'NN'), (u'variability', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'One']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'challenges', u'speaker', u'recognition', u'obtaining', u'invariance', u'varying', u'operating', u'conditions', u'maximum', u'speaker', u'variability']\n",
      "Weak  :  [u'retaining']\n",
      "\n",
      "Type:  Not applicable\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Different', u'JJ'), (u'handset', u'NN'), (u'type', u'NN'), (u',', u','), (u'transmission', u'NN'), (u'line/coding', u'NN'), (u',', u','), (u'and', u'CC'), (u'background', u'NN'), (u'noise', u'NN'), (u'are', u'VBP'), (u'typical', u'JJ'), (u'factors', u'NNS'), (u',', u','), (u'which', u'WDT'), (u'lead', u'VBP'), (u'to', u'TO'), (u'signal', u'NN'), (u'mismatch', u'NN'), (u'across', u'IN'), (u'training', u'NN'), (u'and', u'CC'), (u'recognition', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'handset', u'type', u'transmission', u'line/coding', u'background', u'noise']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'signal', u'mismatch', u'training', u'recognition']\n",
      "Weak  :  [u'factors']\n",
      "\n",
      "Type:  Not applicable\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'For', u'IN'), (u'a', u'DT'), (u'speaker', u'NN'), (u'recognition', u'NN'), (u'system', u'NN'), (u'to', u'TO'), (u'be', u'VB'), (u'useful', u'JJ'), (u'in', u'IN'), (u'practice', u'NN'), (u'it', u'PRP'), (u'needs', u'VBZ'), (u'to', u'TO'), (u'be', u'VB'), (u'optimized', u'VBN'), (u'against', u'IN'), (u'the', u'DT'), (u'mismatch', u'NN'), (u'problem', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  [u'recognition']\n",
      "Stress:\n",
      "Strong:  [u'it', u'mismatch', u'problem']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[u'recognition'], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'it', u'mismatch', u'problem'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'One', u'CD'), (u'of', u'IN'), (u'the', u'DT'), (u'biggest', u'JJS'), (u'challenges', u'NNS'), (u'in', u'IN'), (u'automatic', u'JJ'), (u'speaker', u'NN'), (u'recognition', u'NN'), (u'is', u'VBZ'), (u'obtaining', u'VBG'), (u'invariance', u'NN'), (u'across', u'IN'), (u'varying', u'VBG'), (u'operating', u'NN'), (u'conditions', u'NNS'), (u',', u','), (u'while', u'IN'), (u'retaining', u'VBG'), (u'maximum', u'NN'), (u'speaker', u'NN'), (u'variability', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'One']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'challenges', u'speaker', u'recognition', u'obtaining', u'invariance', u'varying', u'operating', u'conditions', u'maximum', u'speaker', u'variability']\n",
      "Weak  :  [u'retaining']\n",
      "\n",
      "Type:  Not applicable\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'Different', u'JJ'), (u'handset', u'NN'), (u'type', u'NN'), (u',', u','), (u'transmission', u'NN'), (u'line/coding', u'NN'), (u',', u','), (u'and', u'CC'), (u'background', u'NN'), (u'noise', u'NN'), (u'are', u'VBP'), (u'typical', u'JJ'), (u'factors', u'NNS'), (u',', u','), (u'which', u'WDT'), (u'lead', u'VBP'), (u'to', u'TO'), (u'signal', u'NN'), (u'mismatch', u'NN'), (u'across', u'IN'), (u'training', u'NN'), (u'and', u'CC'), (u'recognition', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  [u'recognition']\n",
      "Stress:\n",
      "Strong:  [u'handset', u'type', u'transmission', u'line/coding', u'background', u'noise']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Inverted topic\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[u'recognition'], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'handset', u'type', u'transmission', u'line/coding', u'background', u'noise'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'State-of-the-art', u'JJ'), (u'text-independent', u'JJ'), (u'speaker', u'NN'), (u'recognizers', u'NNS'), (u'use', u'VBP'), (u'mean', u'JJ'), (u'subtraction', u'NN'), (u'at', u'IN'), (u'the', u'DT'), (u'utterance', u'NN'), (u'level', u'NN'), (u',', u','), (u'often', u'RB'), (u'referred', u'VBN'), (u'to', u'TO'), (u'as', u'IN'), (u'cepstral', u'JJ'), (u'mean', u'JJ'), (u'subtraction', u'NN'), (u'(CMS)', u'NN'), (u'in', u'IN'), (u'the', u'DT'), (u'context', u'NN'), (u'of', u'IN'), (u'cepstral', u'JJ'), (u'features', u'NNS'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'speaker', u'recognizers']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'subtraction', u'utterance', u'level', u'subtraction', u'(CMS)', u'context', u'features']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Not applicable\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'The', u'DT'), (u'assumption', u'NN'), (u'in', u'IN'), (u'mean', u'JJ'), (u'subtraction', u'NN'), (u'is', u'VBZ'), (u'that', u'IN'), (u'all', u'PDT'), (u'the', u'DT'), (u'feature', u'NN'), (u'vectors', u'NNS'), (u'have', u'VBP'), (u'been', u'VBN'), (u'translated', u'VBN'), (u'by', u'IN'), (u'an', u'DT'), (u'unknown', u'JJ'), (u'channel-dependent', u'JJ'), (u'vector', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  [u'subtraction']\n",
      "Stress:\n",
      "Strong:  [u'assumption', u'feature', u'vectors', u'vector']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Fluid\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[u'subtraction'], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'assumption', u'feature', u'vectors', u'vector'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'By', u'IN'), (u'subtracting', u'VBG'), (u'the', u'DT'), (u'mean', u'NN'), (u'from', u'IN'), (u'both', u'CC'), (u'the', u'DT'), (u'training', u'NN'), (u'and', u'CC'), (u'testing', u'NN'), (u'vectors', u'NNS'), (u',', u','), (u'the', u'DT'), (u'matching', u'NN'), (u'is', u'VBZ'), (u'less', u'RBR'), (u'affected', u'VBN'), (u'by', u'IN'), (u'this', u'DT'), (u'bias', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  [u'vectors']\n",
      "Stress:\n",
      "Strong:  [u'matching', u'matching', u'bias']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Inverted topic\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[u'vectors'], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'matching', u'matching', u'bias'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'For', u'IN'), (u'clean', u'JJ'), (u'data', u'NNS'), (u'(no', u'NN'), (u'channel', u'NN'), (u'mismatch)', u'NN'), (u',', u','), (u'CMS', u'NNP'), (u'degrades', u'VBZ'), (u'accuracy', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'CMS']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'data', u'(no', u'channel', u'mismatch)', u'accuracy']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Disconnected\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'CMS'], [u'CMS'], [u'CMS']]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'The', u'DT'), (u'assumption', u'NN'), (u'in', u'IN'), (u'mean', u'JJ'), (u'subtraction', u'NN'), (u'is', u'VBZ'), (u'that', u'IN'), (u'all', u'PDT'), (u'the', u'DT'), (u'feature', u'NN'), (u'vectors', u'NNS'), (u'have', u'VBP'), (u'been', u'VBN'), (u'translated', u'VBN'), (u'by', u'IN'), (u'an', u'DT'), (u'unknown', u'JJ'), (u'channel-dependent', u'JJ'), (u'vector', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'assumption']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'subtraction', u'feature', u'vectors', u'vector']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Not applicable\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'By', u'IN'), (u'subtracting', u'VBG'), (u'the', u'DT'), (u'mean', u'NN'), (u'from', u'IN'), (u'both', u'CC'), (u'the', u'DT'), (u'training', u'NN'), (u'and', u'CC'), (u'testing', u'NN'), (u'vectors', u'NNS'), (u',', u','), (u'the', u'DT'), (u'matching', u'NN'), (u'is', u'VBZ'), (u'less', u'RBR'), (u'affected', u'JJ'), (u'bythis', u'NN'), (u'bias', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  []\n",
      "Weak  :  [u'vectors']\n",
      "Stress:\n",
      "Strong:  [u'matching', u'matching', u'bythis', u'bias']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Inverted topic\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[u'vectors'], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'matching', u'matching', u'bythis', u'bias'], [], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'For', u'IN'), (u'clean', u'JJ'), (u'data', u'NNS'), (u'(no', u'NN'), (u'channel', u'NN'), (u'mismatch)', u'NN'), (u',', u','), (u'CMS', u'NNP'), (u'degrades', u'VBZ'), (u'accuracy', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'CMS']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'data', u'(no', u'channel', u'mismatch)', u'accuracy']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Disconnected\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'CMS'], [u'CMS'], []]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Sentence:  [(u'A', u'DT'), (u'general', u'JJ'), (u'affine', u'NN'), (u'channel/environment', u'NN'), (u'model', u'NN'), (u'includes', u'VBZ'), (u'rotation', u'NN'), (u'and', u'CC'), (u'scaling', u'NN'), (u'of', u'IN'), (u'the', u'DT'), (u'feature', u'NN'), (u'vectors', u'NNS'), (u'in', u'IN'), (u'addition', u'NN'), (u'to', u'TO'), (u'the', u'DT'), (u'additive', u'JJ'), (u'bias', u'NN'), (u'.', u'.')]\n",
      "Topic:\n",
      "Strong:  [u'affine', u'channel/environment', u'model']\n",
      "Weak  :  []\n",
      "Stress:\n",
      "Strong:  [u'rotation', u'scaling', u'feature', u'vectors', u'addition', u'bias']\n",
      "Weak  :  []\n",
      "\n",
      "Type:  Disconnected\n",
      "\n",
      "Wordsets from checkups with Sn-1, Sn-2, Sn-3:\n",
      "Topic:\n",
      "Strong:  [[], [], []]\n",
      "Weak:  [[], [], []]\n",
      "Stress:\n",
      "Strong:  [[u'affine', u'channel/environment', u'model'], [u'affine', u'channel/environment', u'model'], [u'affine', u'channel/environment', u'model']]\n",
      "Weak:  [[], [], []]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "############################################################################################################\n",
      "Total amount:\n",
      "Fluid sentences:  6\n",
      "Inverted topic sentences:  5\n",
      "Out of sync sentences:  0\n",
      "Disconnected sentences:  7\n"
     ]
    }
   ],
   "source": [
    "total_amount = [0, 0, 0, 0] #fluid, inverted_topic, out_of_sync, disconnected\n",
    "state = 0\n",
    "text = SourceText(paragraphs)\n",
    "while state == 0:\n",
    "    print \"\\nNext Paragraph:\"\n",
    "    sent_list = []\n",
    "    sent, p = text.nextSent()\n",
    "    sent_list.append(SentInfo(sent, NOT_APPLICABLE))\n",
    "    sent_list[-1].setDefaultWordSet()\n",
    "    PrintSentInfo(sent_list[-1])\n",
    "    sent, p = text.nextSent()\n",
    "    while p == 0:\n",
    "        sent_list.append(SentInfo(sent, UNKNOWN))\n",
    "        if sent_list[-1].beginsWithFluidWords():\n",
    "            sent_list[-1].type = FLUID\n",
    "            sent_list[-1].setDefaultWordSet()\n",
    "        else:\n",
    "            CheckSentenceProgression(sent_list[-1], sent_list[-2], sent_list)\n",
    "            if sent_list[-1].type == INVERTED_TOPIC_CANDIDATE:\n",
    "                sent_list[-1].type = INVERTED_TOPIC\n",
    "            elif sent_list[-1].type == UNKNOWN:\n",
    "                if len(sent_list) > 2:\n",
    "                    for offset in range(2):\n",
    "                        CheckSentenceProgression(sent_list[-1], sent_list[-2], sent_list)\n",
    "                        if sent_list[-1].type != UNKNOWN or len(sent_list) < 4:\n",
    "                            break\n",
    "            DefineResults(sent_list[-1], total_amount)\n",
    "        PrintSentInfo(sent_list[-1])\n",
    "        sent, p = text.nextSent()\n",
    "    state = text.nextPar()\n",
    "    print \"############################################################################################################\"\n",
    "print \"Total amount:\"\n",
    "print \"Fluid sentences: \", total_amount[0]\n",
    "print \"Inverted topic sentences: \", total_amount[1]\n",
    "print \"Out of sync sentences: \", total_amount[2]\n",
    "print \"Disconnected sentences: \", total_amount[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print sent_list[-1].offset_wordset.topic_strong_words\n",
    "# print sent_list[-1].offset_wordset.topic_weak_words\n",
    "# print sent_list[-1].offset_wordset.stress_strong_words\n",
    "# print sent_list[-1].offset_wordset.stress_weak_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
