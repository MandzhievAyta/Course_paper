A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions  This work focuses on the task of generating natural language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al., 1990), which concern surface realization (ordering and inﬂecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of  speciﬁc forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al., 2009) or database entries (Angeli et al., 2010).

While these algorithms usually work well on speciﬁc semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we propose a general probabilistic model that performs generation from underlying formal semantics in the form of typed lambda calculus expressions (we refer to them as λ-expressions throughout this paper), where both lexical acquisition and surface realization are integrated in a single framework.

One natural proposal is to adopt a state-of-the-art statistical machine translation approach. However, unlike text to text translation, which has been extensively studied in the machine translation community, translating from logical forms into text presents additional challenges. Speciﬁcally, logical forms such as λ-expressions may have complex internal structures and variable dependencies across subexpressions. Problems arise when performing automatic acquisition of a translation lexicon, as well as performing lexical selection and surface realization during generation.

In this work, we tackle these challenges by making the following contributions:  • A novel forest-to-string generation algorithm: Inspired by the work of Chiang (2007), we introduce a novel reduction-based weighted binary synchronous context-free grammar formalism for generation from logical forms (λexpressions), which can then be integrated with a probabilistic forest-to-string generation algo • A novel grammar induction algorithm: To automatically induce such synchronous grammar rules, we propose a novel generative model that establishes phrasal correspondences between logical sub-expressions and natural language word sequences, by extending a previous model proposed for parsing natural language into meaning representations (Lu et al., 2008).

To our best knowledge, this is the ﬁrst probabilistic model for generating sentences from the lambda calculus encodings of their underlying formal meaning representations, that concerns both surface realization and lexical acquisition. We demonstrate the effectiveness of our model in Section 5.

The task of language generation from logical forms has a long history. Many early works do not rely on probabilistic approaches. Wang (1980) presented an approach for generation from an extended predicate logic formalism using hand-written rules. Shieber et al. (1990) presented a semantic head-driven approach for generation from logical forms based on rules written in Prolog. Shemtov (1996) presented a system for generation of multiple paraphrases from ambiguous logical forms. Langkilde (2000) presented a probabilistic model for generation from a packed forest meaning representation, without concerning lexical acquisition. Speciﬁcally, we are not aware of any prior work that handles both automatic unsupervised lexical acquisition and surface realization for generation from logical forms in a single framework.
Another line of research efforts focused on the task of language generation from other meaning representation formalisms. Wong and Mooney (2007a) as well as Chen and Mooney (2008) made use of synchronous grammars to transform a variablefree tree-structured meaning representation into sentences. Lu et al. (2009) presented a language generation model using the same meaning representation based on tree conditional random ﬁelds. Angeli et al. (2010) presented a domain-independent probabilistic approach for generation from database entries. All these models are probabilistic models.
Recently there are also substantial research efforts on the task of mapping natural language to meaning  representations in various formalisms – the inverse task of language generation called semantic parsing.
Examples include Zettlemoyer and Collins (2005; 2007; 2009), Kate and Mooney (2006), Wong and Mooney (2007b), Lu et al. (2008), Ge and Mooney (2009), as well as Kwiatkowski et al. (2010).
Of particular interest is our prior work Lu et al.
(2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. One important property of the model in our prior work is that it induces a hybrid tree structure automatically in an unsupervised manner, which reveals the correspondences between natural language word sequences and semantic elements. We extend our prior model in the next section, so as to support λ-expressions.
The model in turn serves as the basis for inducing the synchronous grammar rules later.

In Lu et al. (2008), a generative model was presented to model the process that jointly generates both natural language sentences and their underlying meaning representations of a variable-free treestructured form. The model was deﬁned over a hybrid tree, which consists of meaning representation tokens as internal nodes and natural language words as leaves. One limitation of the hybrid tree model is that it assumes a single ﬁxed tree structure for the meaning representation. However, λexpressions exhibit complex structures and variable dependencies, and thus it is not obvious how to represent them in a single tree structure.
In this section, we present a novel λ-hybrid tree model that provides the following extensions over the model of Lu et al. (2008): 1. The internal nodes of a meaning representation tree involve λ-expressions which are not necessarily of variable-free form; 2. The meaning representation has a packed forest representation, rather than a single deterministic tree structure.

We represent a λ-expression with a packed forest of meaning representation trees (called λ-meaning for est). Multiple different meaning representation trees (called λ-meaning trees) can be extracted from the same λ-meaning forest, but they all convey equivalent semantics via reductions, as discussed next.
Constructing a λ-meaning forest for a given λexpression requires decomposition of a complete λexpression into semantically complete and syntactically correct sub-expressions in a principled manner. This can be achieved with a process called higher order uniﬁcation (Huet, 1975). The process was known to be very complex and was shown to be undecidable in unrestricted form (Huet, 1973). Recently a restricted form of higher order uniﬁcation was applied to a semantic parsing task (Kwiatkowski et al., 2010). In this work, we employ a similar technique for building the λ-meaning forest.
For a given λ-expression e, our algorithm ﬁnds either two expressions h and f such that (h f ) ≡ e, or three expressions h, f , and g such that ((h f ) g) ≡ e, where the symbol ≡ is interpreted as α-equivalent after reductions1 (Barendregt, 1985). We then build the λ-meaning forest based on the expressions h, f , and g. In practice, we develop a BUILDFOREST(e) procedure which recursively builds λ-forests by applying restricted higher-order uniﬁcation rules on top of the λ-expression e. Each node of the λ-forest is called a λ-production, to which we will give more details in Section 3.2. For example, once a candidate triple (h, f, g) as in ((h f ) g) ≡ e has been identiﬁed, the procedure creates a λ-forest with the root node being a λ-production involving h, and two sets of child λ-forests given by BUILDFOREST(f ) and BUILDFOREST(g) respectively. For restricted higher-order uniﬁcation, besides the similar assumptions made by Kwiatkowski et al. (2010), we also impose one additional assumption: limited free variable, which states that the expression h must contain no more than one free variable. Note that this process provides a semantically equivalent packed forest representation of the original λ-expression, without altering its semantics in any way.
For better readability, we introduce the symbol as an alternative notation for functional application. In other words, h f refers to (h f ) or h(f ), and h f g refers to ((h f ) g). For ex 1In this work, for reductions, we consider α-conversions (changing bound variables) and β-conversions (applying functors to their arguments).

ample, the expression λx.state(x) ∧ loc(boston, x) can be represented as the functional application form of [λf.λx.f (x) ∧ loc(boston, x)] λx.state(x).2 Such a packed forest representation contains exponentially many tree structures which all convey the same semantics. We believe such a semantic representation is more advantageous than the single ﬁxed tree-structured representation. In fact, one could intuitively regard a different decomposition path as a different way of interpreting the same semantics. Thus, such a representation could potentially accommodate a wider range of natural language expressions, which all share the same semantics but with very different word choices, phrase orderings, and syntactic structures (like paraphrases).
It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b). We will validate our belief later through experiments.

The generative process for a sentence together with its corresponding λ-meaning tree is illustrated in Figure 1, which results in a λ-hybrid tree. Internal nodes of a λ-hybrid tree are called λ-productions, which are building blocks of a λ-forest. Each λ-production in turn has at most two child λproductions. A λ-production has the form τa : πa τb, where τa is the expected type3 after type evaluation of the terms to its right, πa is a λ-expression (serves as the functor), and τb are types of the child λ-productions (as the arguments). The leave nodes  2Throughout this paper, we abuse this notation a bit by allowing the arguments to be types rather than actual expressions, such as λy.λx.loc(y, x)) e, which indicates that the functor λy.λx.loc(y, x) expects an expression of type e to serve as its argument.
3This work considers basic types: e (entities) and t (truth values). It also allows function types, e.g., he, ti is the type assigned to functions that map from entities to truth values.

he, ti 2 : λf.λg.λx.∃y.g(y) ∧ (f(x) y) he, he, tii 1 he, ti 2  he, he, tii 1 : λy.λx.next to(x, y) he, ti 2 : λg.λf.λx.g(x) ∧ f(x) he, ti 1 he, ti 2  w are contiguous word sequences. The model repeatedly generates λ-hybrid sequences, which consist of words intermixed with λ-productions, from each λ-production at different levels.

Consider part of the example λ-hybrid tree in Figure 2. The probability associated with generation of the subtree that spans the sub-sentence “that the mississippi runs through” can be written as:  P λx.loc(miss r, x), that the mississippi runs through  = φ(m → wYw|p1) × ψ(that e 1 runs through|p1) ×ρ(p2|p1, arg1) × φ(m → w|p2) × ψ(the mississippi|p2)  Following the work of Lu et al. (2008), the generative process involves three types of parameters θ¯ = {φ, ψ, ρ}: 1) pattern parameters φ, which model in what way the words and child λ-productions are intermixed; 2) emission parameters ψ, which model the generation process of words from λ-productions, where either a unigram or a bigram assumption can be made (Lu et al., 2008); and 3) meaning representation (MR) model parameters ρ, which model the generation process from one λ-production to its child λ-productions. An analogous inside-outside algorithm (Baker, 1979) used there is employed here. Since we allow a packed λ-meaning forest representation rather than a ﬁxed tree structure, the MR model parameters ρ in this work should be estimated with the inside-outside algorithm as well, rather than being estimated directly from the training data by simple counting, as was done in Lu et al. (2008).

Now we present the algorithm for language generation. We introduce the grammar ﬁrst, followed by the features we use. Next, we present the method for grammar induction, and then discuss the decoder.

We use a weighted synchronous context free grammar (SCFG) (Aho and Ullman, 1969), which was previously used in Chiang (2007) for hierarchical phrase-based machine translation. The grammar is deﬁned as follows: τ → hpλ , hw, ∼i (1)  where τ is the type associated with the λ-production pλ4, and hw is a sequence consisting of natural language words intermixed with types. The symbol ∼ denotes the one-to-one correspondence between nonterminal occurrences (i.e., in this case types of λ-expressions) in both pλ and hw.
We allow a maximum of two nonterminal symbols in each synchronous rule, as was also assumed in Chiang (2007), which makes the grammar a binary SCFG. Two example rules are:  he, ti → Dλy.λx.loc(y, x) e 1 , that e 1 runs throughE  where the boxed indices give the correspondences between nonterminals.
A derivation with the above two synchronous rules results in the following λ-expression paired with its natural language counterpart:  4Since type is already indicated by τ , we avoid redundancy by omitting it when writing pλ, without loss of information.

Dλg.λf.λx.g(x) ∧ f (x) he, ti 1 he, ti 2 , he, ti 2 he, ti 1 E Dλx.loc(miss r, x) ∧ state(x) , states that the mississippi runs throughE Dλx.loc(miss r, x) , that the mississippi runs throughE  Dλf.λx.state(x) ∧ ∃y.[f (y) ∧ next to(y, x)] he, ti 1 , the states bordering he, ti 1 E Dλy.λx.loc(y, x) ∧ state(x) e 1 , states that e 1 runs throughE  he, ti → Dλx.loc(miss r, x) , that the mississippi runs throughE  where the source side λ-expression is constructed from the application λy.λx.loc(y, x) miss r followed by a reduction (β-conversion). Assuming the λ-expression to be translated is λx.loc(miss r, x), the above rule in fact gives one candidate translation “that the mississippi runs through”.

Following the work of Chiang (2007), we assign scores to derivations with a log-linear model, which are essentially weighted products of feature values.
For generality, we only consider the following four simple features in this work:  1. p˜(hw|pλ): the relative frequency estimate of a hybrid sequence hw given the λ-production pλ; 2. p˜(pλ|hw, τ ): the relative frequency estimate of a λ-production pλ given the phrase hw and the type τ ; 3. exp(−wc(hw)): the number of words generated, where wc(hw) refers to the number of words in hw (i.e., word penalty); and 4. pLM (sˆ): the language model score of the generated sentence sˆ.
The ﬁrst three features, which are also widely used in state-of-the-art machine translation models (Koehn et al., 2003; Chiang, 2007), are rule-speciﬁc and thus can be computed before decoding. The last feature is computed during the decoding phase in combination with the sibling rules used.
We score a derivation D with a log-linear model: !  w(D) = Y Y fi(r)wi × pLM (sˆ)wLM (2) r∈D i where r ∈ D refers to a rule r that appears in the derivation D, sˆ is the target side (sentence) associated with the derivation D, and fi is a rulespeciﬁc feature (one of features 1–3 above) which  is weighted with wi. The language model feature is weighted with wLM .
Once the feature values are computed, our goal is to ﬁnd the optimal weight vector w¯∗ that maximizes a certain evaluation metric when used for decoding, as we will discuss in Section 4.4.
Following popular approaches to learning feature weights in the machine translation community (Och and Ney, 2004; Chiang, 2005), we use the minimum error rate training (MERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Speciﬁcally, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work.

Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996; Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models.
Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the mappings from λ-sub-expressions to (possibly discontiguous) word sequences with a joint generative process. This motivates us to extract grammar rules from the λ-hybrid trees. Thus, we ﬁrst ﬁnd the Viterbi λ-hybrid trees for all training instances,  : (substitution) λy0.hλg.λf.λx.g(x) ∧ f(x) [λy.λx.loc(y, x) y0] λx.state(x)i e 1  based on the learned parameters of the generative λhybrid tree model.

The overall algorithm for learning the grammar rules is sketched in Figure 5.

Next, we extract grammar rules on top of these λ-hybrid trees. Speciﬁcally, we extract the following three types of synchronous grammar rules, with examples given in Figure 3:  1. λ-hybrid sequence rules: They are the conventional rules constructed from one λ-production and its corresponding λ-hybrid sequence.
2. Subtree rules: These rules are constructed from a complete subtree of the λ-hybrid tree. Each rule provides a mapping between a complete sub-expression and a contiguous sub-sentence.
3. Two-level λ-hybrid sequence rules: These rules are constructed from a tree fragment with one of its grandchild subtrees (the subtree rooted by one of its grandchild nodes) being abstracted with its type only. These rules are constructed via substitution and reductions.
Figure 4 gives an example based on a tree fragment of the λ-hybrid tree in Figure 2. Note that the ﬁrst step makes use of the auxiliary variable y0 of type e to represent the grandchild subtree. λy0 is introduced so as to allow any λ-expression of type e serving as this expression’s argument to replace y0. In fact, if the semantics conveyed by the grandchild subtree serves as its argument, we will obtain the exact complete semantics of the current subtree. As we can see, the resulting rule is more general, and is able to capture longer structural dependencies. Such rules are thus potentially more useful.

Our goal in decoding is to ﬁnd the most probable sentence sˆ for a given λ-expression e:  where e(D) refers to the source side (λ-expression) of the derivation D, and s(D) refers to the target side (natural language sentence) of D.

A conventional CKY-style decoder as used by Chiang (2007) is not applicable to this work since the source side does not exhibit a linear structure.
As discussed in Section 3.1, λ-expressions are represented as packed λ-meaning forests. Thus, in this work, we make use of a bottom-up dynamic programming chart-parsing algorithm that works directly on translating forest nodes into target natural language words. The algorithm is similar to that of Langkilde (2000) for generation from an underlying packed semantic forest. Language models are incorporated when scoring the n-best candidates at each forest node, where the cube-pruning algorithm of Chiang (2007) is used. In order to accommodate type 2 and type 3 rules as discussed in Section 4.3, whose source side λ-productions are not present in the nodes of the original λ-meaning forest, new λproductions are created (via substitution and reductions) and attached to the original λ-meaning forest.

• f ← BUILDFOREST(e) It takes in a λ-expression e and outputs its λmeaning forest f . (Sec. 3.1) • θ¯ ← TRAINGENMODEL(f, s) It takes in λ-meaning forest-sentence pairs (f, s), performs EM training of the generative model, and outputs the parameters θ¯. (Sec. 3.2) • h ← FINDHYBRIDTREE(f, s, θ¯) It ﬁnds the most probable λ-hybrid tree h containing the given f -s pair, under the generative model parameters θ¯. (Sec. 4.3) • Γh ← EXTRACTRULES(h) It takes in a λ-hybrid tree h, and extracts a set of grammar rules Γh out of it. (Sec. 4.3)  1. Inputs and initializations: • A training set (e, s), an empty rule set Γ = ∅ 2. Learn the grammar: • For each ei ∈ e, ﬁnd its λ-meaning forest: fi = BUILDFOREST(ei). This gives the set (f, s).

• θL¯∗ea=rnTthReAgIeNnGerEaNtiMveOmDoEdLe(lf,psa)r.ameter :  • For each (fi, si) ∈ (f, s), ﬁnd the most probable λ-hybrid tree hi, and then extract the grammar rules from it: hi = FINDHYBRIDTREE(fi, si, θ¯∗) Γ = Γ ∪ EXTRACTRULES(hi) 3. Output the learned grammar rule set Γ.

For experiments, we evaluated on the GEOQUERY dataset, which consists of 880 queries on U.S. geography. The dataset was manually labeled with λexpressions as their semantics in Zettlemoyer and Collins (2005). It was used in many previous research efforts on semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010).
The original dataset was annotated with English sentences only. In order to assess the generation performance across different languages, in our work the entire dataset was also manually annotated with Chinese by a native Chinese speaker with linguistics background5.
For all the experiments we present in this section, we use the same split as that of Kwiatkowski  5The annotator created annotations with both λ-expressions and corresponding English sentences available as references.

et al. (2010), where 280 instances are used for testing, and the remaining instances are used for learning. We further split the learning set into two portions, where 500 instances are used for training the models, which includes induction of grammar rules, training a language model, and computing feature values, and the remaining 100 instances are used for tuning the feature weights.
As we have mentioned earlier, we are not aware of any previous work that performs generation from formal logical forms that concerns both lexical acquisition and surface realization. The recent work by Angeli et al. (2010) presented a generation system from database records with an additional focus on content selection (selection of records and their subﬁelds for generation). It is not obvious how to adopt their algorithm in our context where content selection is not required but the more complex logical semantic representation is used as input. Other earlier approaches such as the work of Wang (1980) and Shieber et al. (1990) made use of rule-based approaches without automatic lexical acquisition.
We thus compare our system against two stateof-the-art machine translation systems: a phrasebased translation system, implemented in the Moses toolkit (Koehn et al., 2007)6, and a hierarchical phrase-based translation system, implemented in the Joshua toolkit (Li et al., 2009), which is a reimplementation of the original Hiero system (Chiang, 2005; Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments.
We train a trigram language model with modiﬁed Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing MERT.

5.1 Automatic Evaluation For automatic evaluation, we measure the original IBM BLEU score (Papineni et al., 2002) (4-gram precision with brevity penalty) and the TER score (Snover et al., 2006) (the amount of edits required to change a system output into the reference)7. Note that TER measures the translation error rate, thus a  6We used the default settings, and enabled the default lexicalized reordering model, which yielded better performance.
7We used tercom version 0.7.25 with the default settings.

smaller score indicates a better result. For clarity, we report 1−TER scores. Following the tuning procedure as conducted in Galley and Manning (2010), we perform MERT using BLEU as the metric.
We compare our model against state-of-the-art statistical machine translation systems. As a baseline, we ﬁrst conduct an experiment with the following naive approach: we treat the λ-expressions as plain texts. All the bound variables (e.g., x in λx.state(x)) which do not convey semantics are removed, but free variables (e.g., state in λx.state(x)) which might convey semantics are left intact. Quantiﬁers and logical connectives are also left intact. While this naive approach might not appear very sensible, we merely want to treat it as our simplest baseline.
Alternatively, analogous to the work of Wong and Mooney (2007a), we could ﬁrst parse the λexpressions into binary tree structures with a deterministic procedure, and then linearize the tree structure as a sequence. Since there exists different ways to linearize a binary tree, we consider preorder, inorder, and postorder traversal of the trees, and linearize them in these three different ways.
As for our system, during the grammar learning phase, we initialize the generative model parameters with output from the IBM alignment model 1 (Brown et al., 1993)8, and run the λ-hybrid tree generative model with the unigram emission assumption for 10 iterations, followed by another 10 iterations with the bigram assumption. Grammar rules are then extracted based on the λ-hybrid trees obtained from such learned generative model parameters.
Since MERT is prone to search errors, we run each experiment 5 times with randomly initialized feature weights, and report the averaged scores. Experimental results for both English and Chinese are presented in Table 1. As we can observe, the way that a meaning representation tree is linearized has a signiﬁcant impact on the translation performance.
Interestingly, for both Moses and Joshua, the preorder setting yields the best performance for English, whereas it is inorder that yields the best performance for Chinese. This is perhaps due to the fact that Chinese presents a very different syntactic structure and word ordering from English.

8We assume word unigrams are generated from free variables, quantiﬁers, and logical connectives in IBM model 1.

Our system, on the other hand, employs a packed forest representation for λ-expressions. Therefore, it eliminates the ordering constraint by encompassing exponentially many possible tree structures during both the alignment and decoding stage. As a result, our system obtains signiﬁcant improvements in both BLEU and 1−TER using the signiﬁcance test under the paired bootstrap resampling method of Koehn (2004). We obtain p < 0.01 for all cases, except when comparing against Joshua-preorder for English, where we obtain p < 0.05 for both metrics.

text Moses pirneoorrddeerr postorder text Joshua pirneoorrddeerr postorder This work (t) (t) w/o type 2 rules (t) w/o type 3 rules  We also conducted human evaluation with 5 evaluators each on English and Chinese. We randomly selected about 50% (139) test instances and obtained output sentences from the three systems. Moses and Joshua were run with the top-performing settings in terms of automatic metrics (i.e., preorder for English and inorder for Chinese). Following Angeli et al. (2010), evaluators are instructed to give scores based on language ﬂuency and semantic correctness, on the following scale:  For each test instance, we ﬁrst randomly shufﬂed the output sentences of the three systems, and presented them together with the correct reference to the evaluators. The evaluators were then asked to score all the output sentences at once. This evaluation process not only ensures that the annotators have no access to which system generated the out put, but also minimizes bias associated with scoring different outputs for the same input. The detailed and averaged results (with one standard deviation) for human evaluation are presented in Table 2 for English and Chinese respectively. For both languages, our system achieves a signiﬁcant improvement over Moses and Joshua (p < 0.01 with paired t-tests), in terms of both language ﬂuency and semantic correctness. This set of results is important, as it demonstrates that our system produces more ﬂuent texts with more accurate semantics when perceived by real humans.

We also performed the following additional experiments. First, we attempted to increase the number of EM iterations (to 100) when training the model with the bigram assumption, so as to assess the effect of the number of EM iterations on the ﬁnal generation performance. We observed similar performance. Second, in order to assess the importance of the two types of novel rules – subtree rules (type 2) and two-level λ-hybrid sequence rules (type 3), we also conducted experiments without these rules for generation. Experiments show that these two types of rules are important. Speciﬁcally, type 3 rules, which are able to capture longer structural dependencies, are of particular importance for generating Chinese. Detailed results for these additional experiments are presented in Table 1.

Experiments on Variable-free Meaning Representations  Finally, we also assess the effectiveness of our model on an alternative meaning representation formalism in the form of variable-free tree structures.
Speciﬁcally, we tested on the R OBOCUP dataset (Kuhlmann et al., 2004), which consists of 300 English instructions for coaching robots for soc cer games, and a variable-free version of the GEOQUERY dataset. These are the standard datasets used in the generation tasks of Wong and Mooney (2007a) and Lu et al. (2009). Similar to the technique introduced in Kwiatkowski et al. (2010), our proposed algorithm could still be applied to such datasets by writing the tree-structured representations as function-arguments forms. The higher order uniﬁcation-based decomposition algorithm could be applied on top of such forms accordingly. For example, midf ield(opp) ≡ λx.midf ield(x) opp. See Kwiatkowski et al. (2010) for more details. However, since such forms present monotonous structures, and thus give less alternative options in the higher-order uniﬁcation-based decomposition process, it prevents the algorithm from creating many disjunctive nodes in the packed forest. It is thus hypothesized that the advantages of the packed forest representation could not be fully exploited with such a meaning representation formalism.

Following previous works, we performed 4 runs of 10-fold cross validation based on the same split as that of Wong and Mooney (2007a) and Lu et al. (2009), and measured standard BLEU percentage and NIST (Doddington, 2002) scores. For experimentation on each fold, we trained a trigram language model on the training data of that fold, and randomly selected 70% of the training data for grammar induction, with the remaining 30% for learning of the feature weights using MERT.
Next, we performed grammar induction with the complete training data of that fold, and used the learned feature weights for decoding of the test instances. The averaged results are shown in Table 3. Our approach outperforms the previous system WASP−1++ (Wong and Mooney, 2007a) signiﬁcantly, and achieves comparable or slightly better performance as compared to Lu et al. (2009).
This set of results is particularly striking. We note  that the algorithm of Lu et al. (2009) is capable of modeling dependencies over phrases, which gives global optimization over the sentence generated, and works by building conditional random ﬁelds (Lafferty et al., 2001) over trees. But the algorithm of Lu et al. (2009) is also limited to handling treestructured meaning representation, and is therefore unable to accept inputs such as the variable version of λ-expressions. Our algorithm works well by introducing additional new types of synchronous rules that are able to capture longer range dependencies. WASP−1++, on the other hand, also makes use of a synchronous parsing-based statistical machine translation approach. Their system, however, requires linearization of the tree structure for both alignment and translation. In contrast, our model directly performs alignment and translation from a packed forest representation to a sentence. As a result, though WASP−1++ made use of additional features (lexical weights), our system yielded better performance. Sample English output sentences are given in Figure 6.

In this work, we presented a novel algorithm for generating natural language sentences from their under lying semantics in the form of typed lambda calculus. We tackled the problem by introducing a novel reduction-based weighted synchronous context-free grammar formalism, which allows sentence generation with a log-linear model. In addition, we proposed a novel generative model that jointly generates lambda calculus expressions and natural language sentences. The model is then used for automatic grammar induction. Empirical results show that our model outperforms state-of-the-art machine translation models, for both English and Chinese, in terms of both automatic and human evaluation.
Furthermore, we have demonstrated that the model can also effectively handle inputs with a variablefree version of meaning representation.
We believe the algorithm used for inducing the reduction-based synchronous grammar rules may ﬁnd applications in other research problems, such as statistical machine translation and phrasal synchronous grammar induction. We are interested in exploring further along such directions in the future.

This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority (MDA) of Singapore. We would like to thank Tom Kwiatkowski and Luke Zettlemoyer for sharing their dataset, and Omar F. Zaidan for his help with ZMERT.