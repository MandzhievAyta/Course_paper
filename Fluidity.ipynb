{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from nltk.parse import stanford\n",
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tree import ParentedTree\n",
    "from IPython.display import Image, display\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RemoveShortStubs(paragraphs):\n",
    "    startStub = [u'it', u'there']\n",
    "    endStub = u'that'\n",
    "    for par in paragraphs:\n",
    "        for sent in par:\n",
    "            startidx = []\n",
    "            endidx = []\n",
    "            flag = 0\n",
    "            for i, word in enumerate(sent):\n",
    "                if word[0].lower() in startStub:\n",
    "                    flag = 1\n",
    "                    start_candidate = i\n",
    "                if (word[0] == endStub) and (flag == 1):\n",
    "                    flag = 0\n",
    "                    startidx.append(start_candidate)\n",
    "                    endidx.append(i)\n",
    "            for stubidx in reversed(range(len(startidx))):\n",
    "                del sent[startidx[stubidx]: endidx[stubidx] + 1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcess(text):\n",
    "    #Deleting literature references\n",
    "    #[([] -open bracket ( or [;  [^([]*? - lazy; \\d+ - number; [^])]*?; [])] - close bracket ) or ]\n",
    "    print \"regex:\"\n",
    "    time_beg = datetime.datetime.now()\n",
    "    pat = re.compile(r\"[([][^([]*?\\d+[^])]*?[])]\", re.IGNORECASE or re.DOTALL)\n",
    "    text = re.sub(pat, r\"\", text)\n",
    "    text = re.sub(r'([,.:;/)(\"])', r' \\g<1> ', text)\n",
    "    print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    \n",
    "    #Splitting into paragraphs\n",
    "    paragraphs = re.split(u\"\\n\", text)\n",
    "    \n",
    "    #Splitting into sentences\n",
    "    for i, par in enumerate(paragraphs):\n",
    "        paragraphs[i] = tokenize.sent_tokenize(par)\n",
    "    paragraphs_for_tag = []\n",
    "    for par in paragraphs:\n",
    "        paragraphs_for_tag.extend(par)\n",
    "        \n",
    "    #Tagging all words\n",
    "    print \"downloading stanford pos tagger:\"\n",
    "    time_beg = datetime.datetime.now()\n",
    "    st = StanfordPOSTagger('english-left3words-distsim.tagger')\n",
    "    print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    time_beg_tagging = datetime.datetime.now()\n",
    "\n",
    "    for i, par in enumerate(paragraphs_for_tag):\n",
    "        paragraphs_for_tag[i] = paragraphs_for_tag[i].split()\n",
    "    print (datetime.datetime.now() - time_beg_tagging).total_seconds()\n",
    "    paragraphs_for_tag = st.tag_sents(paragraphs_for_tag)\n",
    "    prev_par = 0\n",
    "    for i, par in enumerate(paragraphs):\n",
    "        paragraphs[i] = paragraphs_for_tag[prev_par:prev_par + len(paragraphs[i])]\n",
    "        prev_par += len(paragraphs[i])\n",
    "    for i, par in enumerate(paragraphs):\n",
    "        for sentence in range(len(par)):\n",
    "            paragraphs[i][sentence][:] = [x for x in paragraphs[i][sentence] if x[0] != u'(' and x[0] != u')']\n",
    "    print \"actual tagging:\"\n",
    "    print (datetime.datetime.now() - time_beg_tagging).total_seconds()\n",
    "    #print paragraphs\n",
    "    print \"remove short stubs:\"\n",
    "    time_beg = datetime.datetime.now()\n",
    "    RemoveShortStubs(paragraphs)\n",
    "    print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex:\n",
      "0.008271\n",
      "downloading stanford pos tagger:\n",
      "0.000618\n",
      "0.000807\n",
      "actual tagging:\n",
      "7.004806\n",
      "remove short stubs:\n",
      "0.003477\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(u\"./articles/parsed/1_b.txt\", 'r', 'utf-8') as fin:\n",
    "        text = fin.read()\n",
    "paragraphs = PreProcess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SourceText:\n",
    "    def __init__(self, paragraphs):\n",
    "        self._paragraphs = [x for x in paragraphs if x != []]\n",
    "        self._par_iter = 0\n",
    "        self._sent_iter = 0\n",
    "        \n",
    "    def nextPar(self):\n",
    "        self._par_iter += 1\n",
    "        if self._par_iter == len(self._paragraphs):\n",
    "            return 1\n",
    "        else:\n",
    "            self._sent_iter = 0\n",
    "            return 0\n",
    "        \n",
    "    def nextSent(self):\n",
    "        if self._sent_iter < len(self._paragraphs[self._par_iter]):\n",
    "            self._sent_iter += 1\n",
    "            return self._paragraphs[self._par_iter][self._sent_iter - 1], 0\n",
    "        else:\n",
    "            return [], 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OffsetTopicsAndStresses:\n",
    "    def __init__(self):\n",
    "        self.topic_strong_words = [[], [], []]\n",
    "        self.topic_weak_words = [[], [], []]\n",
    "        self.stress_strong_words = [[], [], []]\n",
    "        self.stress_weak_words = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NOT_APPLICABLE = 0\n",
    "UNKNOWN = 1\n",
    "FLUID = 2\n",
    "INVERTED_TOPIC_CANDIDATE = 3\n",
    "OUT_OF_SYNC = 4\n",
    "INVERTED_TOPIC = 5\n",
    "DISCONNECTED = 6\n",
    "\n",
    "class SentInfo:\n",
    "    _fluid_words = [u\"admittedly\", u\"all in all\", u\"as a result\", u\"because\", u\"conversely\", u\"equally\", u\"finally\",\\\n",
    "               u\"for example\", u\"in a similar\", u\"in contrast\", u\"in summary\", u\"initially\", u\"last\", u\"nevertheless\",\\\n",
    "               u\"once\", u\"so far\", u\"such\", u\"after\", u\"along these lines\", u\"as expected\", u\"before\", u\"curiously\",\\\n",
    "               u\"even though\", u\"first\", u\"for instance\", u\"in a way\", u\"in other words\", u\"in the first\",\\\n",
    "               u\"interestingly\", u\"lastly\", u\"next\", u\"regardless\", u\"specifically\", u\"surprisingly\", u\"afterward\",\\\n",
    "               u\"although\", u\"as soon as\", u\"but\", u\"despite\", u\"eventually\", u\"firstly\", u\"for this reason\",\\\n",
    "               u\"in comparison\", u\"in particular\", u\"in the same way\", u\"it follows\", u\"likewise\", u\"nonetheless\",\\\n",
    "               u\"similarly\", u\"still\", u\"that is why\", u\"again\", u\"as a consequence\", u\"be that as it may\",\\\n",
    "               u\"consequently\", u\"during\", u\"figure\", u\"following\", u\"in a certain sense\", u\"in conclusion\", u\"in short\",\\\n",
    "               u\"indeed\", u\"it is as if\", u\"meanwhile\", u\"now\", u\"so\", u\"subsequently\", u\"the first\", u\"the last\",\\\n",
    "               u\"this\", u\"to elaborate\", u\"the next\", u\"this is why\", u\"to explain\", u\"the reason\", u\"thus\",\\\n",
    "               u\"to illustrate\", u\"then\", u\"to conclude\", u\"to put it another way\", u\"to put it succinctly\",\\\n",
    "               u\"unexpectedly\", u\"while\", u\"to sum up\", u\"until\", u\"while\", u\"to summarize\", u\"up to now\", u\"yet\",\\\n",
    "               u\"ultimately\", u\"whereas\"]\n",
    "    def __init__(self, sentence, type, tree):\n",
    "        self.sent = sentence\n",
    "        self.type = type\n",
    "        self.topic_strong_words = []\n",
    "        self.topic_weak_words = []\n",
    "        self.stress_strong_words = []\n",
    "        self.stress_weak_words = []\n",
    "        self._begins_fluid_words = -1\n",
    "        self.current_offset = 0\n",
    "        #0 ~ Sn-1; 1 ~ Sn-2; 2 ~ Sn-3 \n",
    "        self.offset_wordset = OffsetTopicsAndStresses()\n",
    "        self._tree = next(tree)\n",
    "    \n",
    "    def _deleteNonMainClauses(self, tree):\n",
    "        idxs = tree.treepositions()\n",
    "        for s in list(tree.subtrees(lambda t: t.label() == u\"SBAR\" or t.label() == u\"SBARQ\")):\n",
    "            idx = -1\n",
    "            for x in idxs:\n",
    "                if s == tree[x]:\n",
    "                    idx = x\n",
    "                    break\n",
    "            if idx != -1:\n",
    "                del tree[idx]\n",
    "                idxs = tree.treepositions()\n",
    "        return tree\n",
    "    \n",
    "    def _recursive_search(self, tree, subj_list):\n",
    "        if tree.height() == 2 or (tree.height() == 3 and len(tree.leaves()) == 1):\n",
    "            subj_list.extend(tree.leaves())\n",
    "        else:\n",
    "            for np_idx in range(len(tree)):\n",
    "                l = tree[np_idx].label()\n",
    "                target_tags = [u\"NP\", u\"NN\", u\"NNS\", u\"PRP\", u\"CD\"]\n",
    "                if  l in target_tags:\n",
    "                    self._recursive_search(tree[np_idx], subj_list)    \n",
    "    \n",
    "    def _findSubjects(self, tree):\n",
    "        res = []\n",
    "        for s in list(tree.subtrees(lambda t: t.label() == u\"S\")):\n",
    "            child_labels_list = []\n",
    "            for i in range(len(s)):\n",
    "                child_labels_list.append(s[i].label())\n",
    "            if u\"NP\" in child_labels_list and u\"VP\" in child_labels_list:\n",
    "                for np_idx in range(len(s)):\n",
    "                    if s[np_idx].label() == u\"NP\":\n",
    "                        self._recursive_search(s[np_idx], res)\n",
    "        return res\n",
    "    \n",
    "    def _appearsBeforeFirstPunct(self, idx):\n",
    "        punctuation_marks = [tuple([u\",\", u\",\"]), tuple([u\":\", u\":\"]), tuple([u\";\", u\";\"]), tuple([r'\"', r'``'])]\n",
    "        if ([x for x in punctuation_marks if x in self.sent[:idx]] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _appearsAfterLastPunctOrConj(self, idx):\n",
    "        punctuation_marks = [tuple([u\",\", u\",\"]), tuple([u\":\", u\":\"]), tuple([u\";\", u\";\"]), tuple([r'\"', r'``'])]\n",
    "        conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "        if ([x for x in punctuation_marks if x in self.sent[idx:]] == []) or\\\n",
    "        ([x for x in self.sent[idx:] if x[1] in conj_tags] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isMainClauseContainsTopic(self, main_clause_words):\n",
    "        topic = list(self.topic_strong_words)\n",
    "        topic.extend(self.topic_weak_words)\n",
    "        if ([x for x in topic if not x in main_clause_words] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isNumber(self, word):\n",
    "        numbers = [\"one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen,\\\n",
    "        fifteen, sixteen, seventeen, eighteen, nineteen, twenty, thirty, fourty, fifty, sixty, seventy, eighty, ninety,\\\n",
    "        hundred, thousand, million, billion\"]\n",
    "        if (word in numbers) or (re.match(r\"^[-+]?[0-9]+$\", word) != None):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _appearsAfterConj(self, idx):\n",
    "        conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "        if ([x for x in self.sent[:idx] if x[1] in conj_tags] != []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isStrongStress(self, stress_word, main_clause_words):\n",
    "        noun_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\"]\n",
    "        if stress_word[1] in noun_tags:\n",
    "            idx = self.sent.index(stress_word)\n",
    "            if self._appearsBeforeFirstPunct(idx) or self._appearsAfterLastPunctOrConj(idx):\n",
    "                return True\n",
    "            if (stress_word in main_clause_words) and self._isMainClauseContainsTopic(main_clause_words) and\\\n",
    "            self._appearsAfterConj(idx):\n",
    "                return True\n",
    "            if self._isNumber(self.sent[idx - 1][0]):\n",
    "                return True\n",
    "        #verb derived; stress word in main clause\n",
    "        elif stress_word[0] in main_clause_words:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _addStressWords(self, stress_words, main_clause_words):\n",
    "        for stress_word in stress_words:\n",
    "            if self._isStrongStress(stress_word, main_clause_words):\n",
    "                self.stress_strong_words.append(stress_word[0])\n",
    "            else:\n",
    "                self.stress_weak_words.append(stress_word[0])\n",
    "    \n",
    "    def setDefaultWordSet(self):\n",
    "        self.topic_weak_words = []\n",
    "        self.topic_strong_words = []\n",
    "        self.stress_weak_words = []\n",
    "        self.stress_strong_words = []\n",
    "        tree = self._tree.copy(True)           \n",
    "        #display(tree)\n",
    "        \n",
    "        #Добавляем все слова с метками NN, NNS, NNP, NNPS\n",
    "        #Отличить verb derived nouns в VBG FIX\n",
    "        target_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\", u\"VBG\"]\n",
    "        nounsAndVerbDerivedNouns = []\n",
    "        for tup in self.sent:\n",
    "            if tup[1] in target_tags:\n",
    "                nounsAndVerbDerivedNouns.append(tup)\n",
    "        #print \"nounsAndVerbDerivedNouns: \", nounsAndVerbDerivedNouns, \"\\n\"\n",
    "        \n",
    "        #Удаляем non main clauses\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #Ищем subjects\n",
    "        self.topic_strong_words = self._findSubjects(tree)\n",
    "        stress_words = [x for x in nounsAndVerbDerivedNouns if x[0] not in self.topic_strong_words]\n",
    "        self._addStressWords(stress_words, tree.leaves())\n",
    "        #print \"topic_strong_words(main clause subj): \", self.topic_strong_words, \"\\n\"\n",
    "        #print \"strong stress: \", self.stress_strong_words, \"\\n\"\n",
    "        #print \"weak stress: \", self.stress_weak_words, \"\\n\"\n",
    "        #print \"current sentence: \", self.sent, \"\\n\"\n",
    "    \n",
    "    def addStressWords(self, stress_words, offset):\n",
    "        tree = self._tree.copy(True)\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #print \"stress_words:\"\n",
    "        #print stress_words\n",
    "        for stress_word in stress_words:\n",
    "            if self._isStrongStress(stress_word, tree.leaves()):\n",
    "                self.offset_wordset.stress_strong_words[offset - 1].append(stress_word[0])\n",
    "            else:\n",
    "                self.offset_wordset.stress_weak_words[offset - 1].append(stress_word[0])\n",
    "    \n",
    "    def getMainClauseSubjects(self):\n",
    "        tree = self._tree.copy(True)\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #display(tree)\n",
    "        return self._findSubjects(tree) \n",
    "    \n",
    "    def getMainClauseWords(self):\n",
    "        tree = self._tree.copy(True)\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        return tree.leaves()\n",
    "        \n",
    "    def beginsWithFluidWords(self):\n",
    "        if self._begins_fluid_words != -1:\n",
    "            return self._begins_fluid_words\n",
    "        verb_tags = [u\"VB\", u\"VBD\", u\"VBN\", u\"VBP\", u\"VBZ\"]\n",
    "        ordinal_numbers = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\",\\\n",
    "                           \"tenth\", \"eleventh\", \"twelfth\", \"thirteenth\", \"fourteenth\", \"fifteenth\", \"sixteenth\",\\\n",
    "                           \"seventeenth\", \"eighteenth\", \"nineteenth\", \"twentieth\", \"thirtieth\", \"fortieth\", \"fiftieth\",\\\n",
    "                           \"sixtieth\", \"seventieth\", \"eightieth\", \"ninetieth\", \"hundredth\", \"thousandth\"]\n",
    "        pronouns_tags = [u\"PRP$\", u\"PRP\"]\n",
    "        patt = re.compile(r\"([A-Z]+\\))|(\\([A-Z]+\\))|([1-9]+\\))|(\\([1-9]+\\))|([1-9]*1st)|([1-9]*2nd)|([1-9]*3rd)|([1-9]*[4-9]th)|([1-9]+0th)\")\n",
    "        for word in self.sent:\n",
    "            if word[1] in verb_tags:\n",
    "                break\n",
    "            #FIX некоторые FLUID WORDS состоят из двух и более слов, а я рассматриваю только по одному\n",
    "            if (word[0].lower() in self._fluid_words) or (word[1] in pronouns_tags) or\\\n",
    "            (re.search(patt, word[0]) != None) or (word[0].lower() in ordinal_numbers):\n",
    "                self.beginsWithFluidWords = 1\n",
    "            else:\n",
    "                self.beginsWithFluidWords = 0\n",
    "        return self.beginsWithFluidWords\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#проверка алгоритма(поиск defaultWordSet для каждого предложения)\n",
    "# state = 0\n",
    "# text = SourceText(paragraphs)\n",
    "# sent_list = []\n",
    "\n",
    "# while state == 0:\n",
    "#     print state\n",
    "#     sent, p = text.nextSent()\n",
    "#     while p != 1:\n",
    "#         sent_list.append(SentInfo(sent, UNKNOWN))\n",
    "#         sent_list[-1].setDefaultWordSet()\n",
    "#         sent, p = text.nextSent()\n",
    "#     state = text.nextPar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STRONG_TOPIC = 10\n",
    "WEAK_TOPIC = 11\n",
    "\n",
    "def BetweenFluidOrInverted(sent_list, offset):\n",
    "    #check Sn-1\n",
    "    if sent_list[-2].type != FLUID and sent_list[-2].type != INVERTED_TOPIC_CANDIDATE:\n",
    "        return False\n",
    "    #check Sn-2\n",
    "    if sent_list[-3].type != FLUID and sent_list[-3].type != INVERTED_TOPIC_CANDIDATE:\n",
    "        return False\n",
    "    if offset == 3:\n",
    "        if sent_list[-4].type != FLUID and sent_list[-4].type != INVERTED_TOPIC_CANDIDATE:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def TopicFound(topic_words, topic_type, sentence, previous_sentence, reached_verb, offset, sent_list):\n",
    "#     print \"trying to find topic!\", topic_words, topic_type\n",
    "    if offset == 1:\n",
    "        if not reached_verb:\n",
    "            sentence.type = FLUID\n",
    "        else:\n",
    "            sentence.type = INVERTED_TOPIC_CANDIDATE\n",
    "    else:\n",
    "        if not reached_verb:\n",
    "            if BetweenFluidOrInverted(sent_list, offset):\n",
    "                sentence.type = FLUID\n",
    "            else:\n",
    "                sentence.type = OUT_OF_SYNC\n",
    "    if topic_type == WEAK_TOPIC:\n",
    "        if not set(topic_words).issubset(set(sentence.offset_wordset.topic_strong_words[offset - 1])):\n",
    "            sentence.offset_wordset.topic_weak_words[offset - 1].extend(topic_words)\n",
    "    elif topic_type == STRONG_TOPIC:\n",
    "        sentence.offset_wordset.topic_strong_words[offset - 1].extend(topic_words)\n",
    "                 \n",
    "def CheckSentenceMainClauses(sentence, previous_sentence, sent_list):\n",
    "    offset = sentence.current_offset\n",
    "    main_clause_subjects = sentence.getMainClauseSubjects()\n",
    "    prev_sent_wordset = list(previous_sentence.topic_weak_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.topic_strong_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.stress_strong_words)\n",
    "#     print \"prev_sent_wordset: \", prev_sent_wordset\n",
    "#     print \"main clause subjects: \", main_clause_subjects\n",
    "    matched_words = [x for x in main_clause_subjects if x in prev_sent_wordset]\n",
    "#     print \"found strong topic! Give me sec to check: \", matched_words\n",
    "    if matched_words != []:\n",
    "        TopicFound(matched_words, STRONG_TOPIC, sentence, previous_sentence, False, offset, sent_list)\n",
    "    #FIX какие именно subjects имеются в виду?\n",
    "    stress_words = [x for x in main_clause_subjects if x not in matched_words]\n",
    "    sentence.addStressWords([(x, u\"\") for x in stress_words], offset)\n",
    "    \n",
    "def CheckWholeSentence(sentence, previous_sentence, sent_list):\n",
    "    offset = sentence.current_offset\n",
    "    reached_verb = False\n",
    "    reached_topic_or_main = False\n",
    "    prev_sent_wordset = list(previous_sentence.topic_weak_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.topic_strong_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.stress_strong_words)\n",
    "    main_clause_words = sentence.getMainClauseWords()\n",
    "#     print \"prev_sent_wordset: \", prev_sent_wordset\n",
    "#     print \"main clause words: \", main_clause_words\n",
    "    conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    noun_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\"]\n",
    "    \n",
    "    for word in sentence.sent:\n",
    "        reached_verb = word[1] in conj_tags or reached_verb\n",
    "        reached_topic_or_main = word[0] in main_clause_words or reached_topic_or_main\n",
    "#         print word, reached_verb, reached_topic_or_main\n",
    "        if not reached_verb:\n",
    "            matches = word[0] in prev_sent_wordset\n",
    "            if (matches and word[1] in noun_tags) or (matches and reached_topic_or_main and (word[1] == u\"VBG\")):\n",
    "                TopicFound([word[0]], WEAK_TOPIC, sentence, previous_sentence, False, offset, sent_list)\n",
    "            elif (word[1] == u\"VBG\"):\n",
    "                sentence.addStressWords([word], offset)\n",
    "        else:\n",
    "            if (sentence.offset_wordset.topic_strong_words[offset - 1] != []) or (sentence.offset_wordset.topic_weak_words[offset - 1] != []):\n",
    "                if word[1] in noun_tags:\n",
    "                    sentence.addStressWords([word], offset)\n",
    "            else:\n",
    "                matches = word[0] in prev_sent_wordset\n",
    "                if (word[1] in noun_tags) and matches:\n",
    "#                     print \"special for recognition!!!\", [word[0]]\n",
    "                    TopicFound([word[0]], WEAK_TOPIC, sentence, previous_sentence, True, offset, sent_list)\n",
    "                    reached_topic_or_main = True\n",
    "                elif (word[1] == u\"VBG\") and matches:\n",
    "                    sentence.addStressWords([word], offset)\n",
    "                    \n",
    "def CheckSentenceProgression(sentence, previous_sentence, sent_list):\n",
    "    sentence.current_offset += 1\n",
    "    CheckSentenceMainClauses(sentence, previous_sentence, sent_list)\n",
    "    CheckWholeSentence(sentence, previous_sentence, sent_list)\n",
    "    \n",
    "def DefineResults(sentence, total_amount):\n",
    "    if sentence.type == UNKNOWN:\n",
    "        sentence.type = DISCONNECTED\n",
    "        sentence.setDefaultWordSet()\n",
    "    else:\n",
    "        if sentence.type == INVERTED_TOPIC:\n",
    "            word_set_from_round = 1\n",
    "        elif sentence.type in [FLUID, OUT_OF_SYNC]:\n",
    "            word_set_from_round = sentence.current_offset\n",
    "        sentence.topic_strong_words = sentence.offset_wordset.topic_strong_words[word_set_from_round - 1]\n",
    "        sentence.topic_weak_words = sentence.offset_wordset.topic_weak_words[word_set_from_round - 1]\n",
    "        sentence.stress_strong_words = sentence.offset_wordset.stress_strong_words[word_set_from_round - 1]\n",
    "        sentence.stress_weak_words = sentence.offset_wordset.stress_weak_words[word_set_from_round - 1]\n",
    "    if sentence.type == FLUID:\n",
    "        total_amount[0] += 1\n",
    "    elif sentence.type == INVERTED_TOPIC:\n",
    "        total_amount[1] += 1\n",
    "    elif sentence.type == OUT_OF_SYNC:\n",
    "        total_amount[2] +=1\n",
    "    elif sentence.type == DISCONNECTED:\n",
    "        total_amount[3] += 1\n",
    "        \n",
    "def TypeToString(type):\n",
    "    if type == FLUID:\n",
    "        return \"Fluid\"\n",
    "    elif type == INVERTED_TOPIC:\n",
    "        return \"Inverted topic\"\n",
    "    elif type == OUT_OF_SYNC:\n",
    "        return \"Out if sync\"\n",
    "    elif type == DISCONNECTED:\n",
    "        return \"Disconnected\"\n",
    "    elif type == NOT_APPLICABLE:\n",
    "        return \"Not applicable\"\n",
    "    \n",
    "def PrintSentInfo(sent):\n",
    "    print \"--------------------------------------------------------------------------------------------------------\"\n",
    "    print \"\\nSentence: \", sent.sent\n",
    "    print \"Topic:\"\n",
    "    print \"Strong: \", sent.topic_strong_words\n",
    "    print \"Weak  : \", sent.topic_weak_words\n",
    "    print \"Stress:\"\n",
    "    print \"Strong: \", sent.stress_strong_words\n",
    "    print \"Weak  : \", sent.stress_weak_words\n",
    "    print \"\\nType: \", TypeToString(sent_list[-1].type)\n",
    "    print \"\\nWordsets from checkups with Sn-1, Sn-2, Sn-3:\"\n",
    "    print \"Topic:\"\n",
    "    print \"Strong: \", sent.offset_wordset.topic_strong_words\n",
    "    print \"Weak: \", sent.offset_wordset.topic_weak_words\n",
    "    print \"Stress:\"\n",
    "    print \"Strong: \", sent.offset_wordset.stress_strong_words\n",
    "    print \"Weak: \", sent.offset_wordset.stress_weak_words\n",
    "    print \"--------------------------------------------------------------------------------------------------------\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build trees time:\n",
      "317.023608\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.00063\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000895\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.008863\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000309\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000539\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.001576\n",
      "setDefaultWordSet:\n",
      "0.002332\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000869\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000287\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000295\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.005189\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.00133\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.0012\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000859\n",
      "############################################################################################################\n",
      "Total amount:\n",
      "Fluid sentences:  35\n",
      "Inverted topic sentences:  21\n",
      "Out of sync sentences:  0\n",
      "Disconnected sentences:  78\n"
     ]
    }
   ],
   "source": [
    "total_amount = [0, 0, 0, 0] #fluid, inverted_topic, out_of_sync, disconnected\n",
    "state = 0\n",
    "text = SourceText(paragraphs)\n",
    "\n",
    "plain_paragraphs = []\n",
    "for par in paragraphs:\n",
    "    plain_paragraphs.extend(par)\n",
    "parser = stanford.StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "print \"build trees time:\"\n",
    "time_beg = datetime.datetime.now()\n",
    "all_trees = parser.tagged_parse_sents(plain_paragraphs)\n",
    "print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "\n",
    "while state == 0:\n",
    "    print \"\\nNext Paragraph:\"\n",
    "    sent_list = []\n",
    "    sent, p = text.nextSent()\n",
    "    if sent == []:\n",
    "        text.nextPar()\n",
    "        continue\n",
    "    sent_list.append(SentInfo(sent, NOT_APPLICABLE, next(all_trees)))\n",
    "    sent_list[-1].setDefaultWordSet()\n",
    "    #PrintSentInfo(sent_list[-1])\n",
    "    sent, p = text.nextSent()\n",
    "    while p == 0:\n",
    "        sent_list.append(SentInfo(sent, UNKNOWN, next(all_trees)))\n",
    "        if sent_list[-1].beginsWithFluidWords():\n",
    "            sent_list[-1].type = FLUID\n",
    "            print \"setDefaultWordSet:\"\n",
    "            time_beg = datetime.datetime.now()\n",
    "            sent_list[-1].setDefaultWordSet()\n",
    "            print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "        else:\n",
    "            CheckSentenceProgression(sent_list[-1], sent_list[-2], sent_list)\n",
    "            if sent_list[-1].type == INVERTED_TOPIC_CANDIDATE:\n",
    "                sent_list[-1].type = INVERTED_TOPIC\n",
    "            elif sent_list[-1].type == UNKNOWN:\n",
    "                if len(sent_list) > 2:\n",
    "                    for offset in range(2):\n",
    "                        CheckSentenceProgression(sent_list[-1], sent_list[-2], sent_list)\n",
    "                        if sent_list[-1].type != UNKNOWN or len(sent_list) < 4:\n",
    "                            break\n",
    "            DefineResults(sent_list[-1], total_amount)\n",
    "        #PrintSentInfo(sent_list[-1])\n",
    "        sent, p = text.nextSent()\n",
    "    state = text.nextPar()\n",
    "    print \"############################################################################################################\"\n",
    "print \"Total amount:\"\n",
    "print \"Fluid sentences: \", total_amount[0]\n",
    "print \"Inverted topic sentences: \", total_amount[1]\n",
    "print \"Out of sync sentences: \", total_amount[2]\n",
    "print \"Disconnected sentences: \", total_amount[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
