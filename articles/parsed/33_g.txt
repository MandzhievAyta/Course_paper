Boosting Combinatorial Search Through Randomization  The time required by complete search methods to solve similar combinatorial problems can be surprisingly variable. Two problem instances may be identical, except for the order in which the variables are numbered. A particular complete search algorithm may solve the ﬁrst in seconds, yet require hours or days to solve the second. Even for a single problem instance, a seemingly trivial change in a detail of the search algorithm may drastically alter the solution time. In addition, in many domains there are problem instances that can be quickly solved by incomplete methods, but apparently cannot be solved by complete methods, even methods guided by powerful heuristics.
This unpredictability in the running time of a complete algorithm undermines one of the main reasons that one may choose to employ such a method, namely the desire for a guarantee that the algorithm will determine whether or not each problem instance in fact has a solution. It is desirable, therefore, to ﬁnd ways to improve the robustness and predictability of these algorithms.

This paper discusses a general technique for improving complete search methods by introducing a controlled amount of randomization. The technique actually takes advantage of the variability of the underlying search method in order to ﬁnd solutions more quickly and with less variance in solution time. We demonstrate the effectiveness of this strategy on SAT and CSP algorithms, in the domains of logistics planning, circuit synthesis, and roundrobin scheduling. Solutions times are reduced by an order of magnitude or more, and some instances are solved for the ﬁrst time by a method other than local search.
We will show that the unpredictability in running times for combinatorial algorithms can often be explained by a phenomenon called a “heavy-tailed cost distribution” (Gomes et al. 1998a). In our preliminary experiments, we plotted the solution times for a deterministic search algorithm running on a random distribution of scheduling problem instances. We noticed that at any time during the experiment there was a non-negligible probability of hitting a problem that required exponentially more time to solve than any that had been encountered before. This so-called “heavy-tail” phenomena causes the mean solution time to increase with the length of the experiment, and to be inﬁnite in the limit.
Previous authors have noted the occurrence of seemingly exceptionally hard problems in ﬁxed problem distributions (Gent and Walsh 1994; Smith and Grant 1996). However, we further discovered that when a small amount of randomization was introduced into the heuristic used by the search algorithm, then, on some runs, the instances were solved quickly. Thus, the “hardness” did not reside in the instances, but rather in the combination of the instance with the details of the deterministic algorithm. When we plotted the solution times for many runs of the randomized complete algorithm (with different random seeds) on a single problem instance, we discovered the same heavy-tailed distribution as we had seen before on a collection of instances.
This observation led us to realize that a deterministic search algorithm can be viewed as a single run of a randomized algorithm. The unpredictability of deterministic,  logistics.d 3bit-adder-32 3bit-adder-31 round-robin 14 round-robin 16 round-robin 18 block-world.d  Deterministic soln. time 108 min > 24 hrs > 24 hrs 411 sec > 24 hrs > 48 hrs 30 min  complete algorithms is thus explained by the variance one would expect in any one run of a randomized algorithm.
Furthermore, by analyzing the shape of the cost distribution we developed simple techniques that provably reduce the mean solution time.
For our experiments, we used known hard problem instances from scheduling, planning, and circuit synthesis, and a state of-the-art satisﬁability engine (Satz, by Li and Anbulagan (1997)), and a highly efﬁcient CSP solver built using the ILOG C++ constraint programming library (Puget and Leconte 1995). It is important to note that in both cases the underlying deterministic complete search engines are among the fastest (and on many problems, the fastest) in their class. Thus, the techniques discussed in this paper extend the range of complete methods to problems that were previously beyond their reach. For a preview of our main results, see Table 1. The table shows how our randomization strategy enabled us to solve several previously unsolved problem instances, and other instances were solved up to an order of magnitude faster.
Given the techniques’ simplicity and generality, our approach can be easily adapted to improve the performance of other backtrack-style search methods used in planning, scheduling, and other tasks of interest to AI.

Our problem domains are timetable scheduling, planning, and circuit synthesis. The ﬁrst is formalized as a CSP problem, and the latter two as propositional satisﬁability.
Timetabling consists in determining whether there exists a feasible schedule that takes into consideration a set of pairing and distribution constraints. More speciﬁcally, we consider problems derived from sports scheduling applications. The literature in this area is growing, and one can begin to get a sense of the range and mathematical difﬁculty of the problems encountered (McAloon et al. 1997; Nemhauser and Trick 1997; and Schreuder 1992). Here we consider the timetabling problem for the classic “roundrobin” schedule: every team must play every other team  exactly once. The problem is formally deﬁned as follows: 1. There are N teams (N even) and every two teams play each other exactly once.
2. The season lasts N − 1 weeks.
3. Every team plays one game in each week of the season.
4. There are N=2 periods and, each week, every period is scheduled for one game.
5. No team plays more than twice in the same period over the course of the season.
Up to 8-team problems are relatively simple and can be done by brute force. However, the combinatorics of this scheduling problem are explosive. For an N team league, there are N=2 (N −1) matchups (i; j) with 0 i < j < N to be played. A schedule can be thought of as a permutation of these matchups. So, for N teams the search space size is (N=2 (N − 1))!, i.e., the search space size grows as the factorial of the square of N=2. Published algorithms for this problem all scale poorly, and the times for our deterministic solver (as shown in Table 1) are among the best (see also Gomes et al. 1998b).
The second domain is planning. Kautz and Selman (1996) showed that propositional SAT encodings of difﬁcult STRIPS-style planning problems could be efﬁciently solved by SAT engines. While both a complete backtrackstyle engine and an incomplete local-search engine worked well on moderate-sized problems, the largest problems from the domain of logistics scheduling could only be solved by local search. However, it turns out that the deterministic version of Satz can solve all of the logistics instances from that paper in less than 2 minutes. Therefore we constructed a still-larger planning problem, labeled “logistics.d”. This domain involves moving packages on trucks and airplanes between different locations in different cities.
While the largest logistics problem from the Kautz and Selman (1996) paper involved 1,141 variables, “logistics.d” involves 2,160 variables.
The ﬁnal domain is circuit synthesis. Kamath et al.
(1993) developed a technique for expressing the problem of synthesizing a programmable logic array (PLA) as a propositional satisﬁable problem. The statement of the problem  includes a table specifying the function to be computed, and an upper-bound on the number of gates that may appear in the circuit. In general, these problems become more difﬁcult to solve as the number of gates is reduced, until the limit is reached where the instance is unsatisﬁable. These problems are quite hard to solve with complete SAT procedures, and have been used as part of the test-beds for numerous SAT competitions and research studies. The problems considered in this paper, “3bit-adder-32” and “3bit-adder31” are (as one would guess) based on synthesizing a 3-bit adder using 32 and 31 gates respectively. Although Selman and Kautz (1993) solve the instances using local search, no one has previously solved either using a backtrack-style procedure.

We now consider general techniques for adding randomization to complete, systematic, backtrack-style search procedures. Such a procedure constructs a solution incrementally. At each step a heuristic is used to select an operation to be applied to a partial solution, such as assigning a value to an unassigned variable. Eventually either a complete solution is found, or the algorithm determines that the current partial solution is inconsistent. In the latter case, the algorithm backtracks to an earlier point in its search tree.
If several choices are heuristically determined to be equally good, then a deterministic algorithm applies some ﬁxed rule to pick one of the operations; for example, to select the lowest-numbered variable to assign. The most obvious place to apply randomization, therefore, is in this step of tie-breaking: if several choices are ranked equally, choose among them at random. Even this simple modiﬁcation can dramatically change the behavior of a search algorithm, as we will see in the section on CSP below.
However, if the heuristic function is particular powerful, it may rarely assign more than one choice the highest score. To handle this, we can introduce a “heuristic equivalence” parameter to the algorithm. Setting the parameter to a value H greater than zero means all choices who receive scores within H-percent of the highest score are considered equally good. This expands the choice-set for random tiebreaking.
With these changes each run of the search algorithm on a particular instance will differ in the order in which choices are made and potentially in time to solution. As we will discuss in detail below, it can be advantageous to terminate searches which appear to be “stuck”, exploring a part of the space far from a solution. Therefore we will also introduce a “cutoff” parameter, that limits search to a speciﬁed number of backtracks. When the cutoff is reached, the algorithm is restarted at the root of the search tree.
We should note that introducing randomness in the branching variable selection does not effect the complete ness of the backtrack-style search. Some basic bookkeeping (only linear space) ensures that the procedures do not revisit any previously explored part of the search space, which means that we can still determine inconsistencies, unlike local search methods. The cutoff parameter does limit the size of the space that can be searched exhaustively between restarts. In practice, we gradually increase the cutoff, to allow us to determine inconsistencies, if necessary.

A variable-order randomization and restart strategy was employed in Crawford and Baker’s (1994) “probing” algorithm for SAT. Despite the fact that it performed no backtracking at all, it was shown to solve a number of examples.
Even though, the “power of randomization” in combinatorial search has been informally recognized by others (for recent work in scheduling domains, see e.g., Bresina 1996 and Oddi and Smith 1997), our work provides the ﬁrst explanation for the potential success of this kind of strategy, in terms of heavy-tailed distributions (Gomes et al. 1998a).
As we will see, our data also shows that there is often a clear optimal cutoff value; simply probing down with unit propagation but no backtracking can be ineffective. For example, in Table 3 we have a 0% success rate for a cutoff value of 2. More recently, Bayardo and Schrag (1997) introduced a backtrack-style solver, rel-sat, that included randomized tie-breaking and restarts, but with only a ﬁxed, high cutoff value. The focus of that work was on the backtracking technique, rather than the effect of restarts.

The ﬁrst complete search algorithm we randomized was a CSP solver. ILOG SOLVER is a powerful C++ constraint programming library (Puget and Leconte 1995). For the round-robin scheduling problems discussed below, we used the library to build a deterministic, backtrack-style CSP engine. (See Dechter (1991) and Freuder and Mackworth (1994) for an overview of basic CSP algorithms.) It employs the ﬁrst-fail heuristic for variable assignment, which selects the variables with the smallest domain ﬁrst; ties are broken lexicographically. The performance of this deterministic version already matches or exceeds all the published results on solving these types of problems. We then randomized the solver by breaking ties randomly, and adding a cutoff parameter (Gomes et al. 1998b).

The second algorithm we randomized was for propositional satisﬁability. One of the fastest complete search engines for propositional satisﬁability testing is the Satz system of Li and Anbulagan (1997). Satz is a version of the Davis-Putnam-Loveland procedure (Davis et al. 1962), with a heuristic based on choosing a branch variable that maximizes a function of the number of the unit propagations performed when it is set positively or negatively.
Satz is the fastest deterministic SAT procedure we have found for the instances discussed in this paper. It can often solve smaller instances of these types with less than 100 backtracks. Because its heuristic usually chooses a single  branching variable without ties, we added a heuristic equivalence parameter to enlarge the choice-set.

In previous work (Gomes et al. 1998a), we show that the tail behavior of randomized complete backtrack style methods is often best modeled using distributions which asymptotically have tails of the Pareto-Le´vy form, viz.

where > 0 is a constant (Mandelbrot 1960; and Samorodnitsky 1994). These are heavy-tailed distributions, i.e., distributions whose tails have a power law decay. The constant is called the index of stability of the distribution. For < 2, moments of X of order less than are ﬁnite while all higher order moments are inﬁnite, i.e., = supfa > 0 : EjX ja < 1g. For example, when = 1:5, the distribution has a ﬁnite mean but no ﬁnite variance. With = 0:6, the distribution has neither a ﬁnite mean nor a ﬁnite variance.
If a Pareto-Le´vy tail is observed, then the rate of decrease of the distribution is a power law. (Standard distributions exhibit exponential decay.) From (1), we have 1 − F (x) = Pr fX > xg C:x− , so the complementto-one of the cumulative distribution, F (x), also decays according to a power law. Given the power law decay of the complement-to-one of the cumulative distribution of a heavy-tailed random variable, its log-log plot should show an approximately linear decrease in the tail. Moreover, the slope of the observed linear decrease provides an estimate of the index .

F(x), for our 12 team round-robin problem. The linear nature of the tail in this plot directly reveals heavy-tails of the Pareto-Le´vy type.
To complement our visual check of heavy-tailed behavior of Figure 1, we calculate the maximum likelihood estimate of the index of stability (the value of ): For our roundrobin scheduling problem, for N = 12, we obtain = 0:7, which is consistent with the hypothesis of inﬁnite mean and inﬁnite variance, since 1.1 So far, we have identiﬁed heavy-tailed behavior of the cost distribution to the right of the median. The heavy tail nature shows that there is a computationally signiﬁcant fraction of very long runs, decaying only at a polynomial rate. The strategy of running the search procedure with a cutoff near the median value of the distribution clearly avoids these long runs in the tail.
However, our experiments in Gomes (1998a) also suggest a heavy tail phenomenon on the left-hand side of the median value of the cost distribution, which means that the success rate for a solution only increases polynomially with the number of backtracks. This explains how a relatively low cutoff value still gives a sufﬁciently high success rate to allow us to solve a problem instance. For example, for our round-robin scheduling problems with N = 16, we observed several runs that took less than 200 backtracks, compared to a median value of around 2,000,000. For N = 18, we ran with a cutoff of 500; 000 and solved the instance after 20 tries. Each try took about 1 hour, and the successful run took 350,632 backtracks.
Tails on the left are also characterized by an index of stability. Based on our data (Gomes 1998a), we conjecture that for the tail on the left is less than 1.0 on hard combinatorial search problems. This conjecture has strong implications in terms of algorithm design: It means that in order to obtain the minimal expected run time, a preferred strategy consists of relatively short runs of a randomized backtrack-style procedure.
We do not wish to give the impression that every search problem gives rise to a heavy-tailed distribution. In fact, doing so would give rise to the suspicion that the distributions we found were an artifact of our methodology, rather than a real phenomena of the problem domain! One domain in which we have not found heavy-tails is on blocksworld planning problems. The hardest blocks-world problem from Kautz and Selman (1996) is blocks-world.d, and it can be solved by deterministic Satz in 30 minutes. We ran the randomized version of Satz on this instance at a wide range of cutoff values and heuristic equivalence settings.

1Of course, the computational cost of complete backtrack-style algorithms has a ﬁnite upper-bound. However, since we are dealing with NP-complete problems, this upper-bound is exponential in the size of the problem, which means that de facto, for realisticsize hard instances, it can be treated as inﬁnite for practical purposes: no practical procedure can explore the search full space.

The optimal equivalence parameter setting was 30%. However, over a range of cutoff values, there was no evidence of a heavy-tailed distribution, and, therefore, randomization only slightly increases the effectiveness of Satz: the mean cost is 23 minutes. Further studies are needed to determine exactly what characteristics of combinatorial search problems lead to heavy-tailed behavior.

So far, we have discussed how heavy-tailed probability distributions underlie the large variability observed when running a randomized backtrack-style procedure on a variety of problem instances. We can obtain more efﬁcient and more predictable procedures by running the search up to a certain cutoff point and then restarting at the root of the tree.
Restarts clearly prevent the procedure from getting trapped in the long tails on the right of the distribution. In addition, a very low cutoff value can also be used to exploit the heavy-tails to the left of the median, and will allow us to solve previously unsolved problem instances after a sufﬁcient number of restarts. In Table 1, the mean solution times in the “Randomized” column are based on empirically determined near-optimal cutoff values. For each randomized solution time the standard deviation is of the same order of magnitude as the mean. This is to be expected because the distribution is geometric, as will be shown in the next section. Without restarts, of course, the variance and mean tend to inﬁnity to a ﬁrst approximation.
We will now discuss these results in more detail.
Our deterministic CSP procedure on the round-robin scheduling problem gives us us a solution for N = 14 in about 411 seconds. (Experiments ran on a 200MHz SGI Challenge.) We could not ﬁnd a solution for N = 16 and N = 18. Apparently, the problem quickly becomes very difﬁcult, even for moderate values of N . The subtle interaction between global and local constraints makes the search for a globally consistent solution surprisingly hard.
For problems for which we can empirically determine the overall cost proﬁle, we can calculate an optimal cutoff value to minimize the expected cost of ﬁnding a solution. Our main interest, however, is in solving previously unsolved instances, such as the N = 16 and N = 18 case. These problems are too hard to obtain a full cost distribution. For example, for N = 16, running with a cutoff of 1,000,000 gives a success rate of less than 40%, so we do not even reach the median point of the distribution. Each run takes about 2 hours to complete. (We estimate that the median value is around 2,000,000. Our deterministic procedure apparently results in a run that still lies to the right of the expected median cost.) In order to ﬁnd a good cutoff value for very hard problem instances, the best available strategy is a trial-and-error process, where one experiments with vari succ.
rate 0.0 0.00003 0.0016 0.009 0.014 0.018 0.14 0.32  ous cutoff values, starting at relatively low values, since the optimal cutoff for these problems tends to lie below the median value of the distribution. This can be seen from Table 2, which gives the expected cost (backtracks) for ﬁnding a solution for N = 16 for a range of cutoff values. The optimal cutoff is around 5:104, resulting in an expected cost per solution of 7:105 backtracks ( 1.4 hrs). For the N = 18 case, we ran with a cutoff of 5:105, and found a solution after approximately 22 hours.2 Table 3 gives the performance of Satz for a range of cutoff values on the logistics.d instance. Again, there is a clear optimal value: In this case, it’s surprisingly low, 16 backtracks. Despite the low success rate (less than 1%) at this cutoff value, the overall performance is close to optimal here, requiring around 1,800 backtracks total per solution, which takes around 95 seconds. Compare this with the 108 minutes for the deterministic version of Satz. It’s important to note that the 108 minutes run is not just an “unlucky” determinist run. Given the shape of the underlying heavytailed distribution, most runs take more than 100,000 backtracks (over 1 hour). The trick is to exploit the fact that we  2Since the submission of this paper, a lot of progress has been made in terms of solving larger instances ( McAloon et al. in preparation). By using multiple threads on a 14 processor Sun system, 26 and 28 teams schedules were generated, which is the record as of this writing (Wetzel and Zabatta, 1998). We believe these numbers can be improved upon with our randomization technique.

have a non-negligible probably of solving the instance in a very short run. Our fast restart strategy exploits this.
See Table 1 for other improvements due to randomization. Until now, the 3bit-adder problems had not been solved by any backtrack-style procedure. On the blockworld problem, we obtain little improvement, which can be attributed to the absence of heavy-tails as discussed above.
These results show that introducing a stochastic element into a backtrack-style search procedure, combined with an appropriate restart strategy, can signiﬁcantly enhance the procedure’s performance. In fact, as we see here, it allows us to solve previously unsolved problem instances.

In this section we formalize the strategy of restarts S of a complete stochastic procedure A. We derive the probability distribution of S assuming the full knowledge of the probability distribution of A. We demonstrate that the probability distribution associated with S does not exhibit heavy tails.
Furthermore, S has a ﬁnite mean and variance, even if the stochastic procedure A has an inﬁnite mean and variance.
Let us consider a complete stochastic procedure and associate with it the random variable A, where A is the number of backtracks that it takes to ﬁnd a solution or prove that it does not exist. Let us now consider the following stochastic strategy for running A: run A for a ﬁxed number of backtracks c (the cutoff); if A ﬁnds a solution or proves it does not exist, then our stochastic strategy has also found a solution (or proved that it does not exist) and it stops. Otherwise, restart A from the beginning, using an independent random seed, for another c backtracks, and so on. Deﬁne S as the number of backtracks that the stochastic strategy of restarts of A with cutoff c takes to ﬁnd a solution or prove that it does not exist. Let’s assume that we know P[A c], i.e., the probability that the stochastic procedure A will ﬁnd a solution or prove that it does not exist in no more than c backtracks. The sequence of runs of A executed by our restart strategy are independent, and therefore they can be seen as a sequence of Bernoulli trials, in which the success consists in ﬁnding a solution (or proving that it doesn’t exist) before the end of the run.
It’s convenient to also deﬁne a random variable R, giving the number of restarts until a solution is found (or the instance is shown inconsistent). Note that R = dS=ce. R follows a geometric distribution with parameter p = P [A c]. The probability of the tail of S, P [S > s], is given by  Taking into consideration that R = dS=ce and that it follows a geometric distribution (exponential decay; ﬁnite mean and variance), it follows that the tail of the distribution of S also exhibits exponential decay and S has a ﬁnite mean and variance.

We should emphasize that when adopting a low cutoff the strategy of restarts partially eliminates the heavy tail on the left: the lower the cutoff, the shorter the tail. This is true since the distribution of S exhibits exponential decay for S >cutoff.

Building on our previous work on heavy-tailed behavior in combinatorial search (Gomes et al. 1998a), we have shown that performance of complete, backtrack-style search algorithms on hard real-world problems can be greatly enhanced by the addition of randomization combined with a rapid restart strategy. Speedups of several orders of magnitude were observed, and some test problem instances were solved for the ﬁrst time by any backtrack-style procedure.
The success of our approach is based on exploiting the heavy-tailed nature of the cost distributions. We saw that in most of the domains we found that “outliers” on both sides of the median occur with a relatively high frequency.
Heavy-tails to the right of the median cause the mean solution time to grow without bounds. Adding cutoffs and restarts to the search algorithm, however, both theoretically and empirically eliminate the heavy-tail and bound the mean. Heavy-tails to the left of the mean can be exploited by performing many rapid restarts with short runs, leading to a further dramatic decrease in expected solution time.
We applied the randomization techniques to two stateof-the-art search engines for CSP and propositional satisﬁability. We were able to solve hard round-robin scheduling instances of up to size 18, when the corresponding deterministic version could only handle instances up to size 14.
In the domain of planning as satisﬁability, we extended the range of logistics problems that could be solved by complete methods from problems containing 1,141 variables to ones involving 2,160 variables (solved with mean cost of 95 seconds).
It would be interesting to explore our randomization approach in context of other backtrack-style approaches, such as dynamic backtracking (Ginsberg 1993). We believe that the generality of the approach will lead to further advances in planning, scheduling, diagnosis, game-playing, and other areas of AI.

Acknowledgments The ﬁrst author is sponsored by the Air Force Research Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-98-1-0008 and the Air Force Ofﬁce of Scientiﬁc Research, under the New World Vistas Initiative, AFOSR NWV project 2304, LIRL 97RL005N25. The second author is supported by an NSF Faculty Early Career Development Award. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied,  of the Air Force Research Laboratory or the U.S. Government.