Over the last decade, deep convolutional neural networks (CNNs) have revolutionized supervised learning for tasks such as object recognition, action recognition, and semantic segmentation [3, 15, 6, 19]. Recently, CNNs have been applied to reinforcement learning (RL) tasks with visual observations such as Atari games [21], robotic manipulation [18], and imitation learning (IL) [9]. In these tasks, a neural network (NN) is trained to represent a policy – a mapping from an observation of the system’s state to an action, with the goal of representing a control strategy that has good long-term behavior, typically quantiﬁed as the minimization of a sequence of time-dependent costs.

The sequential nature of decision making in RL is inherently different than the one-step decisions in supervised learning, and in general requires some form of planning [2]. However, most recent deep RL works [21, 18, 9] employed NN architectures that are very similar to the standard networks used in supervised learning tasks, which typically consist of CNNs for feature extraction, and fully connected layers that map the features to a probability distribution over actions. Such networks are inherently reactive, and in particular, lack explicit planning computation. The success of reactive policies in sequential problems is due to the learning algorithm, which essentially trains a reactive policy to select actions that have good long-term consequences in its training domain.

To understand why planning can nevertheless be an important ingredient in a policy, consider the grid-world navigation task depicted in Figure 1 (left), in which the agent can observe a map of its domain, and is required to navigate between some obstacles to a target position. One hopes that after training a policy to solve several instances of this problem with different obstacle conﬁgurations, the policy would generalize to solve a different, unseen domain, as in Figure 1 (right). However, as we show in our experiments, while standard CNN-based networks can be easily trained to solve a set of such maps, they do not generalize well to new tasks outside this set, because they do not understand the goal-directed nature of the behavior. This observation suggests that the computation learned by reactive policies is different from planning, which is required to solve a new task1.

1In principle, with enough training data that covers all possible task conﬁgurations, and a rich enough policy representation, a reactive policy can learn to map each task to its optimal policy. In practice, this is often too expensive, and we offer a more data-efﬁcient approach by exploiting a ﬂexible prior about the planning computation underlying the behavior.

In this work, we propose a NN-based policy that can effectively learn to plan. Our model, termed a value-iteration network (VIN), has a differentiable ‘planning program’ embedded within the NN structure.

The key to our approach is an observation that the classic value-iteration (VI) planning algorithm [1, 2] may be represented by a speciﬁc type of CNN. By embedding such a VI network Figure 1: Two instances of a grid-world domain.
module inside a standard feed-forward classiﬁ- Task is to move to the goal between the obstacles.
cation network, we obtain a NN model that can learn the parameters of a planning computation that yields useful predictions. The VI block is differentiable, and the whole network can be trained using standard backpropagation. This makes our policy simple to train using standard RL and IL algorithms, and straightforward to integrate with NNs for perception and control.

Connections between planning algorithms and recurrent NNs were previously explored by Ilin et al. [12]. Our work builds on related ideas, but results in a more broadly applicable policy representation. Our approach is different from model-based RL [25, 4], which requires system identiﬁcation to map the observations to a dynamics model, which is then solved for a policy. In many applications, including robotic manipulation and locomotion, accurate system identiﬁcation is difﬁcult, and modelling errors can severely degrade the policy performance. In such domains, a model-free approach is often preferred [18]. Since a VIN is just a NN policy, it can be trained model free, without requiring explicit system identiﬁcation. In addition, the effects of modelling errors in VINs can be mitigated by training the network end-to-end, similarly to the methods in [13, 11].

We demonstrate the effectiveness of VINs within standard RL and IL algorithms in various problems, among which require visual perception, continuous control, and also natural language based decision making in the WebNav challenge [23]. After training, the policy learns to map an observation to a planning computation relevant for the task, and generate action predictions based on the resulting plan. As we demonstrate, this leads to policies that generalize better to new, unseen, task instances.

In this section we provide background on planning, value iteration, CNNs, and policy representations for RL and IL. In the sequel, we shall show that CNNs can implement a particular form of planning computation similar to the value iteration algorithm, which can then be used as a policy for RL or IL.

Value Iteration: A standard model for sequential decision making and planning is the Markov decision process (MDP) [1, 2]. An MDP M consists of states s 2 S, actions a 2 A, a reward function R(s; a), and a transition kernel P (s0js; a) that encodes the probability of the next state given the current state and action. A policy (ajs) prescribes an action distribution for each state. The goal in an MDP is to ﬁnd a policy that obtains high rewards in the long term. Formally, the value V (s) eoxfeacsuttaitnegupnodleircypoli,cVy (iss)t=h:e Eexp[ePctet1=d0distcro(usntt;eadt)sjusm0 o=f rse]w;warhdesrwehe2n s(t0ar;t1in)gisfraomdistchoautnsttaftaecatnodr, and E denotes an expectation over trajectories of states and actions (s0; a0; s1; a1 : : : ), in which actions are selected according to ,: and states evolve according to the transition kernel P (s0js; a).
The optimal value function V (s) = max V (s) is the maximal long-term return possible from a state. A policy is said to be optimal if V (s) = V (s) 8s. A popular algorithm for calculating V and is value iteration (VI):  Vn+1(s) = maxa Qn(s; a) 8s; where Qn(s; a) = R(s; a) + Ps0 P (s0js; a)Vn(s0): (1) It is well known that the value function Vn in VI converges as n ! 1 to V , from which an optimal policy may be derived as (s) = arg maxa Q1(s; a).

Convolutional Neural Networks (CNNs) are NNs with a particular architecture that has proved useful for computer vision, among other domains [8, 16, 3, 15]. A CNN is comprised of stacked convolution and max-pooling layers. The input to each convolution layer is a 3dimensional signal X, typically, an image with l channels, m horizontal pixels, and n vertical pixels, and its output h is a l0-channel convolution of the image with kernels W 1; : : : ; W l0 , hl0;i0;j0 = Pl;i;j Wll;0i;j Xl;i0 i;j0 j ; where is some scalar activation function. A max-pooling  layer selects, for each channel l and pixel i; j in h, the maximum value among its neighbors N (i; j),  maxpool = maxi0;j02N(i;j) hl;i0;j0 : Typically, the neighbors N (i; j) are chosen as a k hl;i;j  patch around pixel i; j. After max-pooling, the image is down-sampled by a constant factor d, commonly 2 or 4, resulting in an output signal with l0 channels, m=d horizontal pixels, and n=d vertical pixels. CNNs are typically trained using stochastic gradient descent (SGD), with backpropagation for computing gradients.

Reinforcement Learning and Imitation Learning: In MDPs where the state space is very large or continuous, or when the MDP transitions or rewards are not known in advance, planning algorithms cannot be applied. In these cases, a policy can be learned from either expert supervision – IL, or by trial and error – RL. While the learning algorithms in both cases are different, the policy representations – which are the focus of this work – are similar. Additionally, most state-of-the-art algorithms such as [24, 21, 26, 18] are agnostic to the policy representation, and only require it to be differentiable, for performing gradient descent on some algorithm-speciﬁc loss function. Therefore, in this paper we do not commit to a speciﬁc learning algorithm, and only consider the policy.

Let (s) denote an observation for state s. The policy is speciﬁed as a parametrized function (aj (s)) mapping observations to a probability over actions, where are the policy parameters.
For example, the policy could be represented as a neural network, with denoting the network weights. The goal is to tune the parameters such that the policy behaves well in the sense that (aj (s)) (aj (s)), where is the optimal policy for the MDP, as deﬁned in Section 2.

of N state observations and ( (si)) i=1;:::;N is generated by an expert.

corresponding optimal actions Learning a policy then becomes  an instance of supervised learning [24, 9]. In RL, the optimal action is not available, but instead, the agent can act in the world and observe the rewards and state transitions its actions effect. RL algorithms such as in [27, 21, 26, 18] use these observations to improve the value of the policy.

In this section we introduce a general policy representation that embeds an explicit planning module.
As stated earlier, the motivation for such a representation is that a natural solution to many tasks, such as the path planning described above, involves planning on some model of the domain.

Let M denote the MDP of the domain for which we design our policy . We assume that there is some unknown MDP M such that the optimal plan in M contains useful information about the optimal policy in the original task M . However, we emphasize that we do not assume to know M in advance. Our idea is to equip the policy with the ability to learn and solve M , and to add the solution of M as an element in the policy . We hypothesize that this will lead to a policy that automatically learns a useful M to plan on. We denote by s 2 S, a 2 A, R(s; a), and P (s0js; a) the states, actions, rewards, and transitions in M . To facilitate a connection between M and M , we let R and P depend on the observation in M , namely, R = fR( (s)) and P = fP ( (s)), and we will later learn the functions fR and fP as a part of the policy learning process.

For example, in the grid-world domain described above, we can let M have the same state and action spaces as the true grid-world M . The reward function fR can map an image of the domain to a high reward at the goal, and negative reward near an obstacle, while fP can encode deterministic movements in the grid-world that do not depend on the observation. While these rewards and transitions are not necessarily the true rewards and transitions in the task, an optimal plan in M will still follow a trajectory that avoids obstacles and reaches the goal, similarly to the optimal plan in M .

Once an MDP M has been speciﬁed, any standard planning algorithm can be used to obtain the value function V . In the next section, we shall show that using a particular implementation of VI for planning has the advantage of being differentiable, and simple to implement within a NN framework.
In this section however, we focus on how to use the planning result V within the NN policy . Our approach is based on two important observations. The ﬁrst is that the vector of values V (s) 8s encodes all the information about the optimal plan in M . Thus, adding the vector V as additional features to the policy is sufﬁcient for extracting information about the optimal plan in M .

However, an additional property of V is that the optimal decision (s) at a state s can depend only on a subset of the values of V , since (s) = arg maxa R(s; a) + Ps0 P (s0js; a)V (s0).
Therefore, if the MDP has a local connectivity structure, such as in the grid-world example above, the states for which P (s0js; a) > 0 is a small subset of S.

In NN terminology, this is a form of attention [31], in the sense that for a given label prediction (action), only a subset of the input features (value function) is relevant. Attention is known to improve learning performance by reducing the effective number of network parameters during learning.
Therefore, the second element in our network is an attention module that outputs a vector of (attention  modulated) values (s). Finally, the vector (s) is added as additional features to a reactive policy re(aj (s); (s)). The full network architecture is depicted in Figure 2 (left).

Returning to our grid-world example, at a particular state s, the reactive policy only needs to query the values of the states neighboring s in order to select the correct action. Thus, the attention module in this case could return a (s) vector with a subset of V for these neighboring states.

Let denote all the parameters of the policy, namely, the parameters of fR, fP , and re, and note that (s) is in fact a function of (s). Therefore, the policy can be written in the form (aj (s)), similarly to the standard policy form (cf. Section 2). If we could back-propagate through this function, then potentially we could train the policy using standard RL and IL algorithms, just like any other standard policy representation. While it is easy to design functions fR and fP that are differentiable (and we provide several examples in our experiments), back-propagating the gradient through the planning algorithm is not trivial. In the following, we propose a novel interpretation of an approximate VI algorithm as a particular form of a CNN. This allows us to conveniently treat the planning module as just another NN, and by back-propagating through it, we can train the whole policy end-to-end.

We now introduce the VI module – a NN that encodes a differentiable planning computation.
Our starting point is the VI algorithm (1). Our main observation is that each iteration of VI may be seen as passing the previous value function Vn and reward function R through a convolution layer and max-pooling layer. In this analogy, each channel in the convolution layer corresponds to the Q-function for a speciﬁc action, and convolution kernel weights correspond to the discounted transition probabilities. Thus by recurrently applying a convolution layer K times, K iterations of VI are effectively performed.

Following this idea, we propose the VI network module, as depicted in Figure 2B. The inputs to the VI module is a ‘reward image’ R of dimensions l; m; n, where here, for the purpose of clarity, we follow the CNN formulation and explicitly assume that the state space S maps to a 2-dimensional grid. However, our approach can be extended to general discrete state spaces, for example, a graph, as we report in the WikiNav experiment in Section 4.4. The reward is fed into a convolutional layer Q with A channels and a linear activation function, Qa;i0;j0 = Pl;i;j Wla;i;j Rl;i0 i;j0 j : Each channel  in this layer corresponds to Q(s; a) for a particular action a. This layer is then max-pooled along the actions channel to produce the next-iteration value function layer V , Vi;j = maxa Q(a; i; j): The next-iteration value function layer V is then stacked with the reward R, and fed back into the convolutional layer and max-pooling layer K times, to perform K iterations of value iteration.

The VI module is simply a NN architecture that has the capability of performing an approximate VI computation. Nevertheless, representing VI in this form makes learning the MDP parameters and reward function natural – by backpropagating through the network, similarly to a standard CNN. VI modules can also be composed hierarchically, by treating the value of one VI module as additional input to another VI module. We further report on this idea in the supplementary material.

We now have all the ingredients for a differentiable planning-based policy, which we term a value iteration network (VIN). The VIN is based on the general planning-based policy deﬁned above, with the VI module as the planning algorithm. In order to implement a VIN, one has to specify the state  and action spaces for the planning module S and A, the reward and transition functions fR and fP , and the attention function; we refer to this as the VIN design. For some tasks, as we show in our experiments, it is relatively straightforward to select a suitable design, while other tasks may require more thought. However, we emphasize an important point: the reward, transitions, and attention can be deﬁned by parametric functions, and trained with the whole policy2. Thus, a rough design can be speciﬁed, and then ﬁne-tuned by end-to-end training.

Once a VIN design is chosen, implementing the VIN is straightforward, as it is simply a form of a CNN. The networks in our experiments all required only several lines of Theano [28] code. In the next section, we evaluate VIN policies on various domains, showing that by learning to plan, they achieve a better generalization capability.

In this section we evaluate VINs as policy representations on various domains. Additional experiments investigating RL and hierarchical VINs, as well as technical implementation details are discussed in the supplementary material. Source code is available at https://github.com/avivt/VIN.
Our goal in these experiments is to investigate the following questions:  1. Can VINs effectively learn a planning computation using standard RL and IL algorithms?  2. Does the planning computation learned by VINs make them better than reactive policies at generalizing to new domains?  An additional goal is to point out several ideas for designing VINs for various tasks. While this is not an exhaustive list that ﬁts all domains, we hope that it will motivate creative designs in future work.

4.1 Grid-World Domain Our ﬁrst experiment domain is a synthetic grid-world with randomly placed obstacles, in which the observation includes the position of the agent, and also an image of the map of obstacles and goal position. Figure 3 shows two random instances of such a grid-world of size 16 16. We conjecture that by learning the optimal policy for several instances of this domain, a VIN policy would learn the planning computation required to solve a new, unseen, task.

In such a simple domain, an optimal policy can easily be calculated using exact VI. Note, however, that here we are interested in evaluating whether a NN policy, trained using RL or IL, can learn to plan. In the following results, policies were trained using IL, by standard supervised learning from demonstrations of the optimal policy. In the supplementary material, we report additional RL experiments that show similar ﬁndings.

We design a VIN for this task following the guidelines described above, where the planning MDP M is a grid-world, similar to the true MDP. The reward mapping fR is a CNN mapping the image input to a reward map in the grid-world. Thus, fR should potentially learn to discriminate between obstacles, non-obstacles and the goal, and assign a suitable reward to each. The transitions P were deﬁned as 3 3 convolution kernels in the VI block, exploiting the fact that transitions in the grid-world are local3. The recurrence K was chosen in proportion to the grid-world size, to ensure that information can ﬂow from the goal state to any other state. For the attention module, we chose a trivial approach that selects the Q values in the VI block for the current state, i.e., (s) = Q(s; ). The ﬁnal reactive policy is a fully connected network that maps (s) to a probability over actions.

We compare VINs to the following NN reactive policies:  CNN network: We devised a CNN-based reactive policy inspired by the recent impressive results of DQN [21], with 5 convolution layers, and a fully connected output. While the network in [21] was trained to predict Q values, our network outputs a probability over actions. These terms are related, since (s) = arg maxa Q(s; a). Fully Convolutional Network (FCN): The problem setting for this domain is similar to semantic segmentation [19], in which each pixel in the image is assigned a semantic label (the action in our case). We therefore devised an FCN inspired by a state-of-the-art semantic segmentation algorithm [19], with 3 convolution layers, where the ﬁrst layer has a ﬁlter that spans the whole image, to properly convey information from the goal to every other state.

In Table 1 we present the average 0 1 prediction loss of each model, evaluated on a held-out test-set of maps with random obstacles, goals, and initial states, for different problem sizes. In addition, for each map, a full trajectory from the initial state was predicted, by iteratively rolling-out the next-states  2VINs are fundamentally different than inverse RL methods [22], where transitions are required to be known.
3Note that the transitions deﬁned this way do not depend on the state s. Interestingly, we shall see that the network learned to plan successful trajectories nevertheless, by appropriately shaping the reward.

predicted by the network. A trajectory was said to succeed if it reached the goal without hitting obstacles. For each trajectory that succeeded, we also measured its difference in length from the optimal trajectory. The average difference and the average success rate are reported in Table 1.

Clearly, VIN policies generalize to domains outside the training set. A visualization of the reward mapping fR (see supplementary material) shows that it is negative at obstacles, positive at the goal, and a small negative constant otherwise. The resulting value function has a gradient pointing towards a direction to the goal around obstacles, thus a useful planning computation was learned. VINs also signiﬁcantly outperform the reactive networks, and the performance gap increases dramatically with the problem size. Importantly, note that the prediction loss for the reactive policies is comparable to the VINs, although their success rate is signiﬁcantly worse. This shows that this is not a standard case of overﬁtting/underﬁtting of the reactive policies. Rather, VIN policies, by their VI structure, focus prediction errors on less important parts of the trajectory, while reactive policies do not make this distinction, and learn the easily predictable parts of the trajectory yet fail on the complete task.

The VINs have an effective depth of K, which is larger than the depth of the reactive policies. One may wonder, whether any deep enough network would learn to plan. In principle, a CNN or FCN of depth K has the potential to perform the same computation as a VIN. However, it has much more parameters, requiring much more training data. We evaluate this by untying the weights in the K recurrent layers in the VIN. Our results, reported in the supplementary material, show that untying the weights degrades performance, with a stronger effect for smaller sizes of training data.

In this experiment we show that VINs can learn to plan from natural image input. We demonstrate this on path-planning from overhead terrain images of a Mars landscape.

Each domain is represented by a 128 128 image patch, on which we deﬁned a 16 16 grid-world, where each state was considered an obstacle if the terrain in its corresponding 8 8 image patch contained an elevation angle of 10 degrees or more, evaluated using an external elevation data base.
An example of the domain and terrain image is depicted in Figure 3. The MDP for shortest-path planning in this case is similar to the grid-world domain of Section 4.1, and the VIN design was similar, only with a deeper CNN in the reward mapping fR for processing the image.

The policy was trained to predict the shortest-path directly from the terrain image. We emphasize that the elevation data is not part of the input, and must be inferred (if needed) from the terrain image.

4.3 Continuous Control We now consider a 2D path planning domain with continuous states and continuous actions, which cannot be solved using VI, and therefore a VIN cannot be naively applied. Instead, we will construct the VIN to perform ‘high-level’ planning on a discrete, coarse, grid-world representation of the continuous domain. We shall show that a VIN can learn to plan such a ‘highlevel’ plan, and also exploit that plan within its ‘low-level’ continuous control policy. Moreover, the VIN policy results in better generalization than a reactive policy.

After training, VIN achieved a success rate of 84:8%. To put this rate in context, we compare with the best performance achievable without access to the elevation data, which is 90:3%. To make this comparison, we trained a CNN to classify whether an 8 8 patch is an obstacle or not. This classiﬁer was trained using the same image data as the VIN network, but its labels were the true obstacle classiﬁcations from the elevation map (we reiterate that the VIN did not have access to these ground-truth obstacle labels during training or testing). The success rate of planner that uses the obstacle map generated by this classiﬁer from the raw image is 90:3%, showing that obstacle identiﬁcation from the raw image is indeed challenging. Thus, the success rate of the VIN, which was trained without any obstacle labels, and had to ‘ﬁgure out’ the planning process is quite remarkable.

Consider the domain in Figure 4. A red-colored Figure 4: Continuous control domain. Top: averparticle needs to be navigated to a green goal us- age distance to goal on training and test domains ing horizontal and vertical forces. Gray-colored for VIN and CNN policies. Bottom: trajectories obstacles are randomly positioned in the domain, predicted by VIN and CNN on test domains.
and apply an elastic force and friction when contacted. This domain presents a non-trivial control problem, as the agent needs to both plan a feasible trajectory between the obstacles (or use them to bounce off), but also control the particle (which has mass and inertia) to follow it. The state observation consists of the particle’s continuous position and velocity, and a static 16 16 downscaled image of the obstacles and goal position in the domain. In principle, such an observation is sufﬁcient to devise a ‘rough plan’ for the particle to follow.

As in our previous experiments, we investigate whether a policy trained on several instances of this domain with different start state, goal, and obstacle positions, would generalize to an unseen domain.

For training we chose the guided policy search (GPS) algorithm with unknown dynamics [17], which is suitable for learning policies for continuous dynamics with contacts, and we used the publicly available GPS code [7], and Mujoco [29] for physical simulation. We generated 200 random training instances, and evaluate our performance on 40 different test instances from the same distribution.

Our VIN design is similar to the grid-world cases, with some important modiﬁcations: the attention module selects a 5 5 patch of the value V , centered around the current (discretized) position in the map. The ﬁnal reactive policy is a 3-layer fully connected network, with a 2-dimensional continuous output for the controls. In addition, due to the limited number of training domains, we pre-trained the VIN with transition weights that correspond to discounted grid-world transitions. This is a reasonable prior for the weights in a 2-d task, and we emphasize that even with this initialization, the initial value function is meaningless, since the reward map fR is not yet learned. We compare with a CNN-based reactive policy inspired by the state-of-the-art results in [21, 20], with 2 CNN layers for image processing, followed by a 3-layer fully connected network similar to the VIN reactive policy.

In the previous experiments, the planning aspect of the task corresponded to 2D navigation. We now consider a more general domain: WebNav [23] – a language based search task on a graph.

In WebNav [23], the agent needs to navigate the links of a website towards a goal web-page, speciﬁed by a short 4-sentence query. At each state s (web-page), the agent can observe average wordembedding features of the state (s) and possible next states (s0) (linked pages), and the features of the query (q), and based on that has to select which link to follow. In [23], the search was performed  on the Wikipedia website. Here, we report experiments on the ‘Wikipedia for Schools’ website, a simpliﬁed Wikipedia designed for children, with over 6000 pages and at most 292 links per page.

In [23], a NN-based policy was proposed, which ﬁrst learns a NN mapping from ( (s); (q)) to a hidden state vector h. The action is then selected according to (s0j (s); (q)) / exp h> (s0) . In essence, this policy is reactive, and relies on the word embedding features at each state to contain meaningful information about the path to the goal. Indeed, this property naturally holds for an encyclopedic website that is structured as a tree of categories, sub-categories, sub-sub-categories, etc.

We sought to explore whether planning, based on a VIN, can lead to better performance in this task, with the intuition that a plan on a simpliﬁed model of the website can help guide the reactive policy in difﬁcult queries. Therefore, we designed a VIN that plans on a small subset of the graph that contains only the 1st and 2nd level categories (< 3% of the graph), and their word-embedding features.

Designing this VIN requires a different approach from the grid-world VINs described earlier, where the most challenging aspect is to deﬁne a meaningful mapping between nodes in the true graph and nodes in the smaller VIN graph. For the reward mapping fR, we chose a weighted similarity measure between the query features (q), and the features of nodes in the small graph (s). Thus, intuitively, nodes that are similar to the query should have high reward. The transitions were ﬁxed based on the graph connectivity of the smaller VIN graph, which is known, though different from the true graph.
The attention module was also based on a weighted similarity measure between the features of the possible next states (s0) and the features of each node in the simpliﬁed graph (s). The reactive policy part of the VIN was similar to the policy of [23] described above. Note that by training such a VIN end-to-end, we are effectively learning how to exploit the small graph for doing better planning on the true, large graph.

Both the VIN policy and the baseline reactive policy were trained by supervised learning, on random trajectories that start from the root node of the graph. Similarly to [23], a policy is said to succeed a query if all the correct predictions along the path are within its top-4 predictions.

After training, the VIN policy performed mildly better than the baseline on 2000 held-out test queries when starting from the root node, achieving 1030 successful runs vs. 1025 for the baseline. However, when we tested the policies on a harder task of starting from a random position in the graph, VINs signiﬁcantly outperformed the baseline, achieving 346 successful runs vs. 304 for the baseline, out of 4000 test queries. These results conﬁrm that indeed, when navigating a tree of categories from the root up, the features at each state contain meaningful information about the path to the goal, making a reactive policy sufﬁcient. However, when starting the navigation from a different state, a reactive policy may fail to understand that it needs to ﬁrst go back to the root and switch to a different branch in the tree. Our results indicate such a strategy can be better represented by a VIN.

We remark that there is still room for further improvements of the WebNav results, e.g., by better models for reward and attention functions, and better word-embedding representations of text.

The introduction of powerful and scalable RL methods has opened up a range of new problems for deep learning. However, few recent works investigate policy architectures that are speciﬁcally tailored for planning under uncertainty, and current RL theory and benchmarks rarely investigate the generalization properties of a trained policy [27, 21, 5]. This work takes a step in this direction, by exploring better generalizing policy representations.

Our VIN policies learn an approximate planning computation relevant for solving the task, and we have shown that such a computation leads to better generalization in a diverse set of tasks, ranging from simple gridworlds that are amenable to value iteration, to continuous control, and even to navigation of Wikipedia links. In future work we intend to learn different planning computations, based on simulation [10], or optimal linear control [30], and combine them with reactive policies, to potentially develop RL solutions for task and motion planning [14].

This research was funded in part by Siemens, by ONR through a PECASE award, by the Army Research Ofﬁce through the MAST program, and by an NSF CAREER grant. A. T. was partially funded by the Viterbi Scholarship, Technion. Y. W. was partially funded by a DARPA PPAML program, contract FA8750-14-C-0011.