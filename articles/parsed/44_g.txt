Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates  Verbal and nominal semantic role labeling (SRL) have been studied independently of each other (Carreras and Ma`rquez, 2005; Gerber et al., 2009) as well as jointly (Surdeanu et al., 2008; Hajicˇ et al., 2009). These studies have demonstrated the maturity of SRL within an evaluation setting that restricts the argument search space to the sentence containing the predicate of interest. However, as shown by the following example from the Penn TreeBank (Marcus et al., 1993), this restriction excludes extra-sentential arguments:  (1) [arg0 The two companies] [pred produce] [arg1 market pulp, containerboard and white paper]. The goods could be manufactured closer to customers, saving [pred shipping] costs.

The ﬁrst sentence in Example 1 includes the PropBank (Kingsbury et al., 2002) analysis of the verbal predicate produce, where arg0 is the agentive  producer and arg1 is the produced entity. The second sentence contains an instance of the nominal predicate shipping that is not associated with arguments in NomBank (Meyers, 2007).
From the sentences in Example 1, the reader can infer that The two companies refers to the agents (arg0) of the shipping predicate. The reader can also infer that market pulp, containerboard and white paper refers to the shipped entities (arg1 of shipping).1 These extra-sentential arguments have not been annotated for the shipping predicate and cannot be identiﬁed by a system that restricts the argument search space to the sentence containing the predicate. NomBank also ignores many within-sentence arguments. This is shown in the second sentence of Example 1, where The goods can be interpreted as the arg1 of shipping.
These examples demonstrate the presence of arguments that are not included in NomBank and cannot easily be identiﬁed by systems trained on the resource. We refer to these arguments as implicit.
This paper presents our study of implicit arguments for nominal predicates. We began our study by annotating implicit arguments for a select group of predicates. For these predicates, we found that implicit arguments add 65% to the existing role coverage of NomBank.2 This increase has implications for tasks (e.g., question answering, information extraction, and summarization) that beneﬁt from semantic analysis. Using our annotations, we constructed a feature-based model for automatic implicit argument identiﬁcation that uniﬁes standard verbal and nominal SRL. Our results indicate a 59% relative (15-point absolute) gain in F1 over an informed baseline. Our analyses highlight strengths and weaknesses of the approach, providing insights for future work on this emerging task.

1In PropBank and NomBank, the interpretation of each role (e.g., arg0) is speciﬁc to a predicate sense.
2Role coverage indicates the percentage of roles ﬁlled.

In the following section, we review related research, which is historically sparse but recently gaining traction. We present our annotation effort in Section 3, and follow with our implicit argument identiﬁcation model in Section 4. In Section 5, we describe the evaluation setting and present our experimental results. We analyze these results in Section 6 and conclude in Section 7.

Palmer et al. (1986) made one of the earliest attempts to automatically recover extra-sentential arguments. Their approach used a ﬁne-grained domain model to assess the compatibility of candidate arguments and the slots needing to be ﬁlled.
A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting in systems that are able to identify missing case-marked expressions in the surrounding discourse (Imamura et al., 2009). Sasano et al. (2004) conducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English).
Fillmore and Baker (2001) provided a detailed case study of implicit arguments (termed null instantiations in that work), but did not provide concrete methods to account for them automatically.
Previously, we demonstrated the importance of ﬁltering out nominal predicates that take no local arguments (Gerber et al., 2009); however, this work did not address the identiﬁcation of implicit arguments. Burchardt et al. (2005) suggested approaches to implicit argument identiﬁcation based on observed coreference patterns; however, the authors did not implement and evaluate such methods. We draw insights from all three of these studies. We show that the identiﬁcation of implicit arguments for nominal predicates leads to fuller semantic interpretations when compared to traditional SRL methods. Furthermore, motivated by Burchardt et al., our model uses a quantitative  analysis of naturally occurring coreference patterns to aid implicit argument identiﬁcation.
Most recently, Ruppenhofer et al. (2009) conducted SemEval Task 10, “Linking Events and Their Participants in Discourse”, which evaluated implicit argument identiﬁcation systems over a common test set. The task organizers annotated implicit arguments across entire passages, resulting in data that cover many distinct predicates, each associated with a small number of annotated instances. In contrast, our study focused on a select group of nominal predicates, each associated with a large number of annotated instances.

Implicit arguments have not been annotated within the Penn TreeBank, which is the textual and syntactic basis for NomBank. Thus, to facilitate our study, we annotated implicit arguments for instances of nominal predicates within the standard training, development, and testing sections of the TreeBank. We limited our attention to nominal predicates with unambiguous role sets (i.e., senses) that are derived from verbal role sets. We then ranked this set of predicates using two pieces of information: (1) the average difference between the number of roles expressed in nominal form (in NomBank) versus verbal form (in PropBank) and (2) the frequency of the nominal form in the corpus. We assumed that the former gives an indication as to how many implicit roles an instance of the nominal predicate might have. The product of (1) and (2) thus indicates the potential prevalence of implicit arguments for a predicate. To focus our study, we ranked the predicates in NomBank according to this product and selected the top ten, shown in Table 1.
We annotated implicit arguments document-bydocument, selecting all singular and plural nouns derived from the predicates in Table 1. For each missing argument position of each predicate instance, we inspected the local discourse for a suitable implicit argument. We limited our attention to the current sentence as well as all preceding sentences in the document, annotating all mentions of an implicit argument within this window.
In the remainder of this paper, we will use iargn to refer to an implicit argument position n. We will use argn to refer to an argument provided by PropBank or NomBank. We will use p to mark  Predicate price sale investor fund loss plan investment cost bid loan Overall  predicate instances. Below, we give an example annotation for an instance of the investment predicate:  (2) [iarg0 Participants] will be able to transfer [iarg1 money] to [iarg2 other investment funds]. The [p investment] choices are limited to [iarg2 a stock fund and a money-market fund].

NomBank does not associate this instance of investment with any arguments; however, we were able to identify the investor (iarg0), the thing invested (iarg1), and two mentions of the thing invested in (iarg2).
Our data set was also independently annotated by an undergraduate linguistics student. For each missing argument position, the student was asked to identify the closest acceptable implicit argument within the current and preceding sentences.
The argument position was left unﬁlled if no acceptable constituent could be found. For a missing argument position, the student’s annotation agreed with our own if both identiﬁed the same constituent or both left the position unﬁlled. Analysis indicated an agreement of 67% using Cohen’s kappa coefﬁcient (Cohen, 1960).

Role coverage for a predicate instance is equal to the number of ﬁlled roles divided by the number  of roles in the predicate’s lexicon entry. Role coverage for the marked predicate in Example 2 is 0/3 for NomBank-only arguments and 3/3 when the annotated implicit arguments are also considered. Returning to Table 1, the third column gives role coverage percentages for NomBank-only arguments. The sixth column gives role coverage percentages when both NomBank arguments and the annotated implicit arguments are considered.
Overall, the addition of implicit arguments created a 65% relative (18-point absolute) gain in role coverage across the 1,253 predicate instances that we annotated.
The predicates in Table 1 are typically associated with fewer arguments on average than their corresponding verbal predicates. When considering NomBank-only arguments, this difference (compare columns four and ﬁve) varies from zero (for price) to a factor of ﬁve (forfund). When implicit arguments are included in the comparison, these differences are reduced and many nominal predicates express approximately the same number of arguments on average as their verbal counterparts (compare the ﬁfth and seventh columns).
In addition to role coverage and average count, we examined the location of implicit arguments.
Figure 1 shows that approximately 56% of the implicit arguments in our data can be resolved within the sentence containing the predicate. The remaining implicit arguments require up to forty-six sen tences for resolution; however, a vast majority of these can be resolved within the previous few sentences. Section 6 discusses implications of this skewed distribution.

In our study, we assumed that each sentence in a document had been analyzed for PropBank and NomBank predicate-argument structure. NomBank includes a lexicon listing the possible argument positions for a predicate, allowing us to identify missing argument positions with a simple lookup. Given a nominal predicate instance p with a missing argument position iargn, the task is to search the surrounding discourse for a constituent c that ﬁllsiargn. Our model conducts this search over all constituents annotated by either PropBank or NomBank with non-adjunct labels.
A candidate constituent c will often form a coreference chain with other constituents in the discourse. Consider the following abridged sentences, which are adjacent in their Penn TreeBank document:  (4) Conservative Japanese investors are put off by [Mexico’s] investment regulations.

(5) Japan is the fourth largest investor in [c Mexico], with 5% of the total [p investments].

NomBank does not associate the labeled instance of investment with any arguments, but it is clear  from the surrounding discourse that constituent c (referring to Mexico) is the thing being invested in (the iarg2). When determining whether c is the iarg2 of investment, one can draw evidence from other mentions in c’s coreference chain. Example 3 states that Mexico needs investment. Example 4 states that Mexico regulates investment. These propositions, which can be derived via traditional SRL analyses, should increase our conﬁdence that c is the iarg2 of investment in Example 5.
Thus, the unit of classiﬁcation for a candidate constituent c is the three-tuple hp, iargn, c0i, where c0 is a coreference chain comprising c and its coreferent constituents.3 We deﬁned a binary classiﬁcation function P r(+| hp, iargn, c0i) that predicts the probability that the entity referred to by c ﬁlls the missing argument position iargn of predicate instance p. In the remainder of this paper, we will refer to c as the primary ﬁller , differentiating it from other mentions in the coreference chain c0. In the following section, we present the feature set used to represent each three-tuple within the classiﬁcation function.

Starting with a wide range of features, we performed ﬂoating forward feature selection (Pudil et al., 1994) over held-out development data comprising implicit argument annotations from section 24 of the Penn TreeBank. As part of the feature selection process, we conducted a grid search for the best per-class cost within LibLinear’s logistic regression solver (Fan et al., 2008). This was done to reduce the negative effects of data imbalance, which is severe even when selecting candidates from the current and previous few sentences. Table 2 shows the selected features, which are quite different from those used in our previous work to identify traditional semantic arguments (Gerber et al., 2009).4 Below, we give further explanations for some of the features.
Feature 1 models the semantic role relationship between each mention in c0 and the missing argument position iargn. To reduce data sparsity, this feature generalizes predicates and argument positions to their VerbNet (Kipper, 2005) classes and  3We used OpenNLP for coreference identiﬁcation: http://opennlp.sourceforge.net 4We have omitted many of the lowest-ranked features.
Descriptions of these features can be obtained by contacting the authors.

Feature value description For every f , the VerbNet class/role of pf /argf concatenated with the class/role of p/iargn.
Average pointwise mutual information between hp, iargni and any hpf , argf i.
Percentage of all f that are deﬁnite noun phrases.
Minimum absolute sentence distance from any f to p.
Minimum pointwise mutual information between hp, iargni and any hpf , argf i.
Frequency of the nominal form of p within the document that contains it.
Nominal form of p concatenated with iargn.
Nominal form of p concatenated with the sorted integer argument indexes from all argn of p.
Number of mentions in c0.
Head word of p’s right sibling node.
For every f , the synset (Fellbaum, 1998) for the head of f concatenated with p and iargn.
Part of speech of the head of p’s parent node.
Average absolute sentence distance from any f to p.
Discourse relation whose two discourse units cover c (the primary ﬁller) andp.
Number of left siblings of p.
Whether p is the head of its parent node.
Number of right siblings of p.

semantic roles using SemLink.5 For explanation purposes, consider again Example 1, where we are trying to ﬁll theiarg0 of shipping. Let c0 contain a single mention, The two companies, which is the arg0 of produce. As described in Table 2, feature 1 is instantiated with a value of create.agentsend.agent, where create and send are the VerbNet classes that contain produce and ship, respectively.
In the conversion to LibLinear’s instance representation, this instantiation is converted into a single binary feature create.agent-send.agent whose value is one. Features 1 and 11 are instantiated once for each mention in c0, allowing the model to consider information from multiple mentions of the same entity.
Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using pointwise mutual information (PMI) between syntactic positions. We used a similar PMI score, but deﬁned it with respect to semantic arguments instead of syntactic dependencies. Thus, the values for features 2 and 5 are computed as follows (the notation is explained in  Pcoref (hp, iargni , ∗)Pcoref (hpf , argf i , ∗) (6)  To compute Equation 6, we ﬁrst labeled a subset of the Gigaword corpus (Graff, 2003) using the verbal SRL system of Punyakanok et al. (2008) and the nominal SRL system of Gerber et al. (2009).
We then identiﬁed coreferent pairs of arguments using OpenNLP. Suppose the resulting data has N coreferential pairs of argument positions. Also suppose that M of these pairs comprise hp, argni and hpf , argf i. The numerator in Equation 6 is deﬁned as MN . Each term in the denominator is obtained similarly, except that M is computed as the total number of coreference pairs comprising an argument position (e.g., hp, argni) and any other argument position. Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for lowfrequency observations. The PMI score is somewhat noisy due to imperfect output, but it provides information that is useful for classiﬁcation.

Feature 10 does not depend on c0 and is speciﬁc to each predicate. Consider the following example:  (7) Statistics Canada reported that its [arg1 industrial-product] [p price] index dropped 2% in September.

The “[ p price] index” collocation is rarely associated with an arg0 in NomBank or with an iarg0 in our annotations (both argument positions denote the seller). Feature 10 accounts for this type of behavior by encoding the syntactic head of p’s right sibling. The value of feature 10 for Example 7 is price:index. Contrast this with the following:  (8) [iarg0 The company] is trying to prevent further [p price] drops.

The value of feature 10 for Example 8 is price:drop. This feature captures an important distinction between the two uses of price: the former rarely takes an iarg0, whereas the latter often does. Features 12 and 15-17 account for predicatespeciﬁc behaviors in a similar manner.
Feature 14 identiﬁes the discourse relation (if any) that holds between the candidate constituent c and the ﬁlled predicatep. Consider the following example:  (9) [iarg0 SFE Technologies] reported a net loss of $889,000 on sales of $23.4 million.

(10) That compared with an operating [p loss] of [arg1 $1.9 million] on sales of $27.4 million in the year-earlier period.

In this case, a comparison discourse relation (signaled by the underlined text) holds between the ﬁrst and sentence sentence. The coherence provided by this relation encourages an inference that identiﬁes the marked iarg0 (the loser). Throughout our study, we used gold-standard discourse relations provided by the Penn Discourse TreeBank (Prasad et al., 2008).

We trained the feature-based logistic regression model over 816 annotated predicate instances associated with 650 implicitly ﬁlled argument positions (not all predicate instances had implicit arguments). During training, a candidate three-tuple hp, iargn, c0i was given a positive label if the candidate implicit argument c (the primary ﬁller) was  annotated as ﬁlling the missing argument position.
To factor out errors from standard SRL analyses, the model used gold-standard argument labels provided by PropBank and NomBank. As shown in Figure 1 (Section 3.2), implicit arguments tend to be located in close proximity to the predicate. We found that using all candidate constituents c within the current and previous two sentences worked best on our development data.
We compared our supervised model with the simple baseline heuristic deﬁned below:6  Fill iargn for predicate instance p with the nearest constituent in the twosentence candidate window that ﬁlls argn for a different instance of p, where all nominal predicates are normalized to their verbal forms.

The normalization allows an existing arg0 for the verb invested to ﬁll an iarg0 for the noun investment. We also evaluated an oracle model that made gold-standard predictions for candidates within the two-sentence prediction window.
We evaluated these models using the methodology proposed by Ruppenhofer et al. (2009). For each missing argument position of a predicate instance, the models were required to either (1) identify a single constituent that ﬁlls the missing argument position or (2) make no prediction and leave the missing argument position unﬁlled. We scored predictions using the Dice coefﬁcient, which is deﬁned as follows:  P redicted is the set of tokens subsumed by the constituent predicted by the model as ﬁlling a missing argument position. T rue is the set of tokens from a single annotated constituent that ﬁlls the missing argument position. The model’s prediction receives a score equal to the maximum Dice overlap across any one of the annotated ﬁllers. Precision is equal to the summed prediction scores divided by the number of argument positions ﬁlled by the model. Recall is equal to the summed prediction scores divided by the number of argument positions ﬁlled in our annotated data.
Predictions not covering the head of a true ﬁller were assigned a score of zero.

6This heuristic outperformed a more complicated heuristic that relied on the PMI score described in section 4.2.

sale price investor bid plan cost loss loan investment fund Overall  Our evaluation data comprised 437 predicate instances associated with 246 implicitly ﬁlled argument positions. Table 3 presents the results.
Predicates with the highest number of implicit arguments - sale and price - showed F1 increases of 8 points and 18.8 points, respectively. Overall, the discriminative model increased F1 performance 15.8 points (59.6%) over the baseline.
We measured human performance on this task by running our undergraduate assistant’s annotations against the evaluation data. Our assistant achieved an overall F1 score of 58.4% using the same candidate window as the baseline and discriminative models. The difference in F1 between the discriminative and human results had an exact p-value of less than 0.001. All signiﬁcance testing was performed using a two-tailed bootstrap method similar to the one described by Efron and Tibshirani (1993).

We conducted an ablation study to measure the contribution of speciﬁc feature sets. Table 4 presents the ablation conﬁgurations and results.
For each conﬁguration, we retrained and retested the discriminative model using the features described. As shown, we observed signiﬁcant losses when excluding features that relate the semantic roles of mentions in c0 to the semantic role  of the missing argument position (ﬁrst conﬁguration). The second conﬁguration tested the effect of using only the SRL-based features. This also resulted in signiﬁcant performance losses, suggesting that the other features contribute useful information. Lastly, we tested the effect of removing discourse relations (feature 14), which are likely to be difﬁcult to extract reliably in a practical setting. As shown, this feature did not have a statistically signiﬁcant effect on performance and could be excluded in future applications of the model.

Of all the errors made by the system, approximately 19% were caused by the system’s failure to  generate a candidate constituent c that was a correct implicit argument. Without such a candidate, the system stood no chance of identifying a correct implicit argument. Two factors contributed to this type of error, the ﬁrst being our assumption that implicit arguments are also core (i.e., argn) arguments to traditional SRL structures. Approximately 8% of the overall error was due to a failure of this assumption. In many cases, the true implicit argument ﬁlled a non-core (i.e., adjunct) role within PropBank or NomBank.
More frequently, however, true implicit arguments were missed because the candidate window was too narrow. This accounts for 12% of the overall error. Oracle recall (second-to-last column in Table 3) indicates the nominals that suffered most from windowing errors. For example, the sale predicate was associated with the highest number of true implicit arguments, but only 80% of those could be resolved within the two-sentence candidate window. Empirically, we found that extending the candidate window uniformly for all predicates did not increase performance on the development data. The oracle results suggest that predicate-speciﬁc window settings might offer some advantage.

In Section 4.2, we discussed the price predicate, which frequently occurs in the “[ p price] index” collocation. We observed that this collocation is rarely associated with either an overt arg0 or an implicit iarg0. Similar observations can be made for the investment and fund predicates. Although these two predicates are frequent, they are rarely associated with implicit arguments: investment takes only eight implicit arguments across its 21 instances, and fund takes only six implicit arguments across its 43 instances. This behavior is due in large part to collocations such as “[ p investment] banker”, “stock [ p fund]”, and “mutual [p fund]”, which use predicate senses that are not eventive. Such collocations also violate our assumption that differences between the PropBank and NomBank argument structure for a predicate are indicative of implicit arguments (see Section 3.1 for this assumption).
Despite their lack of implicit arguments, it is important to account for predicates such as investment and fund because incorrect prediction of implicit arguments for them can lower precision.

This is precisely what happened for the fund predicate, where the model incorrectly identiﬁed many implicit arguments for “stock [ p fund]” and “mutual [p fund]”. The left context of fund should help the model avoid this type of error; however, our feature selection process did not identify any overall gains from including this information.

The baseline heuristic covers the simple case where identical predicates share arguments in the same position. Thus, it is interesting to examine cases where the baseline heuristic failed but the discriminative model succeeded. Consider the following sentence:  (12) Mr. Rogers recommends that [p investors] sell [iarg2 takeover-related stock].

Neither NomBank nor the baseline heuristic associate the marked predicate in Example 12 with any arguments; however, the feature-based model was able to correctly identify the marked iarg2 as the entity being invested in. This inference captured a tendency of investors to sell the things they have invested in.
We conclude our discussion with an example of an extra-sentential implicit argument:  (13) [iarg0 Olivetti] has denied that it violated the rules, asserting that the shipments were properly licensed. However, the legality of these [p sales] is still an open question.

As shown in Example 13, the system was able to correctly identify Olivetti as the agent in the selling event of the second sentence. This inference involved two key steps. First, the system identiﬁed coreferent mentions of Olivetti that participated in exporting and supplying events (not shown). Second, the system identiﬁed a tendency for exporters and suppliers to also be sellers. Using this knowledge, the system extracted information that could not be extracted by the baseline heuristic or a traditional SRL system.

Current SRL approaches limit the search for arguments to the sentence containing the predicate of interest. Many systems take this assumption a step further and restrict the search to the predicate’s local syntactic environment; however, predicates and the sentences that contain them rarely  exist in isolation. As shown throughout this paper, they are usually embedded in a coherent and semantically rich discourse that must be taken into account. We have presented a preliminary study of implicit arguments for nominal predicates that focused speciﬁcally on this problem.
Our contribution is three-fold. First, we have created gold-standard implicit argument annotations for a small set of pervasive nominal predicates.7 Our analysis shows that these annotations add 65% to the role coverage of NomBank. Second, we have demonstrated the feasibility of recovering implicit arguments for many of the predicates, thus establishing a baseline for future work on this emerging task. Third, our study suggests a few ways in which this research can be moved forward. As shown in Section 6, many errors were caused by the absence of true implicit arguments within the set of candidate constituents. More intelligent windowing strategies in addition to alternate candidate sources might offer some improvement. Although we consistently observed development gains from using automatic coreference resolution, this process creates errors that need to be studied more closely. It will also be important to study implicit argument patterns of non-verbal predicates such as the partitive percent.
These predicates are among the most frequent in the TreeBank and are likely to require approaches that differ from the ones we pursued.
Finally, any extension of this work is likely to encounter a signiﬁcant knowledge acquisition bottleneck. Implicit argument annotation is difﬁcult because it requires both argument and coreference identiﬁcation (the data produced by Ruppenhofer et al. (2009) is similar). Thus, it might be productive to focus future work on (1) the extraction of relevant knowledge from existing resources (e.g., our use of coreference patterns from Gigaword) or (2) semi-supervised learning of implicit argument models from a combination of labeled and unlabeled data.

We would like to thank the anonymous reviewers for their helpful questions and comments. We would also like to thank Malcolm Doering for his annotation effort. This work was supported in part by NSF grants IIS-0347548 and IIS-0840538.

7Our annotation data can be freely downloaded at http://links.cse.msu.edu:8000/lair/projects/semanticrole.html