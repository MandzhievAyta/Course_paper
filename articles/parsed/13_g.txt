The notion of relevance or informational dependency is basic to human reasoning. People tend to judge the 3-place relationships of mediated dependency (i.e., x influences y via z ) with clarity, conviction and consistency. For example, knowing the departure time of the last bus is considered relevant for assessing how long we are about to wait for the next bus. Yet, once we learn the current whereabouts of the next bus, the former no longer provides useful information. These commonsensical judgments are issued qualitatively and reliably and are robust to the uncertainties which accompany the assessed events. Consequently, if one aspires to construct commonsensical reasoning systems, it is important that the language used for representing knowledge should facilitate a quick detection of mediated dependencies by a few primitive operations on the salient features of the representation scheme.

Making effective use of information about dependencies is a computational necessity, essential in any reasoning. If we have acquired a body of knowledge z and now wish to assess the truth of proposition x, it is important to know whether it would be worthwhile to consult another proposition y, which is not in z . In the absence of such information, an inference engine would spend precious time on derivations bearing no relevance to the task at hand. A similar necessity exists in  * This work was supported in part by the National Science Foundation Grant #DCR 85-01234.

truth maintenance systems. If we face a new piece of evidence, contradicting our previously held assumptions, we must retract some of these assumptions and, again, the need arises of quickly identifying those that are relevant to the contradiction discovered. But how would relevance information be encoded in a symbolic system?  Explicit encoding is clearly impractical because the number of (X, y , z ) combinations needed for reasoning tasks is astronomical. Relevance or dependencies are relationships which change dynamically as a function of the information available at any given time. Acquiring new facts may destroy existing dependencies as well as create new ones. For example, learning a child’s age destroys the dependency between the size of his shoes and his reading ability, while learning that a patient suffers from a given symptom creates new dependencies between the diseases that could account for the symptom. What logic would facilitate these two modes of reasoning?  In probability theory, the notion of informational relevance is given precise quantitative underpinning using the device of conditional independence, which successfully captures our intuition about how dependencies should change with learning new facts. A variable x is said to be independent of y) given the information 2, if  Clearly, x and y could be marginally dependent (i.e., dependent, when z is unknown) and become independent given z, and, conversely, x and y could be marginally independent and become dependent only upon learning the value of z. These dynamics are also captured by the qualitative notion of Embedded Multivalued Dependencies (F&MD) in relational databases. Thus, in principle, probability and database theories could provide the machinery for identifying which propositions are relevant to each other with any given state of knowledge.

Yet, it is flatly unreasonable to expect people or machines to resort to numerical equalities or relational tables in order to extract relevance information. FIuman behavior suggests that such information is inferred qualitatively from the organizational structure of human memory. Accordingly, it would be interesting to explore how assertions about  relevance can be inferred qualitatively and whether assertions similar to those of probabilistic or database dependencies can be derived logically without references to numbers or tables.
Preliminary work related to probabilistic dependencies has been reported in [Pearl and Paz, 19861 and is extended in this paper to the qualitative notion of EIVIVD.

Having a logic of dependency might be useful for testing whether a set of dependencies asserted by an expert is selfconsistent and might also allow us to infer new dependencies from a given initial set of such relationships. However, such logic would not, in itself, guarantee that the inferences required would be computationally tractable or that any sequence of inferences would be psychologically meaningful, i.e., correlated with familiar mental steps taken by humans.
To facilitate this latter feature, we must also make sure that most derivational steps in that logic correspond to simple local operations on structures depicting common-sensical associations. We call such structures dependency graphs.

The nodes in these graphs represent propositional variables, and the arcs represent local dependencies among conceptually-related propositions. Graph representations are perfectly suited for meeting the requirements of explicitness, saliency and stability, i.e., the links in the graph permit us to qualitatievely encode dependence relationships, and the graph topology displays these relationships explicitly and preserves them, in fact, under any assignment of numerical parameters.

It is not surprising, therefore, that graphs constitute the most common metaphor for describing conceptual dependencies. Models for human memory are often portrayed in terms of associational graphs (e.g., semantic networks woods, 19751, constraint networks [Montanari, 19741, inference nets muda, Hart and Nilsson, 19761 conceptual dependencies [Schank 19721) and conceptual graphs [Sowa, 19831).
Graph-related concepts are so entrenched in our language (e.g.
“threads of thoughts,” “lines of reasoning,’ ’ “connected ideas,” “far-fetched arguments” etc.) that one wonders whether people can, in fact, reason any other way except by tracing links and arrows and paths in some mental representation of concepts and relations. Therefore, a natural question to ask is whether the intuitive notion of informational relevancy or the formal notions of probabilistic and database dependencies can be captured by graphical representation, in the sense that all dependencies and independencies in a given model would be deducible from the topological properties of some graph.

Despite the prevailing use of graphs as metaphors for communicating and reasoning about dependencies, the task of capturing dependencies by graphs is not at all trivial. When we deal with a phenomenon where the notion of neighborhood or connectedness is explicit (e.g., family relations, electronic circuits, communication networks, etc.), we have no problem configuring a graph which represents the main features of the phenomenon. However, in modeling conceptual relations such as causation, association and relevance, it is often hard to  distinguish direct neighbors from indirect neighbors; so, the task of constructing a graph representation then becomes more delicate. The notion of conditional independence in probability theory provides a perfect example of such a task. For a given probability distribution P and any three variables x, y , z , while it is fairly easy to verify whether knowing z renders x independent of y , P does not dictate which variables should be regarded as direct neighbors. In other words, we are given the means to test whether any given element z intervenes in a relation between elements x and y, but it remains up to us to configure a graph that encodes these interventions. We shall see that some useful properties of dependencies and relevanties cannot be represented graphically and the challenge remains to devise graphical schemes that minimize such deficiencies.

Ideally, we would like to represent dependency between elements by a path connecting their corresponding nodes in some graph G. Similarly, if the dependency between elements x and y is not direct and is mediated by a third element, z, we would like to display z as a node that intercepts the connection between x and y , i.e., z is a cutset separating x from y . This correspondence between conditional dependencies and cutset separation in undirected graphs forms the basis of the theory of Markov fields [Lauritzen, 19821, and has been given an axiomatic characterization in [Pearl and Paz, 19861.

The main weakness of undirected graphs stems from their inability to represent nontransitive dependencies; two independent variables will end up being connected if there exists some other variable that depends on both. As a result, many useful independencies remain unrepresented in the graph. To overcome this deficiency, one can employ directed graphs and use the arrow directionality to distinguish between dependencies in various contexts. For instance, if the sound of a bell is functionally determined by the outcomes of two coins, we will use the network coin 1 + bell t coin 2, without connecting coin I to coin 2. This pattern of converging arrows is interpreted as stating that the outcomes of the two coins are normally independent but may become dependent upon knowing the outcome of the bell (or any external evidence bearing on that outcome). This facility of directed graphs forms the basis of causal networks which have a long tradition in the social sciences [Kenny, 19791, and have also been adopted for evidential reasoning tasks [Pearl, 19861.

This paper treats directed graphs as a language of expressing dependencies. Section II presents formal definitions for two models of data dependencies (Probabilistic and EMVD) and two models of graphical dependencies (undirected and directed). An axiomatic definition is then provided for a relational structure called semi graphoid which covers all four models, thus formalizing the general notion of mediated dependence. Section III compares the expressive power of directed graphs to that of undirected graphs and shows the superiority of the former. Section IV, explores the power of directed graphs to cover data dependencies of the type produced by probabilistic or logical models. The main contibu tion of the paper lies in showing that directed acyclic graphs (DAGs) are powerful tools for encoding and inferring data dependencies of both types, identifying the scource of that power, and highlighting its limitations.

Definition: A Dependency Model M over a set of objects U is any subset of triplets (X, 2, Y) where X , Y and Z are three disjoint subsets of U. The triplets in M represent independenties, that is, (X, Z, Y) E M asserts that X and Y interact only via Z, or, “X is independent of Y given Z”. This statement is also written as Z(X, Z, Y) with an optional subscript to clarify the type of the dependency when necessary.

Definition: A Probabilistic Dependency model (PD) Mp is defined in terms of a probability distribution P over some set of variables U, i.e. a function mapping any instantiation of the variables in U to a non-negative real number such that the sum over the range of P is unity. If X, Y and Z are three subsets of U and x, y and z any instantiation of the variables in these subsets, then by definition I (X, Z , Y), iff  This definition is equivalent to that given in (1) and conveys the idea that, once Z is fixed, knowing Y can no longer influence the probability of X . pawid, 19791.

Definition: A dependency model iV is said to be in PD, M E PD, if there exists a probability distribution P such that the definition above @q.(2)) holds for every triplet (X, Z, Y) in M .
Thus, PD (and, similarly, PD-, UGD, DAGD, and SG defined below) represents a class of dependency models, all sharing a common criterion for selecting triplets in M .

Definition: A Non-Extreme Probabilistic Dependency model (PO-) is any model Mp in PD where the range of P is restricted to the positive real numbers, (Le., excluding O’s and l’s).

Definition [Fagin, 19771: An Embedded Multivalued Dependency model (EMVD) MR is defined in terms of a database R over a set of attributes U, i.e. a set of tuples of values of the attributes. The notation <aI a2 *. + a,> is conventionally used to denote that the tuple is in the relation R. If X , Y and Z are three disjoint subsets of U and x1, x2, yl,y2, z any instantiations of the corresponding attributes in X , Y and Z, then by definition Z(X, Z, Y), iff  In other words, the existence of the subtuples c x t y I z > and cx2y2z > guarantees the existence of cx1y2z >. EMVD is a powerful class of dependencies used in databases and it conveys the idea that, once Z is fixed, knowing Y cannot further restrict the range of values permitted for X. This definition was also used by Shenoy and Shafer [1986] to devise a “‘qualitative” extension of probabilistic dependencies.

Definition: An Undirected Graph Dependency model (UGD) & is defined in terms of an undirected graph G . If X, Y and 2 are three disjoint subsets of nodes in G then by definition Z(X, 2, Y)o iff every path between nodes in X and Y contains at least one node in Z. In other words, 2 is a cutset separating X from Y. A complete axiomatization of UGD is given in [Pearl and Paz, 19861.

Definition: A Directed Acyclic Graph Dependency model (DAGD) Mo is defined in terms of a directed acyclic graph (DAG) G . If X, Y and Z are three disjoint subsets of nodes in G , then by definition Z(X, Z, Y), iff there is no bi-directed path from a node in X to a node in Y along which every node with converging arrows either is or has a descendent in Z and every other node is outside Z .

The latter condition corresponds to ordinary cutset separation in undirected graphs while the former conveys the idea that the inputs of any causal mechanism become dependent once the output is known. This criteriorn was called dseparation in [Pearl, 19861. In Figure 1, for example, X = 12) and Y = 13) are d -separated by Z = {l) (i.e. (2,1,3) E MG) because knowing the common cause 1 renders its two possible consequences, 2 and 3, independent. However X and Y are not d-separated by Z’= {l, 51 because learning the value of the consequence 5, renders its causes 2 and 3 dependent, like opening a pathway along the converging arrows at 4.

Definition: An I-map of a dependency model M is any model 11$such that M’c_1~. For example, the undirected graph X&YisanZ-mapoftheDAGX+ZtY.

Definition: A D-map of a dependency model M is any model M’ such that M’ 1 M. For example, if a relation R contains all tuples having non-zero probability in P then MR is a D -map of IM,.

Definition: A Perfect-map of a dependency model M is any model M’ such that M’= M. For example, the undirected graph X-Z-Y is a perfect map of the DAG X + Z + Y .

We will be primarily interested in mapping data dependencies into graphical structures, where the task of testing connectedness is easier than that of testing membership in the original model M. A D -map guarantees that vertices found to be connected are, indeed, dependent; however, it may occasionally display dependent variables as separated vertices. An Z-map works that opposite way: it guarantees that vertices  found to be separated always correspond to genuinely independent variables but does not guarantee that all those shown to be connected are, in fact, dependent. Empty graphs are trivial D -maps, while complete graphs are trivial I-maps.

Definition: A semi-graphoid (SG) is any dependency model M which is closed under the following properties:  Contraction (X,ZY, W) & (X,Z, Y) E M * (X,Z, YW) E M(4)  It is straight forward to show that all the specialized classes of dependency models presented thus far are semi-graphoids, and in view of this generality, these four properties are selected to represent the general notion of mediated dependence between items of information.

With the exception of UGD, none of the specialized dependency classes possesses complete parsimonious axiomatization similar to that of semi-graphoids. EMVD is known to be non-axiomatizable by a bounded set of Horn clauses [Parker, 19801, and a similar result has recently been reported for DAGD [Geiger, 19871. PD is conjectured to be equivalent to SC (i.e., M E PD <=> M E SG) but no proof (nor disproof) is in sight.

Definition: A graphoid is any semi-graphoid M which is also closed under the following property:  It is straight forward to show that classes PD-, UGD, and DAGD are all graphoids. Only EMVDs and pure PDs do not comply to this axiom.

The most important properties of graphoids [Pearl and Paz, 19861 are that they possess unique edge-minimal I-maps in UG, and permit the construction of graphical I-maps from local dependencies. By connecting each variable x to any subset of variables which renders x conditionally independent of all other variables in U, we obtain a graph that is an I-map of u. Such local construction is not guaranteed for semigraphoids. The reason this paper focuses on semi-graphoids is to include dependency models representing logical, functional and definitional constraints; such constraints are excluded from PD-. In Section IV, we will show that the use of DAG’s provides a local construction of I-maps for every semigraphoid. The relationships between the six classes of dependency models are shown in the hierarchy of Figure 2, where arrows stand for set inclusions.

Definition: Let M be a dependency model from some class 44 of dependency models. A subset B GM of triplets is a M-basis of M iff every model M’ E M which contains $3-also contains M. Thus, a basis provides a complete encoding of the information contained in M; knowing B and M enables us, in principle, to decide what triplets belong to ~4.

One of the main advantages of graphical representations is that they posses extremely parsimonious bases and extremely efficient procedures for testing membership in the cloy sures of these bases. For example, to encode all dependencies inferable from a given undirected graph G = (V, E) we need only specify the set of neighbors N(x) for each node x in G, and this corresponds to specifying a neighborhood basis:  Testing membership of an arbitrary triplet (X , Z, Y) in the closure of BN simply amounts to testing whether Z is a cutset of G separating the nodes in X from those in Y.

DAGs also possess efficient bases; to encode all dependencies inferable from a given DAG G, we need only specify the parents PA(x) for each node x E G. To encode those in the form of a basis we arrang-e the nodes in any total order xi,. . . ,x, consistent with the arrows of G and construct the stratified set of triplets:  stating that PA (Xi) d-separates xi from its other predecessors.
B is a DAG-basis of G since the closure of B coincides with the independencies displayed by G .

One would normally expect that the introduction of directionabty into the language of graphs would render them more expressive, capable of portraying a more refined set of dependencies, e.g., non-transitive. Thus, it is natural to ask:  Are all dependencies representable graphs also representable by a DAG?  How well can DAGs represent the type of data dependencies induced by probabilistic or logical models?  The second question will be treated in Section IV while the answer to the first question is, clearly, negative. For instance, the dependency structure of the diamond-shaped graph of Fig.3(a) asserts the two independencies: Z(A,BC,D)andZ(B,AD,C). NoDAGcanexpressthesetwo relationships simultaneously and exclusively. If we direct the arrowsfromA toD,wegetZ(A,BC,D)butnotZ(B,AD,C); if we direct the arrows from B to C, we get the latter but not the former. This limitation will always be encountered in nonchordal graphs, i.e., graphs containing a chordless cycle of length 24 [Tarjan & Yannakakis, 19841; no matter how we direct the arrows, there will always be a pair of non-adjacent parents sharing a common child, a configuration which yields independence in undirected graphs but dependence in DAGs.
This problem does not exists in chordal graphs and, consequently, we have  Theorem 1: UGD and DAGD intersect in a class of dependency models representable by chordal graphs.

Non-chordal graphs represent the one class of dependencies where undirected graphs exhibit expressiveness superior to that of DAGs graphs. However, this superiority can be eliminated by the introduction of auxiliary variables. Consider the diamond-shaped graph of Figure 3(a). Introducing an auxiliary variable E in the manner shown in Figure 3(b) creates a DAG model on five variables which also asserts Z(C , B , D ).

If we “clamp” the auxiliary variable E at some fixed value E = e i, as in-Figure 3(c), the dependency structure projected on A, B, C, D is identical to the original structure of Figure 3(a),i.e.,Z(A,BC,D)andZ(B,AD,C).

In general, since every arc C-D in an undirected graph is equivalent to the bi-directed path C +YtD (with E “clamped”), we have:  Theorem 2: Every dependency model expressible by an undirected graph is also expressible by a DAG, with some auxiliary nodes.

Suppose someone (e.g., an expert) provides us with a list L of positive and negative triplets, representing a set of independencies and dependencies in some (undisclosed) dependency model M, of a known class. Several questions arise:  How can we test whether L is consistent and/or non redundant?  How can we deduce all the implications of L, or, at least test whether a given triplet is logically implied by L?  What additional triplets are required to make the model completely specified?  These questions are extremely difficult to answer if M does not possess a convenient basis or if L does not coincide with that basis. Even in a neatly axiomatized system such as semi-graphoids the answers to these questions involve intractable proof procedures.

Graph representations can be harnessed to alleviate these difficulties; we construct a graph model that entails L and draw inferences from G instead of L. The quality of inferences will depend, of course, on how faithfully G captures the closure of L. The following results (see [Verma, 19871 for proofs) uncover the unique powers of DAGs in performing this task.

Let Ua(“) represent the set of elements smaller than n under some total ordering 8 on the elements of U, i.e., {u E u Ie(u) < e(n)].

efinition: A stratified protocol LOof a dependency model M is any set of pairs  Intuitively, LB lists, for each x E U, a set of predecessors S, of x which renders x conditionally independent of all its other predecessors (in the order 0). In causal modeling, Le specifies the set of direct causes of event x. For example, the causal model of Figure 1 is specified by the protocol:  Stratified protocols were used in [Pearl 19861 to construct DAG representations (called Bayesian Networks) of probabilistic dependencies by connecting the elements in S, as direct parents of x . The following results justify this construction and generalize it to any semi-graphoid, including, in particular, the qualitative dependencies of EIMVD.

The first result states that the DAG constructed in this fashion can faithfully be used to infer dependency information; any independence inferred from that DAG must be true in M and, furthermore, every independence which is implied by the protocol will be displayed in the DAG.

Theorem 3: If M is any semi-graphoid then the DAG generated from any stratified protocol L, of M is an Z-map of M .

Corrollary: If L, is any stratified protocol of some dependency model M , the DAG generated from L, is a perfect map of the semi-graphoid closure of L,.

Another interesting corollary of Theorem 3 is a generalization of the celebrated Mar&v-chain property. It states (informally) that if in a sequence of variables Xl,X,,*..,Xi *.* each Xi “shields” its successor Xj+l from the influence of its predecessors, then each Xi is “shielded” from all other variables by its two nearest neighbors, Xisl and xi+l- (The converse holds only in graphoids). This property has been used extensively in probability theory and Theorem 3 permits its application to qualitative dependencies as well.

Note that, since the topology of the DAG depends only on the set of child-parents pairs contained in the protocol, the order 0 used in generating Lo need not be known; Theorem 3 holds for any generating order, and the only consistency requirement on the structure of L is that 10,, x) Iy E S,) constitutes a partial order.

The second result states that every independence in a semi-graphoid cau be inferred from at least one stratified protocol.

Theorem 4: If M is any semi-graphoid then the set of DAGs generated from all stratified protocols of M is a perfect map of Iki. (The criterion for separation relative to a set of DAGs is that d-separation must exist in at least one of the DAGs.)  Thus, even though every triplet in a stratified protocol asserts an independency relative to a singleton element; the sum total of such triplets is sufficient to encode all the set-toset independencies embedded in the semi-graphoid.

This paper demonstrates that directed acyclic graphs (DAGs) possess powerful inferential properties. If an input set of dependencies is given in the form of a stratified protocol, then all implications of this input can be deduced efficiently, by graphical manipulations, instead of logical derivations.

No equivalent protocol of similar parsimony is known to work for undirected graphs, unless the generating model is a full graphoid, namely, unless logical, functional and definitional constraints are excluded from the model. Thus, DAGs appear to provide powerful inference tools for handling data dependencies of the type encountered in both probabilistic and logical reasoning. This feature helps explain the prevailing use of DAGs in causal models and semantic nets.