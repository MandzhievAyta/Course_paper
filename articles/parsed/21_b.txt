Sequential Feature Extraction Using Information-Theoretic Learning  1 INTRODUCTION Feature extraction is commonly employed as a pre-processor for applications including: visualization, classification, detection, and verification.
Herein, feature reduction, which in the linear case is also known as subspace projection, is investigated as it applies to classification. Fig. 1 shows a block diagram of the major components used in a classification system. In this figure, sj(k), xj(k) and yj(k) are the size (NI x 1) input features, (NO x 1) output (transformed) features and the (NC x 1) outputs of the classifier at time k and having class j (j = 1, 2, ..., NC), respectively. For classification, optimality can be considered as the condition for which the probability of correct classification, 1 - P[e(k) = 1], is maximized, where e(k) = 1 if an error occurred at time k, and is 0 otherwise. An empirical estimation of the error probability will be used as the figure of merit.

Feature reduction methods attempt to improve generalization by reducing the variance in classification performance [32]. For certain classifiers, the improved generalization may occur since a reduction in the number of features causes a reduction in the number of free parameters (relative to the number of data points) required for classification. In other classifiers, the improved generalization may occur as a result of reducing the dimensionality of the required probability density function (pdf) estimation. However, classification performance does not improve indefinitely as the number of features is reduced, due to classifier bias [32]. At some point, the loss of information (about the class) inherent in reducing the number of features overwhelms any benefit gained from reducing, for example, the number of adaptable parameters. Obviously, this biasvariance dilemma is highly dependent upon the specific algorithms used for both the feature extractor and the classifier, and it is the trade-off between the two that provides the motivation for the present  xj,1(k) Feature xj,2(k) : .
Extraction xj,NO(k) Function  study. Another method, Vapnik’s Structural Risk Minimization (and its embodiment, the Support Vector Machine (SVM) [4]-[10]), has recently been determined to provide more explicit control of generalization through regularization. As such, it has become the obvious methodology with which to compare the performance of any feature reduction method.

Feature reduction methods may be categorized based on whether the projector and the classifier are trained sequentially or simultaneously. Sequential methods adapt the projector based on optimizing a criterion at the output of the projector. On the other hand, simultaneous methods adapt the projector based on optimizing a criterion at the output of the classifier. The former is independent of the classifier, while the latter is trained “through” the classifier. This gives sequential methods an implementation advantage. Not only does it take longer to train simultaneously due to the increased computational complexity, but also the cost function landscapes may become more difficult to search.
Moreover, a new set of update equations must be derived and the projector must be re-trained if it is desired to evaluate a new classifier. On the other hand, simultaneous methods have the obvious (theoretically speaking) performance advantage in that they optimize the projector and the classifier together, that is they tune the projector to the classifier discriminant functions. It is expected that, every thing else remaining constant, simultaneous training will produce superior results compared to training  sequentially. Consequently, the onus is on sequential training methods to demonstrate that there exists an easily implementable and general-purpose feature extraction algorithm that provides, with the combination of a suitable classifier, commensurate classification error.

Another difference between sequential and simultaneous systems is the choice of possible criteria for use in training the extractor. Criteria such as Minimum Classification Error (MCE) [1]-[3] and Mean Square Error (MSE) [11], for example, are well suited for training the extractor simultaneously.
However, they are not appropriate for sequential training since both of these criteria are based on an error signal. In order to use the MSE criterion for sequential training, a set of NO-dimensional targets (one for each class) must be defined in the output feature space, yj(k). There is no principled method known to the authors for selecting these targets in the feature space, combined with the expectation that the classification performance will vary considerably depending on the choice of targets used. Similarly, the MCE criterion is based on the assumption that the relative values of the output of the (in this case) projector, are related to the a posteriori probabilities of each class, to wit, it is based on the assumption that the values are the output of a classifier. Other criteria, on the other hand, are well suited for either sequential or simultaneous training. For example, an information-theoretic method that makes use of mutual information (MI) could be used. In this case, the extractor can be trained either  sequentially or simultaneously by maximizing the MI between the class labels and the outputs of the extractor or the classifier, respectively. Here we are particularly interested in measuring the performance of IT methods with respect to the preservation of discriminative information; hence, the informationtheoretic methods considered in this paper are limited to sequentially-trained systems.

Since the theoretical performance advantage of a simultaneously-trained system, as compared to a sequentially-trained system, can not be guaranteed if the two use different criteria, this gives rise to an interesting question:  z How does the performance of information-theoretic, sequential feature reduction methods compare to error-based, simultaneous methods?  Furthermore, in order to put the performance results of the above-mentioned comparison in proper perspective, it may be asked:  z How does the performance of the best feature reduction method considered here, when combined with a simple classifier, compare to the SVM and the closely related Optimal Hyperplane?  In order to answer the first question, a common platform needs to be defined. Section 2 lists and describes the two classifiers used to compare the feature reduction algorithms, while Section 3 gives details on the constraints placed on the structure of the subspace projector. This is followed by Section 4, which gives an introduction to the proposed information-theoretic algorithm, MRMI-SIG. Section 5 then provides a brief description of all the competing feature reduction algorithms. These eight algorithms are then compared in Section 6 in order to address the first question. The results of the SVM and the Optimal Hyperplane (OH) are then given in order to address the second question.

2 CLASSIFIERS The two Bayes (maximum likelihood) classifiers that are used for the comparisons are the Bayes-G and the Bayes-NP classifiers, both of which generate nonlinear decision surfaces. Notice that these  classifiers are used to compare the feature reduction methods, and in a later section, are combined with the proposed feature reduction method in order to compare its performance with the SVM and the OH.

The Bayes-G classifier is a parametric classifier that uses only second-order statistics of the output features. It has a single output for each class, which gives an estimate of the (a posteriori) probability of the associated class given the data (more specifically, it gives an estimate of the likelihood function multiplied by the prior, which is proportional to the a posteriori) [11]. The likelihoods are estimated by assuming that, for every class j, the set of output features, xj, is multi-variate Gaussian distributed.
The estimated class for each output is determined as the one that maximizes the weighted likelihood,  –1 yj(k) = --------------P----(---j--)-------------- e--2--- (x(k) – µj)TCj–1(x(k) – µj) ( 2π)NO Cj 1 ⁄ 2  where (j = 1, 2, ..., NC), NC is the number of classes, x(k) is the (NO x 1) data point at time k that is to be classified, µj is the (NO x 1) mean vector of class j, Cj is the (NO x NO) covariance matrix of class j, and P(j) is the prior probability of class j, i.e. Nj / NT, where Nj is the number of data points contained in the training set belonging to class j, and NT is the total size of the training set.

The Bayes-NP is a non-parametric classifier that uses Parzen Windows [12] to estimate each of the a posteriori distributions (once again, it actually estimates the likelihood multiplied by the associated prior probability, which, for classification, is tantamount to determining the a posteriori). The NOdimensional likelihood for class j is estimated by placing a Gaussian kernel at each point (one for each data point in the training set belonging to class j) in the input space (input to the classifier, which is the output feature space), summing these together, multiplying by the prior and then normalizing by N .
j Once the likelihood functions are determined, a new point is classified by evaluating the weighted likelihood of each class at the location of the data point in question. The class that produces the maximum value is then determined to be the correct class,  xj(i) is the ith data point of the training set of class j (j = 1, 2, ..., NC), σ is a user-defined kernel size, and  --–---1---2xTx G(x, σ2) = ------------1------------ e2σ ( 2π)NOσ  A nice feature of the Bayes-NP classifier is that there are no implicit assumptions on the distribution of the output features. Therefore, this method is able to take into account higher-order statistics of the output features, including multiple-modality, unlike the Bayes-G classifier. It does, however, have a user-defined parameter, σ, whose value must be determined.

Other classifiers could also have been used, for example, the SVM, k-NN [49] or one of several different static artificial neural networks (ANN’s); viz., the Multi-Layer Perceptron (MLP) [13], Radial Basis Function (RBF) [13], or Polynomial Network [14]. The MLP classifier, the most popular of the ANN’s just listed, was considered briefly because it can be considered to encompass both the projector and the classifier in one functional unit. However, results from several papers indicate that the MLP classifier performs quite poorly for two of the three data sets used in the upcoming comparison [15], [16]. More importantly, the two classifiers chosen have the nice feature that either the requisite training is trivial (i.e. estimation of second-order statistics for the Bayes-G classifier) or non-existent (for the Bayes-NP, which is a memory-based classifier).
This is to be compared to an MLP classifier, the performance of which is subject to local minima and other convergence issues. As such, its use in comparing multiple feature extractors would reduce the level of certainty that the measured performance difference is due to the type of feature extractor and not due to imperfect training of the classifier.

3 PROJECTION ARCHITECTURE To simplify the exposition, the projection architecture is limited to the set of linear transformations.

where the weighted likelihood of each class is given by,  Nj yj(k) = -P---(---j--) ∑ G(x(k) – xj(i), 2σ2) Nj i = 1  The equation for a completely general linear transformation is x = Rs + b, where R is a (NO x N ) I matrix of coefficients and b is a (NO x 1) vector of coefficients. Notice that the block diagram in Fig. 1 does not include the b vector since the two classifiers are invariant under a change in the mean. In addition, it is trivial to show that both classifiers are invariant under an invertible, linear transform.
Therefore it is assumed, without loss of generality, that the original features have been shifted, rotated and scaled so that the resulting (NI x 1) input features, s(k), are zero-mean, (spatially) uncorrelated and have unit variance. The invariance of the classifiers to invertible transforms can also be used to reduce the number of free parameters. This is done by constraining the R matrix to be a pure rotation. In this case, the R matrix is formed using the first NO rows of the product of (NI x NI) Given’s rotation matrices (one for each pair of outputs) [17]. This reduces the number of parameters from NONI to NO(NI-NO) without unnecessarily restricting the possible set of decision surfaces that can be produced by a linear projection. Notice that rotations between retained output features have no effect on classification, nor do rotations between rejected outputs. Only rotations between a feature that is retained and a feature that is rejected has any effect.
Due to its generality, unless stated otherwise, the subspace projector of each feature reduction algorithm will be constrained to be a rotation matrix.

4 INFORMATION-THEORETIC EXTRACTION Whereas methods that use second-order statistics compare the linear relationship of (commonly) two random variables, e.g. one output feature and the class label, information-theoretic methods compare the nonlinear relationships of multiple random variables, i.e. a vector of features and the class label.
This is accomplished by maximizing I(X;C), the MI between the output features and the class labels. In words, I(X;C) may be described as the amount of information the random (input feature) vector X carries about the class, C (where, realizations of X and C are given by the (NO x 1) vector x(k) and the scalar c(k), respectively).

A number of different definitions of MI exist, but the one having (arguably) the greatest theoretical importance is the one named in honor of Claude Shannon, which is given by [18],  H(X) = – ∫ fX(x)logfX(x)dx (5) –∞ and fX(x) is the pdf of X. First, maximizing I(X;C) minimizes the amount of information (about the class) lost due to the projection, where an intuitive definition of the amount of information loss is I(S;C) - I(X;C), i.e. the amount of information the input features have about the class minus the information that the projected features have about the class. A projection of s(k) cannot add information so that I(X;C) is always less than or equal to I(S;C) and the difference is zero only if x(k) is an invertible (linear or nonlinear) function of s(k). Since I(S;C) is constant with regards to the projector, the loss is minimized by maximizing I(X;C). A second, more principled, argument for using Shannon’s MI comes from the consideration of the upper and lower bounds it places on the Bayes error rate. A lower bound for the Bayes rate is given by the Fano inequality [19], and an upper bound is given by Hellman and Raviv [63]. Both of these bounds are minimized by maximizing I(X;C), or equivalently, by minimizing H(C|X). While these are good arguments for using Shannon’s MI, it is not in common use due to the difficulty of estimating (5). Part of the difficulty stems from the integral, which may be avoided by discretization of the variables [20]-[24].
However, discretization requires the bin sizes for each region of the multi-dimensional output feature space to be appropriately selected. Too small and a zero probability occurs due to having a finite data set, too large and details of the shape are lost. In addition, the determination of the appropriate bin sizes is time consuming and methods that rely on discretization require a large amount of training data for satisfactory results [21], due to the “curse of dimensionality” [39].

On the other hand, Alfred Renyi introduced a definition of entropy, namely Renyi’s (quadratic) entropy [25], that can be used as an alternative to  Shannon’s definition. This alternative entropy measure is given by,  HR(X) = –log ∫ fX(x)2dx (6) –∞ When the pdf is estimated using Parzen windows with Gaussian kernels, there is no need for discretization. This is due to the fact that the integral of the product of two Gaussians is a Gaussian evaluated at a single point [26]. As a result, there are no truncations or approximations required, outside of the implicit pdf estimation using Parzen windows. This motivates an approximation for mutual information, MI, using Renyi’s entropy, namely,  which is similar to what has been referred to as mutual α-entropy difference by Hero et al. [59] (who use a direct estimation of Renyi’s entropy by means of minimal spanning trees rather than the “plug-in” density estimator [64] used here). This approximation of MI is invariant to a change in scale, is much simpler than the one suggested by Renyi [25] and is similar in form to Shannon’s MI, except that each of the entropies is substituted with the non-parametric estimator for Renyi’s entropy given by [27],  N HR(X) ≅ –log --1---2 ∑ G(x(k) – x(k – 1), 2σ2) (8) N k = 1 where G(x,σ2) was previously defined in equation (3). The combination of equations (7) and (8) results in the proposed information-theoretic criterion for sequential feature extraction, which is referred to as Minimum/Maximum Renyi’s Mutual Information using the Stochastic Information Gradient, or MRMI-SIG [28]. The overall cost function is given by equation (9), where the influence of R is from the relation, x(k) = Rs(k). The SIG modification is an approximation that is used to reduce the complexity  from O(NT2) of the original algorithm to O(NT) of the present algorithm. It turns out that, if the training data is presented multiple times and if the time indices are randomized for each presentation, the SIG algorithm converges in the limit to the original algorithm [28]. Several other methods may also be used to reduce the complexity, including importance sampling [29], random sampling [30], clustering [30], sliding window [31], and GMM, Gaussian  Mixture Models [30]. If gradient ascent is used as the optimization method, the tap weight update equation becomes,  where the second term is the gradient of I(X;C) with respect to R and η is the step size. Notice that the class labels are used only to determine which set of training data is included in the second term of equation (9), so that there is no need to convert the (nominal) class labels into ratio values (where all values may be categorized as either nominal, ordinal, interval, or ratio [60]).

There are two weaknesses of this approach. First, there is no guarantee that maximizing equation (7) using Renyi’s definition, is equivalent to maximizing (4) using Shannon’s definition. Second, due to the difficulty of estimating pdf’s in high dimensional spaces, the number of dimensions should be kept small. Notice, however, the dimensionality of the pdf estimation is not determined by the number of input features, but by the (smaller) number of output features.

MRMI-SIG was previously used in an unsupervised fashion to minimize the mutual information between a set of outputs for the application of blind source separation (BSS) [34]. There are several differences between the formulation above and that used for BSS. The criterion for subspace projection involves maximization instead of minimization, only NO of the outputs of the rotation matrix are kept, mutual information is measured between the output feature set and the class label (instead of between the outputs) and the conditional (or joint) entropy does not disappear. Additional details on the entropy estimator given in equation (8), including proof of convergence when it is used for error entropy minimization, is given by Erdogmus et al.
[28].

5 FEATURE REDUCTION ALGORITHMS The previous section described the proposed information-theoretic algorithm, which uses the MRMISIG criterion. This section will briefly describe a  second sequential method that makes use of an information-theoretic criterion, two benchmark methods, and four simultaneous feature reduction methods. All eight of these feature reduction algorithms are listed in Table 1. Note that when no name is available for a given feature reduction method, in order to prevent the inclusion of more terminology than is necessary, the algorithm will be referred to by the name of the criterion which it uses.

The second information-theoretic sequential method, ED-QMI-SIG, is a slight variation of a criterion published by Principe et al. [19] and used by Torkkola for subspace projections [36]-[38]. It is similar to MRMI-SIG in that it uses Parzen windows with Gaussian kernels for the pdf estimation.
The difference between the two is the choice of the distance measure between the two relevant pdf’s, which for MI are: (1) the joint pdf of X and C, and (2) the product of the two marginal pdf’s. The distance measure for MRMI-SIG is loosely based on an approximation of Kullback-Leibler divergence [18], while ED-QMI-SIG uses Euclidean distance.
The criterion for this method is given in equation (11).

The two benchmark methods are the well-known Principal Components Analysis (PCA) [13], and Linear Discriminant Analysis (LDA, which is an extension of Fisher’s Discriminant Ratio [39]).
These methods use only second-order statistics, and the transformation matrix R for both methods consists of a set of NO eigenvectors of the appropriately defined matrix. As a result, the solution may be obtained analytically (i.e. non-iteratively). The main difference between these two methods is that PCA is unsupervised and LDA is supervised. In addition, the number of output features for LDA is restricted to be less than or equal to NC, the number of classes.

There are a total of four simultaneous feature reduction algorithms considered. The first two of these are Feature Ranking (FR) and Feature Selection (FS). The difference is that FR evaluates each feature independently of the others, while FS con 1 NT NC  N 1 Nj  argmax –log ------ ∑ G(x(k) – x(k – 1), 2σ2) + ∑ -N-----jlog ---- ∑ G(xj(k) – xj(k – 1), 2σ2) R NTk = 1 j = 1 T Njk = 1   siders all possible combinations of the NI inputs, taken NO at a time. Consequently, FS is expected to produce superior classification performance; however, the computational complexity suffers from combinatorial explosion. These methods produce at each output of the projection, a single, unweighted input feature. This differs from feature extraction, where each output feature is a weighted sum of all input features. This difference is completely characterized by the architecture, which for FR and FS is constrained to: (1) have only elements that are 0 or 1, and (2) have only one non-zero element in each row and in each column. Notice that, when this architecture is combined with either classifier, it is not possible to span the full space of linear projections. As a result, the performance may be needlessly reduced. In fact, it is trivial to construct a data set that will cause very poor performance for either method. The criterion for both is to minimize the (empirical) classification error.

The third simultaneous feature reduction method considered is the Mean Square Error (MSE) method, which trains the extractor “through” the classifier by using the MSE criterion at the output of the classifier. Notice that the outputs of both the Bayes-G and the Bayes-NP classifiers are ratios, unlike the classes which are necessarily nominal values, e.g. “person has diabetes” and “person does not have diabetes”. Therefore, to compare the classifier outputs with the class labels, either the ratios (classifier outputs) need to be converted to nominal values (class labels) or vice versa. The former case requires training through the MAX operator of Fig.
1 and involves a non-arbitrary process. This was the method used for FR and FS and is emulated by the MCE method, to be described next. The latter case, which applies to MSE, requires an arbitrary assignment. More specifically, if the training does not take place through the MAX operator, then each class label must be converted to a rather arbitrarily chosen (NC x 1) vector of ratios. This process is indicated in Fig. 1 by the DEMUX (de-multiplexer) operator.

The resulting criterion for this method may be expressed as a function of these targets as, NC NT argRmin -N----C-1--N----T-j ∑=1 k ∑=1 (yj(k) – τj(k))2 (12)  where τj(k) is the user-defined target for the jth output of the classifier at time k and yj(k) is given by either equation (1) or (2). The classification performance for this method depends on how well the subspace projector/classifier combination can approach the targets, which is itself dependent on how well the targets are chosen. Unfortunately, it is not clear how the targets should be chosen to optimize performance of the overall system (although it is much less difficult than for defining targets in the feature space). The 1 of NO scheme is commonly used, which is defined as setting the target associated with the correct class to 1, while all other targets are 0. If the MSE criterion were used to train the classifier, then this particular choice of targets produces the optimal (in the mean square sense) approximation of the a posteriori probabilities [11], [43], [62]. Note that the classifiers used here are not trained using MSE; instead, their outputs are determined using either equation (1) or (2). In this particular case, the concern is how to train the extractor.
Since it also appears to be a reasonable choice of targets for training the extractor, the 1 of NO scheme is used.

Minimum Classification Error (MCE) is the final feature reduction method considered. Notice that the term “minimum classification error”, as before, is used for both the name of the method and for the name of the criterion which it employs. In addition, the expression is used to characterize several criteria, such as FR and FS, since they attempt to minimize the empirical classification error. The MCE  criterion has also been referred to as Minimum Classification Error using Generalized Probabilistic Descent (MCE/GPD) [1], [3], Minimum Error Rate (MER) [33], and Discriminative Feature Extraction (DFE) [2], [44], [16]. It is also related to the Bayesian Back Propagation algorithm of Nedeljkovic [45]. The motivation behind this criterion is to approximate the decision process (the maximization in Fig. 1) using a function that is continuously differentiable. This allows a straightforward application of the stochastic gradient-based optimization method. This criterion can be described in the following manner. It modifies the parameters in an attempt to make the classifier output associated with the correct class have as much margin as possible with respect to the single classifier output having the largest value of all the outputs associated with the incorrect class. Mathematically, the criterion is given by equation (13) where,  has a value of 1 when the correct class for xj(k) is j and is 0 otherwise. There are two user-definable parameters, α and ν, for this criterion. As both approach infinity, the criterion becomes precisely the classification error.

There are many other options for feature reduction that are not included in the following comparison.
For example, there is an extension to LDA that removes the restriction on the number of possible output features [46]. Projection pursuit could be used to find interesting projections [47], as could unsupervised clustering methods [39]. The maximization of the mutual information could be performed in a simultaneous manner, i.e. through the classifier.
Either an SVM or MI could also be used to rank or select features [20]-[23], [48], [58]. For feature selection, greedy selection can be used to reduce the  computational burden of an exhaustive search, producing a computational complexity between that of FR and FS. Greedy algorithms, which are suboptimal, come in three forms; viz., forward selection, backward elimination, and stepwise regression [20].
Another option, which is not always applicable, is a method known as “branch and bound”. This approach is able to reduce the size of the exhaustive search space for FS without loss of optimality [49].
Other techniques include the construction of an MI matrix on which eigenanalysis is applied [22], the use frequency domain transforms [39], or the use of an unsupervised maximum entropy method [50].

6 COMPARISONS There are three data sets used for the comparisons, the important characteristics of which are listed in Table 2. These are the Pima, Landsat, and Letter Recognition data sets. All three of these may be found at the UCI Machine Learning Repository (http://www.ics.uci.edu/~mlearn/MLRepository.html). The data was first pre-processed. This included centering the data, sphering the data, and, for two of the data sets (Pima and Landsat), reducing the original dimension (to NI). The dimension was reduced using PCA by removing the eigendirections that had associated eigenvalues smaller than 0.5% that of the maximum eigenvalue. This same pre-processing was used for all the results included in this paper. One of the differences between the data sets is that the Pima data was determined to have outliers, which for purposes of this paper is defined as missing data points, e.g.
when a feature has a value of 0 even though a value of 0 is not meaningful or physically possible. These outliers are points in feature space that are statistically distant from the mean calculated using the remaining data (with the points in question removed). The Pima data set will be used with all invalid points included, which has the benefit of  helping to identify which feature reduction methods are sensitive to outliers.

The Bayes-G classifier was determined to provide the best classification performance for the Pima and Landsat data sets, while the Bayes-NP classifier was found to produce the best results for the Letter Recognition data set. Therefore, the results shown are restricted to these combinations of data sets and classifiers. It should be noted that, for the other combinations of data sets and classifiers, there is virtually no change in the order of the performance of the eight algorithms. When the Bayes-NP classifier is used, the kernel size, σ, is always set to 0.25, which was experimentally determined (using resubstitution) to be at or near the optimal value. The training was performed using the first NT samples of the overall data set, and then tested on the remaining data. For the gradient-based methods, the number of Monte Carlo runs is set to 10. The initial conditions for the projector for each Monte Carlo repetition were initialized randomly (zero for the first run and, thereafter, normally distributed with a variance of 1). Randomizing the initial conditions was done instead of randomly selecting the training set in hopes that it will facilitate other researcher’s attempts to duplicate the results and/or compare these results with their own work. This has the added benefit of simplifying the task of discerning which algorithms are susceptible to local minima.
The step size for each method was chosen small enough so that, after convergence, the variance of the parameter values have no, or very little, effect on classification performance (convergence of every adaptation was verified manually). As a result, the variation of classification performance for each feature reduction method can be expected to be caused by local maxima/minima.

The following user-defined parameters were determined using resubstitution. The kernel size, σ, for  MRMI-SIG is set to 0.25, 0.35, and 0.5 when NO is 1, 2-4, 5-8, respectively. Notice that the optimal value of σ increases as the number of output dimensions increases, partially offsetting the sparsity of data in high dimensions. Also notice that a fixed value of σ is used during training, which is in contrast to approaches by other researchers [51], [37], [19]. For ED-QMI-SIG, the resubstitution results suggest using a kernel size of 0.5, independent of NO. For MCE, the ν parameter was somewhat arbitrarily set to 10 and the optimal smoothing parameter, α, was determined to be 2. It should be mentioned that several combinations of α and NO were plagued by local minima. In particular, out of the 20 Monte Carlo runs associated with NO = 1, α = 1 and NO = 1, α = 4, the final classification performance (using resubstitution) was between 25-28% for 8 runs and between 64-65% for the remaining 12. Incidentally, local minima of the MCE algorithm can (if performed properly) be avoided by scheduling the value of the smoothing parameter, α. This can also be accomplished with the proposed information-theoretic criteria by annealing the kernel size, σ [52]. Since it is not known precisely how the scheduling should be done, and in order to simplify the experimental procedure, this is not done for either MCE or MRMI-SIG.

For the OH, the free parameter (which is referred to as the ν parameter by Mangasarian et al. [7]) was set to 0.1. For the SVM, pairwise training (as opposed to one vs. all) is used to combine the binary decisions for multi-class data. Unlike the other methods, the user-defined parameters for the SVM are not determined using resubstitution, since the resulting generalization performance was very poor.
This is particularly true for the Pima data set, where the resubstitution results are very negatively correlated with the generalization results. This is possibly due to the known weakness of the SVM, for the case that there is missing data [9], [10]. The usual solution is to use cross-validation [39] in place of resubstitution. However, since the SVM is used as a benchmark, and to ensure that it cannot be claimed that the training of the SVM was insufficient, the parameters are chosen using the best generalization performance. This resulted in the following parameters: penalty = 0.8, σ = 8, for the Pima dataset, and penalty = 0.5, σ = 2 for Landsat (no results are given  for the Letter Recognition data due to its size). The selection of the parameters by means of the generalization error gives the SVM a definite performance advantage over the other methods.

Not only is the criterion for each method optimized, but it is also “de-optimized” by maximizing instead of minimizing, or vice versa. This is done to see how well the criterion can manipulate the error rate in both directions and to give an indication of the worst performance possible, for sake of perspective. Random projections were also used, for which the coefficients were chosen uniformly in [-1,1].
These are always indicated by a dashed line in the following plots. Ideally, the results of the optimized criterion should be well above the results for random projections, and the results of the de-optimized criterion (signified in the plots by placing an asterisk after the name of the algorithm) well below the results for random projections. For PCA and LDA, the de-optimized results are found by selecting the eigenvectors associated with the minimum eigenvalues. For MCE, this was accomplished by maximizing the classification error rate and by changing ν from 10 to -10. The latter was done so that the margin for the output pertaining to the correct class was minimized with respect to the smallest output (of all incorrect classes). Figs. 2-4 show the plots of correct classification percentage, one for each data set, for both the optimized and de-optimized versions of each criterion. In each case, the left subplot shows the results for the PCA, LDA, ED-QMI-SIG and MRMI-SIG algorithms, as well as for the random projection. The right subplot shows the results for the FR, FS, MSE and MCE algorithms. Also shown on the right subplot, for sake of convenience, are the results for MRMI-SIG and the random projection.
In all cases, when NO = NI, the results for all algorithms are the same. This is because both classifiers are invariant under full-rank linear transforms.

The first plot, given in Fig. 2, pertains to the Pima data set. There are several important items to notice in this figure. For example, it is noteworthy that the performance improves by decreasing the dimensionality. Since the projections to a given dimension include as a subset all projections to lower dimensions, the cause of this must be due to inaccurate estimation of the parameters, either of the projector  or the classifier. This anomaly only occurred for the Pima data set, therefore it seems likely that the underlying cause is the existence of the outliers.
Another possible explanation concerns the generalization of the results for the Pima data set since it is the smallest of the three; however, the ratio of free parameters to amount of training data lies between that of the other two data sets (where the number of free parameters is based on the Bayes-G classifier and NO = NI/2). Another unexpected result was that  ED-QMI-SIG* (the “de-optimized” result) outperforms ED-QMI-SIG for NO = 2 and NO = 3. It could be that ED-QMI-SIG has local maxima whose performance is worse than that associated with one or more local minima, or that the “convergence” of several of the Monte Carlo runs was to a saddle point. In either case, the fact that this only occurred for the Pima data set, suggests the possibility that the ED-QMI-SIG algorithm is sensitive to outliers (an additional test, not included here, was performed that seems to verify this claim). Also notice that the result for both LDA and LDA* is only a single point, due to the limitation of the criterion as mentioned in Section 5. PCA* outperforms PCA for NO = 1, which is not too peculiar considering that PCA is an unsupervised method. The Landsat data set is shown in Fig. 3. Since the number of classes for this data is 6, LDA can be calculated for NO < 5. The  results from these two plots indicate that the dimension can be reduced from 8 to 2 without loss of performance. Results for the Letter Recognition data set are shown in Fig. 4, for which a slight improvement is provided by reducing the dimensionality by one-half. Due to the size of this data set, the projections were trained only for NO = 1, 2, 4, and 8.
Notice that no results are shown for the FS algorithm for NO > 4. This is due to the combinatorial explosion associated with this algorithm.

For all three data sets, as NO approaches NI, it becomes less and less important how the projection is chosen. This is shown in the figures in two different ways. First, the result for the random projection approaches the best result as NO approaches N .
I Second, there is a tendency for the difference between the highest and lowest performance (for optimizing and de-optimizing, respectively) to approach zero as NO approaches NI. Consequently, the projection to NO = 1 dimension is likely the most important data point in determining which algorithm performs the best. For generalization purposes, it is also important that an algorithm performs well for all output dimensions. Keep in mind, though, that when results are given that average over all possible output dimensions, the performance difference between the different algorithms  is necessarily deflated for the reason given above.
Consequently, the overall results are shown in two different ways. The upper subplot in Fig. 5 shows the overall results for NO = 1, averaged over all three data sets, where the mean value is shown in parentheses (results for de-optimization are not shown). The lower subplot of Fig. 5, on the other hand, shows the results for each algorithm averaged  over all data sets and all output dimensions. In the former bar graph, FR and FS can be seen to have identical results. This is because the algorithms become identical for NO = 1. In the latter bar graph, no results are shown for LDA or FS since they have missing data points (LDA because of the limitation on the number of outputs and FS because of the  Fig. 4. Classification performance versus output dimension, NO, for Letter Recognition data set.

training time involved), indicating that the two algorithms are not generally applicable.

For each of the gradient-based algorithms, Fig. 5 shows the minimum value, the mean value, and the maximum value of the 10 Monte Carlo runs. The performance for each of the gradient-based algorithms is not Gaussian distributed, otherwise + 3 sigma values would be given. The reason that they are not Gaussian distributed is due to the existence of local minima/maxima. The largest difference between the minimum and maximum results is for the random projection, as expected. The algorithms having the next largest difference is the MSE and MCE algorithms, while the two MI methods are  fairly consistent. These results indicate that the performance of the MSE and MCE algorithms are more sensitive to local optima than the MI methods. For the case that NO = 1, the MRMI-SIG algorithm outperformed all other feature reduction methods by 4.6% to 14.2% (a relative increase of 9% to 34%).
For the case that the results are averaged over all data sets and all output dimensions, MRMI-SIG outperformed all others by 2.0% to 7.7% (a relative increase of 3% to 12%).

It is interesting that the proposed sequential method has a mean classification error less than that produced by any of the simultaneous feature reduction methods, three of which use a criterion that spe cifically minimizes the classification error. This is, however, possible since the minimum classification error methods minimize the error on the finite training set, not on the disjoint finite test set. In order to prove that the error-based methods minimize the probability of error on the disjoint test set, “infinite training” is required, as acknowledged by Watanabe et al. [2] and Katagiri et al. [3]. This is essentially equivalent to knowing the underlying distributions, which would allow any of a number of methods to produce the optimum (Bayes) solution. While it seems perfectly reasonable to use this as a criterion when the data is finite, it is no longer guaranteed to be optimal. In fact, as suggested by the results presented here, it may very well be suboptimal because these methods focus only on training data that is near the decision boundary. This means that much of the already finite data is ignored, so that the results can be expected to be sensitive to outliers.

In order to see how the proposed feature reduction method compares with the SVM and the OH, these results are now given. Mixed in with these results are the best classification performances recorded in each of several different articles in the published literature (results shown without an associated reference are those obtained by the authors). For feature reduction methods, the value of NO is also included.
Keep in mind that the results for the MRMI-SIG algorithm are found using very simple classifiers, especially for the Pima and Landsat data, while the results from the literature are often the result of using sophisticated classifiers and/or training techniques, such as Boosting (which uses a committee of 3 learning machines) [53].

Pima: MRMI-SIG result is 79.1% using a Bayes-G classifier with NO = 1 z ~76% using LVQ with NO = 1 [54] z 76.1% using regularized AdaBoost [42] z 76.6% using Adaptive Margin SVM [42] z 78.7% using LVQ with NO = 3 [36] z 78.7% using OH z 79.5% using SVM z ~81% using LVQ with NO = 6 [54]  Landsat: MRMI-SIG result is 85.3% Bayes-G classifier with NO = 6  z ~78% using LVQ with NO = 1 [54] z 80.4% using an MLP with NO = 3 [22] z 83.8% using OH z 88.8% using AdaBoost [10] z 89.5% using LVQ with NO = 15 [30] z 90.5% using SVM z 93.3% using Bayesian (200 hidden units) [55]  Letter Recognition: MRMI-SIG result is 92.7% using a Bayes-NP classifier with NO = 8 z 79.3% using an MLP (7 hidden layer nodes) [15] z ~80% using Holland-style (1190 rules) [56] z 80.3% using an MLP with NO = 15 [22] z 80.5% using a Bayesian network [57] z 83.2% using OH z 88.6% using LVQ with NO = 8 [30] z 89.9% using k-NN [15] z 92.9% using AdaBoost [10]  Besides classification performance, there are implementation issues that are relevant in algorithm selection. Two such issues include algorithmic complexity and length of time required for training.
Complexity, in terms of computational requirements to update the parameters of the projection, is addressed first. In this regard, the PCA, LDA, FR, and FS methods have trivial complexity. The first two due to the existence of an analytical solution, the latter two because all the possible solutions are enumerated a priori (however, the number of solutions for FS may be very large). A good indication of the complexity of the gradient-based systems, on the other hand, is obtained by examining the criteria given previously in equations (9), (11), (12), and (13). Be mindful, the criteria for MSE and MCE are deceptively simple looking compared to those of the two MI methods. This is not so once one of the classifiers of equation (1) or (2) is inserted into (12) and (13). Also keep in mind that MCE and MSE are not as generally applicable, in that it is very difficult to use them with certain classifiers, such as k-NN. The time required for training is another important implementation issue, which is difficult to address since the experiment was not designed to minimize adaptation time for a given performance level. Nevertheless, the results of adaptation speed are given for the interested reader. These give a first-order estimate of the relative speed of the different methods. In all cases, the training (for a single Monte  Carlo run) took on the order of minutes or seconds.
For the Letter Recognition data set, however, several of the algorithms took significantly longer. For example, when transforming to NO = 8 output features, MRMI-SIG and ED-QMI-SIG took on the order of 2 hours, while the MSE and MCE methods took on the order of 24 hours. In addition, it was estimated that FS would take 3,075 hours, or 128 days, to complete.

7 CONCLUSION The MRMI-SIG method has very interesting implementation characteristics: it has small relative complexity/high adaptation speed (compared to MCE, MSE and sometimes FS); it allows the use of different dimensionality in the class labels and network outputs (unlike MSE); it is not as sensitive to local optima relative to the other gradient-based methods (unlike MCE and MSE); it is independent of the classifier (unlike FR, FS, MSE, MCE); and, it appears to be robust to outliers (compared to EDQMI-SIG, MCE, MSE, SVM, OH). In addition, it can be used to train nonlinear projectors [54], or used to train a classification system in a simultaneous manner. Also, the proposed method is nonparametric, does not require discretization of the data, and is computed directly from the samples. On the other hand, there are several shortcomings. It was mentioned earlier that there is no guarantee that it will produce the same solution as Shannon’s MI; however, results seem to indicate that this is not an issue (notice that it is not claimed that the results indicate that the proposed method produces the same solution as Shannon’s, only that it produces a useful solution). It takes longer to train (for large data sets) than PCA, LDA, or FR. Although the sensitivity to local minima is small, it is larger than for the SVM and all the methods that are not gradientbased (as expected). Also, as previously mentioned, there is a potential loss of performance as NO increases due to the required high-dimensional density estimation (but only when NO << NI, otherwise any method is sufficient).

The goal of this article was to answer two questions. The answer to the first question, “How does the performance of information-theoretic, sequential feature reduction methods compare to errorbased, simultaneous methods?” is “favorably”. This  conclusion is based on the classification performance and indicates that, for real (finite) data sets, samples not near the boundary carry information vital to the proper placement of the decision boundaries. Consequently, since simultaneous methods have a theoretical performance advantage, it would be interesting to use an MI criterion in a simultaneously-trained system (although this may prove to be prohibitively complex for most classifiers). The answer to the second question, “How does the performance of the best feature reduction method considered here, when combined with a simple classifier, compare to the SVM and the closely related Optimal Hyperplane?” is “reasonably well”, especially considering that the parameters of the SVM were chosen using the best generalization performance. In fact, the results here indicate that it would be interesting to combine an SVM with a feature reducer since the former appears to be robust to outliers and may help alleviate the lack of robustness of an SVM (see Weston et al. [9]).

Much more theoretical work is required to validate the mutual information approach for feature extraction in classification. The fundamental difficulty is related to the implicit link between mutual information and classification error, where the only known results are expressed in the form of upper and lower bounds on the Bayes classification error. However, the tightness of the bounds remains unknown. Probably a more productive approach with IT learning is to reverse the question of tuning the classifier topology to the feature extractor, and seek classifiers that will meet the minimization of the Bayes error with MI-derived features. Another important line of research, where some encouraging results using the Information Bottleneck method have been published by Tishby [61], is the optimization of the dimensionality of the feature space.

ACKNOWLEDGEMENTS Work partially supported by NSF ECS #9900394.