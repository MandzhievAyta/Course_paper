Fig. 1. Evolutionary vision-based robots. Left : The Khepera robot equipped with a linear camera (16 pixels, 36◦ FOV) was positioned in an arena with randomly sized black and white stripes. Random size was used to prevent development of trivial solutions whereby the control system would use the size of the stripes to measure distance from walls and self-motion. The robot was connected to a workstation through rotating contacts that provided serial data transmission and power supply. Right: The blimp-like ﬂying robot, provided with a similar linear camera (16 pixels, 150◦ FOV), is closed in a 5x5x3 m room with randomly sized black and white stripes on the walls. The serial data transmission is handled by a BluetoothTM wireless connection and power supply by an onboard battery.

but these developments have been limited to tethered aircraft [8] and simulated ﬂight [9].
The control systems of the robots mentioned above were ‘hand-designed’.
Some authors proposed to evolve vision-based navigation capabilities [10,11]. For example, Huber applied genetic algorithms to simulated 2D agents [12]. Those agents were equipped with only four photoreceptors making up two elementary motion detectors (EMD), symmetrically placed on each side of the agent. The parameters of those EMDs as well as the position and ﬁeld of view (FOV) of the photoreceptors were evolved. The best individuals could successfully navigate in a simulated corridor with textured walls and obstacles. The simulation was rather simple though, especially because inertial forces were not taken into consideration.
In previous work [13], we evolved the architecture of spiking neural networks capable of steering a vision-based, wheeled robot. A Khepera robot with a linear camera was asked to navigate in a rectangular arena with textured walls (ﬁgure 1, left). The best individuals were capable of moving forward and avoiding walls very reliably. However, the complexity of the dynamics of this terrestrial robot is much simpler than that of ﬂying agents.
In this paper, we extend that approach to a ﬂying robot (ﬁgure 1, right) that is expected to navigate within a room using only visual information. Genetic algorithms [14] are used to evolve the architecture of a spiking circuit, which connects low resolution visual input to the motors of a small indoor airship.
Notice that other teams are using blimps for studying insect-like vision-based  navigation [15,16], but none of them are applying the concepts of evolutionary robotics [17].
In the following section, we describe the main challenges of running evolution with real ﬂying systems and give an overview of the experimental setup. Section 3 summarizes the evolutionary and neural dynamics. The results are presented in section 4. Finally, a discussion and future work description are given in section 5.

Evolving aerial robots brings a new set of challenges. The major issues of developing (evolving, learning) a control system for an airship, with respect to a wheeled robot, are (1) the extension to three dimensions1, (2) the impossibility to communicate to a computer via cables, (3) the diﬃculty of deﬁning and measuring performance, and (4) the more complex dynamics. For example, while the Khepera is controlled in speed, the blimp is controlled in thrust (speed derivative) and can slip sideways. Moreover, inertial and aerodynamic forces play a major role. Artiﬁcial evolution is a promising method to automatically develop  1 Although the ﬁrst experiments described hereafter are limited to 2D by the use of a pre-designed altitude regulator.

control systems for complex robots [17], but it requires machines that are capable of moving for long periods of time without human intervention and withstand shocks.
Those requirements led us to the development of the blimp shown in ﬁgure 2. All onboard electronic components are connected to a Microchip PICTM microcontroller with a wireless connection to a desktop computer. The bidirectional digital communication with the computer is handled by a BluetoothTM radio module, allowing more than 15 m range. The energy is provided by a Lithium-Ion battery, which lasts more than 3 hours under normal operation, during evolutionary runs. For purpose of analysis, the evolutionary algorithm and spiking circuits are implemented on the desktop computer which exchanges sensory data and motor commands with the blimp every 100 ms.2 In these experiments, a simple linear camera is attached in front of the gondola (ﬁgure 3), pointing forward. The ﬁsh-eye-view lens gives a horizontal 150◦ FOV mapped onto 16 photoreceptors (subsampled from about 50 active pixels) whose activations are convolved with a Laplace ﬁlter. The Laplace ﬁlter detects contrast over three adjacent photoreceptors.

2 An adapted form of the evolutionary algorithm and spiking circuit could be run on the onboard microcontroller [18], but data analysis would be limited.

The evolutionary method and the spiking controller are very similar to what is described in [13]. The connectivity pattern and neuron signs of a network of 10 spiking neurons connected to 16 spiking visual receptors is genetically encoded and evolved using a standard genetic algorithm [14] with a population of 60 individuals sequentially evaluated on the same physical robot. The architecture is genetically represented by a binary string composed of a series of blocks, each block corresponding to a neuron. The ﬁrst bit of a block encodes the sign of the corresponding neuron (1, -1) and the remaining 26 bits encode the presence/absence (1, 0) of a connection from the 10 neurons and from the 16 visual receptors (ﬁgure 4). The synaptic strengths of all existing connections are set to 1. The spiking neuron model includes the response proﬁle of synaptic and neuron membranes to incoming spikes, time delays to account for axon length, and membrane recovery proﬁle of the refractory period [19]. The parameter values for the equations are predeﬁned and ﬁxed for all networks (no tuning has been done on these parameter values).
The population of 60 individuals is evolved using rank-based truncated selection, one-point crossover, bit mutation, and elitism [17]. The genetic strings of the ﬁrst generation are initialised randomly. After ranking the individuals according to their measured ﬁtness values, the top 15 individuals produce 4 copies each to create a new population of the same size and are randomly paired for  crossover. One-point crossover is applied to each pair with probability 0.1 and each individual is then mutated by switching the value of a bit with probability 0.05 per bit. Finally, a randomly selected individual is substituted by the original copy of the best individual of the previous generation (elitism).
Each individual of the population is tested on the robot two times for 40 seconds each (400 sensory-motor steps). The behaviour of an individual is evaluated by mean of the anemometer, which rotation speed is approximately proportional to the forward motion. The ﬁtness function is thus the amount of estimated forward motion vˆ at every time step t (100 ms) averaged over all T time steps available (T =800):  After each 40 s test, a preprogrammed random movement of 5 seconds is executed to create a randomised initial situation for the next test.

We performed ﬁve evolutionary runs, each starting with a diﬀerent random initialisation (ﬁgure 5, top left). All best evolved individuals of the ﬁve runs developed eﬃcient strategies in less than 20 generations (2-3 days) to navigate around the room in the forward direction. Interestingly, walls are actively used by the robot to stabilise the trajectory. The ﬁtness function (section 3) does not ask individuals to avoid walls, but only to maximise forward motion. The anemometer rotates only if the forward component of the speed vector is not null.
The trajectory of a typical best individual is shown on the top right plot of ﬁgure 5. It starts with a rotational movement due to the previous random movement (a) and keeps rotating until it hits a wall, which stabilises its course (b). It then moves straight forward until a wall is hit frontally (c), the motors are turned oﬀ, and the robot bumps backward. When the robot is free from the wall, the motors are turned on to move forward. Once again, a wall is used to stabilise the course and the same strategy is repeated over and over again.
The evolved spiking circuit clearly reacts when it hits the wall by turning oﬀ the motors, although the only input to the neural network is vision data. This behaviour can be seen also in the pattern of motor activity shown in the bottom graphs of ﬁgure 5, which indicates a strong correlation between a collision event3 and change of motor speeds. It is quite remarkable that such a simple evolved spiking circuit is able to detect collisions with so poor visual information about the environment, using only 16 photoreceptors as input.

3 Note that with the current setup, it is not possible to know which of the six bumpers is in contact with the walls. As a consequence, we cannot distinguish between a frontal and a side collision.

Fig. 5. Top left: Average ﬁtness values of ﬁve evolutionary runs (best ﬁtness = crosses, average ﬁtness = circles). Each data point is the average of ﬁve evolutionary runs starting with diﬀerent random initialisation of the chromosomes. Top right: Handdrawn estimation of the typical path of the selected best individual (solid line = forward movement; dashed line = backward movement; small curves = front collision; cross and circle = place of collision with left back bumper). Bottom: Performance of the best selected individual during two minutes. The upper graph shows collisions, as detected by the bumpers. The second graph shows an approximation of the forward speed, as measured with the anemometer. The motor graphs show the forward thrust of the propellers, which is given by the neural network output (‘Motors’ is the average of both motors). Each vertical line indicates the start of a collision. Multiple collisions on the topgraph are generated by the switches of the bumpers as the robot touches the walls.

These initial explorations with simple neuromorphic vision controllers for ﬂying robots indicate that artiﬁcial evolution can discover eﬃcient (and unexpected) solutions that capitalize on a combination of visual information and interaction dynamics between the physical system and its environment. These evolved solutions can not only encompass visual mechanisms already discovered in insects (such as forms of elementary motion detection), but also incorporate new “tricks” that may, or may not, be used by biological ﬂying organisms.
In current work, we are investigating the behavioural eﬀects of diﬀerent types of imaging devices (such as aVLSI retina) and preprocessing ﬁlters (temporal, spatial, spatio-temporal, EMDs, etc.). A 3D ﬂight simulator under development will help us to speed up evolutionary runs, let the sensor morphology evolve along with the controller, but will require proper handling of the issues related to the diﬀerences between simulation and real world. This last point will probably be approached by evolving hebbian-like synaptic plasticity, which we have shown to support fast self-adaptation to changing environments [20]. Eventually, our goal is to apply this approach to indoor slow ﬂyers with wings [1], instead of airships.

Acknowledgements. This work was supported by the Swiss NSF grant No.
620-58049. The authors are grateful to Jean-Daniel Nicoud (www.didel.com) for providing parts and support for building the blimp, and to Cyril Halter for the wind tunnel tests. Portescap (www.portescap.com) provided the motors equipping the blimp and Sensile (www.sensile.ch) the force gauges for the wind tunnel setup.