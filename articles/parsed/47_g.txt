Non-projective Dependency Parsing using Spanning Tree Algorithms  Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efﬁcient to learn and parse while still encoding much of the predicate-argument information needed in applications.

Dependency representations, which link words to their arguments, have a long history (Hudson, 1984).
Figure 1 shows a dependency tree for the sentence John hit the ball with the bat. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the ﬁgure. The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, the edges can be drawn above the words without crossings, or, equivalently, a word and its descendants form a contiguous substring of the sentence.

In English, projective trees are sufﬁcient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) and is by convention exclusively projective. However, there are certain examples in which a nonprojective tree is preferable. Consider the sentence John saw a dog yesterday which was a Yorkshire Terrier. Here the relative clause which was a Yorkshire Terrier and the object it modiﬁes (the dog) are separated by an adverb. There is no way to draw the dependency tree for this sentence in the plane with no crossing edges, as illustrated in Figure 2. In languages with more ﬂexible word order than English, such as German, Dutch and Czech, non-projective dependencies are more frequent. Rich inﬂection systems reduce reliance on word order to express  He is mostly not even interested in the new things and in most cases, he has no money for it either.

grammatical relations, allowing non-projective dependencies that we need to represent and parse efﬁciently. A non-projective example from the Czech Prague Dependency Treebank (Hajicˇ et al., 2001) is also shown in Figure 2.

Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. They test this system on Czech and show improved accuracy relative to a projective parser. Our approach differs from those earlier efforts in searching optimally and efﬁciently the full space of non-projective trees.

The main idea of our method is that dependency parsing can be formalized as the search for a maximum spanning tree in a directed graph. This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner, 1996) to yield efﬁcient O(n2) exact parsing methods for nonprojective languages like Czech. Using this spanning tree representation, we extend the work of McDonald et al. (2005) on online large-margin discrim inative training methods to non-projective dependencies.
The present work is related to that of Hirakawa (2001) who, like us, reduces the problem of dependency parsing to spanning tree search. However, his parsing method uses a branch and bound algorithm that is exponential in the worst case, even though it appears to perform reasonably in limited experiments. Furthermore, his work does not adequately address learning or measure parsing accuracy on held-out data.
Section 2 describes an edge-based factorization of dependency trees and uses it to equate dependency parsing to the problem of ﬁnding maximum spanning trees in directed graphs. Section 3 outlines the online large-margin learning framework used to train our dependency parsers. Finally, in Section 4 we present parsing results for Czech. The trees in Figure 1 and Figure 2 are untyped, that is, edges are not partitioned into types representing additional syntactic information such as grammatical function. We study untyped dependency trees mainly, but edge types can be added with simple extensions to the methods discussed here.

In what follows, x = x1 xn represents a generic input sentence, and y represents a generic dependency tree for sentence x. Seeing y as the set of tree edges, we write (i, j) 2 y if there is a dependency in y from word xi to word xj .
In this paper we follow a common method of factoring the score of a dependency tree as the sum of the scores of all edges in the tree. In particular, we deﬁne the score of an edge to be the dot product be tween a high dimensional feature representation of the edge and a weight vector,  Thus the score of a dependency tree y for sentence x is,  Assuming an appropriate feature representation as well as a weight vector w, dependency parsing is the task of ﬁnding the dependency tree y with highest score for a given sentence x.
For the rest of this section we assume that the weight vector w is known and thus we know the score s(i, j) of each possible edge. In Section 3 we present a method for learning the weight vector.

We represent the generic directed graph G = (V, E) by its vertex set V = fv1, . . . , vng and set E [1 : n] [1 : n] of pairs (i, j) of directed edges vi ! vj.
Each such edge has a score s(i, j). Since G is directed, s(i, j) does not necessarily equal s(j, i). A maximum spanning tree (MST) of G is a tree y E that maximizes the value P(i,j)∈y s(i, j) such that every vertex in V appears in y. The maximum projective spanning tree of G is constructed similarly except that it can only contain projective edges relative to some total order on the vertices of G. The MST problem for directed graphs is also known as the maximum arborescence problem.
For each sentence x we deﬁne the directed graph Gx = (Vx, Ex) given by  = fx0 = root, x1, . . . , xng = f(i, j) : i 6= j, (i, j) 2 [0 : n] [1 : n]g  That is, Gx is a graph with the sentence words and the dummy root symbol as vertices and a directed edge between every pair of distinct words and from the root symbol to every word. It is clear that dependency trees for x and spanning trees for Gx coincide, since both kinds of trees are required to be rooted at the dummy root and reach all the words in the sentence. Hence, ﬁnding a (projective) dependency tree with highest score is equivalent to ﬁnding a maximum (projective) spanning tree in Gx.

Chu-Liu-Edmonds(G, s) Graph G = (V, E) Edge weight function s : E → R 1. Let M = { (x∗, x) : x ∈ V, x∗ = arg maxx0 s(x0, x)} 2. Let GM = (V, M ) 3. If GM has no cycles, then it is an MST: return GM 4. Otherwise, ﬁnd a cycle C in GM 5. Let GC = contract(G, C, s) 6. Let y = Chu-Liu-Edmonds(GC, s) 7. Find a vertex x ∈ C s. t. (x0, x) ∈ y, (x00, x) ∈ C 8. return y ∪ C − { (x00, x)} contract(G = (V, E), C, s) 1. Let GC be the subgraph of G excluding nodes in C 2. Add a node c to GC representing cycle C 3. For x ∈ V − C : ∃x0∈C(x0, x) ∈ E Add edge (c, x) to GC with s(c, x) = maxx0∈C s(x0, x) 4. For x ∈ V − C : ∃x0∈C(x, x0) ∈ E Add edge (x, c) to GC with s(x, c) = maxx0∈C [s(x, x0) − s(a(x0), x0) + s(C)] where a(v) is the predecessor of v in C and s(C) = Pv∈C s(a(v), v) 5. return GC  To ﬁnd the highest scoring non-projective tree we simply search the entire space of spanning trees with no restrictions. Well-known algorithms exist for the less general case of ﬁnding spanning trees in undirected graphs (Cormen et al., 1990).

Efﬁcient algorithms for the directed case are less well known, but they exist. We will use here the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967), sketched in Figure 3 following Leonidas (2003). Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight. If a tree results, it must be the maximum spanning tree. If not, there must be a cycle. The procedure identiﬁes a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle. It can be shown that a maximum spanning tree on the contracted graph is equivalent to a maximum spanning tree in the original graph (Leonidas, 2003). Hence the algorithm can recursively call itself on the new graph. Naively, this algorithm runs in O(n3) time since each recursive call takes O(n2) to ﬁnd the highest incoming edge for each word and to contract the graph. There are at most O(n) recursive calls since we cannot contract the graph more then n times. However,  Tarjan (1977) gives an efﬁcient implementation of the algorithm with O(n2) time complexity for dense graphs, which is what we need here.
To ﬁnd the highest scoring non-projective tree for a sentence, x, we simply construct the graph Gx and run it through the Chu-Liu-Edmonds algorithm.
The resulting spanning tree is the best non-projective dependency tree. We illustrate here the application of the Chu-Liu-Edmonds algorithm to dependency parsing on the simple example x = John saw Mary, with directed graph representation Gx,  John 30 Mary If the result were a tree, it would have to be the maximum spanning tree. However, in this case we have a cycle, so we will contract it into a single node and recalculate edge weights according to Figure 3.

31 The new vertex wjs represents the contraction of vertices John and saw. The edge from wjs to Mary is 30 since that is the highest scoring edge from any vertex in wjs. The edge from root into wjs is set to 40 since this represents the score of the best spanning tree originating from root and including only the vertices in wjs. The same leads to the edge from Mary to wjs. The fundamental property of the Chu-Liu-Edmonds algorithm is that an MST in this graph can be transformed into an MST in the original graph (Leonidas, 2003). Thus, we recursively call the algorithm on this graph. Note that we need to keep track of the real endpoints of the edges into and out of wjs for reconstruction later. Running the algorithm, we must ﬁnd the best incoming edge to all words  This is a tree and thus the MST of this graph. We now need to go up a level and reconstruct the graph.
The edge from wjs to Mary originally was from the word saw, so we include that edge. Furthermore, the edge from root to wjs represented a tree from root to saw to John, so we include all those edges to get the ﬁnal (and correct) MST, root  A possible concern with searching the entire space of spanning trees is that we have not used any syntactic constraints to guide the search. Many languages that allow non-projectivity are still primarily projective. By searching all possible non-projective trees, we run the risk of ﬁnding extremely bad trees.
We address this concern in Section 4.

2.2.2 Projective Trees It is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996). This algorithm has a runtime of O(n3) and has been employed successfully in both generative and discriminative parsing models (Eisner, 1996; McDonald et al., 2005). Furthermore, it is trivial to show that the Eisner algorithm solves the maximum projective spanning tree problem.
The Eisner algorithm differs signiﬁcantly from the Chu-Liu-Edmonds algorithm. First of all, it is a bottom-up dynamic programming algorithm as opposed to a greedy recursive one. A bottom-up algorithm is necessary for the projective case since it must maintain the nested structural constraint, which is unnecessary for the non-projective case.

In the preceding discussion, we have shown that natural language dependency parsing can be reduced to ﬁnding maximum spanning trees in directed graphs.
This reduction results from edge-based factorization and can be applied to projective languages with  the Eisner parsing algorithm and non-projective languages with the Chu-Liu-Edmonds maximum spanning tree algorithm. The only remaining problem is how to learn the weight vector w.
A major advantage of our approach over other dependency parsing models is its uniformity and simplicity. By viewing dependency structures as spanning trees, we have provided a general framework for parsing trees for both projective and nonprojective languages. Furthermore, the resulting parsing algorithms are more efﬁcient than lexicalized phrase structure approaches to dependency parsing, allowing us to search the entire space without any pruning. In particular the non-projective parsing algorithm based on the Chu-Liu-Edmonds MST algorithm provides true non-projective parsing. This is in contrast to other non-projective methods, such as that of Nivre and Nilsson (2005), who implement non-projectivity in a pseudo-projective parser with edge transformations. This formulation also dispels the notion that non-projective parsing is “harder” than projective parsing. In fact, it is easier since non-projective parsing does not need to enforce the non-crossing constraint of projective trees.
As a result, non-projective parsing complexity is just O(n2), against the O(n3) complexity of the Eisner dynamic programming algorithm, which by construction enforces the non-crossing constraint.

In this section, we review the work of McDonald et al. (2005) for online large-margin dependency parsing. As usual for supervised learning, we assume a training set T = f(xt, yt)gtT=1, consisting of pairs of a sentence xt and its correct dependency tree yt.
In what follows, dt(x) denotes the set of possible dependency trees for sentence x.
The basic idea is to extend the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2003) to learning with structured outputs, in the present case dependency trees.
Figure 4 gives pseudo-code for the MIRA algorithm as presented by McDonald et al. (2005). An online learning algorithm considers a single training instance at each update to w. The auxiliary vector v accumulates the successive values of w, so that the ﬁnal weight vector is the average of the weight vec Training data: T = { (xt, yt)} tT=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. for t : 1..T 4. sm.ti.ns‚‚‚(xwt(,i+yt1)) −− sw((xi)t‚‚‚ , y0) ≥ L(yt, y0), ∀y0 ∈ dt(xt) 5. v = v + w(i+1) 6. i = i + 1 7. w = v/(N ∗ T )  tors after each iteration. This averaging effect has been shown to help overﬁtting (Collins, 2002).
On each update, MIRA attempts to keep the new weight vector as close as possible to the old weight vector, subject to correctly classifying the instance under consideration with a margin given by the loss of the incorrect classiﬁcations. For dependency trees, the loss of a tree is deﬁned to be the number of words with incorrect parents relative to the correct tree. This is closely related to the Hamming loss that is often used for sequences (Taskar et al., 2003).
For arbitrary inputs, there are typically exponentially many possible parses and thus exponentially many margin constraints in line 4 of Figure 4.

One solution for the exponential blow-up in number of trees is to relax the optimization by using only the single margin constraint for the tree with the highest score, s(x, y). The resulting online update (to be inserted in Figure 4, line 4) would then be:  min s.t. s(xt, yt) s(xt, y0) L(yt, y0) where y0 = arg maxy0 s(xt, y0)  McDonald et al. (2005) used a similar update with k constraints for the k highest-scoring trees, and showed that small values of k are sufﬁcient to achieve the best accuracy for these methods. However, here we stay with a single best tree because kbest extensions to the Chu-Liu-Edmonds algorithm are too inefﬁcient (Hou, 1996).
This model is related to the averaged perceptron algorithm of Collins (2002). In that algorithm, the single highest scoring tree (or structure) is used to update the weight vector. However, MIRA aggressively updates w to maximize the margin between  the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy.

It is also possible to exploit the structure of the output space and factor the exponential number of margin constraints into a polynomial number of local constraints (Taskar et al., 2003; Taskar et al., 2004).
For the directed maximum spanning tree problem, we can factor the output by edges to obtain the following constraints:  min w(i+1) w(i) s.t. s(l, j) s(k, j) 1 8(l, j) 2 yt, (k, j) 2/ yt  This states that the weight of the correct incoming edge to the word xj and the weight of all other incoming edges must be separated by a margin of 1.
It is easy to show that when all these constraints are satisﬁed, the correct spanning tree and all incorrect spanning trees are separated by a score at least as large as the number of incorrect incoming edges.
This is because the scores for all the correct arcs cancel out, leaving only the scores for the errors causing the difference in overall score. Since each single error results in a score increase of at least 1, the entire score difference must be at least the number of errors. For sequences, this form of factorization has been called local lattice preference (Crammer et al., 2004). Let n be the number of nodes in graph Gx.
Then the number of constraints is O(n2), since for each node we must maintain n 1 constraints.
The factored constraints are in general more restrictive than the original constraints, so they may rule out the optimal solution to the original problem. McDonald et al. (2005) examines brieﬂy factored MIRA for projective English dependency parsing, but for that application, k-best MIRA performs as well or better, and is much faster to train.

We performed experiments on the Czech Prague Dependency Treebank (PDT) (Hajicˇ, 1998; Hajicˇ et al., 2001). We used the predeﬁned training, development and testing split of this data set. Furthermore, we used the automatically generated POS tags that are provided with the data. Czech POS tags are very  complex, consisting of a series of slots that may or may not be ﬁlled with some value. These slots represent lexical and grammatical properties such as standard POS, case, gender, and tense. The result is that Czech POS tags are rich in information, but quite sparse when viewed as a whole. To reduce sparseness, our features rely only on the reduced POS tag set from Collins et al. (1999). The number of features extracted from the PDT training set was 13, 450, 672, using the feature set outlined by McDonald et al. (2005).
Czech has more ﬂexible word order than English and as a result the PDT contains non-projective dependencies. On average, 23% of the sentences in the training, development and test sets have at least one non-projective dependency. However, less than 2% of total edges are actually non-projective. Therefore, handling non-projective edges correctly has a relatively small effect on overall accuracy. To show the effect more clearly, we created two Czech data sets. The ﬁrst, Czech-A, consists of the entire PDT.
The second, Czech-B, includes only the 23% of sentences with at least one non-projective dependency.
This second set will allow us to analyze the effectiveness of the algorithms on non-projective material. We compared the following systems:  Results are shown in Table 1. There are two main metrics. The ﬁrst and most widely recognized is Accuracy, which measures the number of words that correctly identiﬁed their parent in the tree. Complete measures the number of sentences in which the resulting tree was completely correct.
Clearly, there is an advantage in using the ChuLiu-Edmonds algorithm for Czech dependency pars COLL1999 N&N2005 McD2005 Single-best MIRA Factored MIRA  ing. Even though less than 2% of all dependencies are non-projective, we still see an absolute improvement of up to 1.1% in overall accuracy over the projective model. Furthermore, when we focus on the subset of data that only contains sentences with at least one non-projective dependency, the effect is ampliﬁed. Another major improvement here is that the Chu-Liu-Edmonds non-projective MST algorithm has a parsing complexity of O(n2), versus the O(n3) complexity of the projective Eisner algorithm, which in practice leads to improvements in parsing time. The results also show that in terms of Accuracy, factored MIRA performs better than single-best MIRA. However, for the factored model, we do have O(n2) margin constraints, which results in a signiﬁcant increase in training time over single-best MIRA. Furthermore, we can also see that the MST parsers perform favorably compared to the more powerful lexicalized phrase-structure parsers, such as those presented by Collins et al. (1999) and Zeman (2004) that use expensive O(n5) parsing algorithms. We should note that the results in Collins et al. (1999) are different then reported here due to different training and testing data sets.
One concern raised in Section 2.2.1 is that searching the entire space of non-projective trees could cause problems for languages that are primarily projective. However, as we can see, this is not a problem. This is because the model sets its weights with respect to the parsing algorithm and will disfavor features over unlikely non-projective edges.
Since the space of projective trees is a subset of the space of non-projective trees, it is natural to wonder how the Chu-Liu-Edmonds parsing algorithm performs on projective data since it is asymptotically better than the Eisner algorithm. Table 2 shows the results for English projective dependency trees extracted from the Penn Treebank (Marcus et al., 1993) using the rules of Yamada and Matsumoto (2003).

We presented a general framework for parsing dependency trees based on an equivalence to maximum spanning trees in directed graphs. This framework provides natural and efﬁcient mechanisms for parsing both projective and non-projective languages through the use of the Eisner and Chu-LiuEdmonds algorithms. To learn these structures we used online large-margin learning (McDonald et al., 2005) that empirically provides state-of-the-art performance for Czech.
A major advantage of our models is the ability to naturally model non-projective parses. Nonprojective parsing is commonly considered more difﬁcult than projective parsing. However, under our framework, we show that the opposite is actually true that non-projective parsing has a lower asymptotic complexity. Using this framework, we presented results showing that the non-projective model outperforms the projective model on the Prague Dependency Treebank, which contains a small number of non-projective edges.
Our method requires a tree score that decomposes according to the edges of the dependency tree. One might hope that the method would generalize to  include features of larger substructures. Unfortunately, that would make the search for the best tree intractable (Ho¨ffgen, 1993).

We thank Lillian Lee for bringing an important missed connection to our attention, and Koby Crammer for his help with learning algorithms. This work has been supported by NSF ITR grants 0205448 and 0428193.