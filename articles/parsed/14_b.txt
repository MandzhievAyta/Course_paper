The information overload is a well-known phenomenon of the information age, since due to the progress in computer power and storage capacity over the last decades, data is produced at an incredible rate, and our ability to collect and store these data is increasing at a faster rate than our ability to analyze it.
But, the analysis of these massive, typically messy and inconsistent, volumes of data is crucial in many application domains. For decision makers, analysts or emergency response teams it is an essential task to rapidly extract relevant information from the ood of data. Today, a selected number of software tools is employed to help analysts to organize their information, generate overviews and explore the information space in order to extract potentially useful information.
Most of these data analysis systems still rely on interaction metaphors developed more than a decade ago and it is questionable whether they are able to meet the demands of the ever-increasing mass of information. In fact, huge investments in time and money are often lost, because we still lack the possibilities to  properly interact with the databases. Visual analytics aims at bridging this gap by employing more intelligent means in the analysis process. The basic idea of visual analytics is to visually represent the information, allowing the human to directly interact with the information, to gain insight, to draw conclusions, and to ultimately make better decisions. The visual representation of the information reduces complex cognitive work needed to perform certain tasks. People may use visual analytics tools and techniques to synthesize information and derive insight from massive, dynamic, and often con icting data by providing timely, defensible, and understandable assessments.
The goal of visual analytics research is to turn the information overload into an opportunity. Decision-makers should be enabled to examine this massive, multi-dimensional, multi-source, time-varying information stream to make e ective decisions in time-critical situations. For informed decisions, it is indispensable to include humans in the data analysis process to combine exibility, creativity, and background knowledge with the enormous storage capacity and the computational power of today's computers. The speci c advantage of visual analytics is that decision makers may focus their full cognitive and perceptual capabilities on the analytical process, while allowing them to apply advanced computational capabilities to augment the discovery process. This paper gives an overview on visual analytics, and discusses the most important research challenges in this eld. Real world application examples are presented that show how visual analytics can help to turn information overload as generated by today's applications into useful information.
The rest of the paper is organized as follows: section 2 de nes visual analytics and discusses its scope. The visual analytics process is formalized in section 3.
Section 4 covers the 10 most important application challenges in the eld and presents some approaches addressing these problems. It is followed by the 10 most important technical challenges in section 5. Finally, section 6 concludes our work and gives a short outlook of the future of visual analytics.

In general, visual analytics can be described as \the science of analytical reasoning facilitated by interactive visual interfaces" [1]. To be more precise, visual analytics is an iterative process that involves information gathering, data preprocessing, knowledge representation, interaction and decision making. The ultimate goal is to gain insight in the problem at hand which is described by vast amounts of scienti c, forensic or business data from heterogeneous sources. To reach this goal, visual analytics combines the strengths of machines with those of humans. On the one hand, methods from knowledge discovery in databases (KDD), statistics and mathematics are the driving force on the automatic analysis side, while on the other hand human capabilities to perceive, relate and conclude turn visual analytics into a very promising eld of research.
Historically, visual analytics has evolved out of the elds of information and scienti c visualization. According to Colin Ware, the term visualization is mean while understood as \a graphical representation of data or concepts" [2], while the term was formerly applied to form a mental image. Nowadays fast computers and sophisticated output devices create meaningful visualizations and allow us not only to mentally visualize data and concepts, but also to see and explore an exact representation of the data under consideration on a computer screen.
However, the transformation of data into meaningful visualizations is not a trivial task that will automatically improve through steadily growing computational resources. Very often, there are many di erent ways to represent the data under consideration and it is unclear which representation is the best one. State-ofthe-art concepts of representation, perception, interaction and decision-making need to be applied and extended to be suitable for visual data analysis.

The elds of information and scienti c visualization deal with visual representations of data. The main di erence among the two is that scienti c visualization examines potentially huge amounts of scienti c data obtained from sensors, simulations or laboratory tests. Typical scienti c visualization applications are ow visualization, volume rendering, and slicing techniques for medical illustrations.
In most cases, some aspects of the data can be directly mapped onto geographic coordinates or into virtual 3D environments. We de ne Information visualization more generally as the communication of abstract data relevant in terms of action through the use of interactive interfaces. There are three major goals of visualization, namely a) presentation, b) con rmatory analysis, and c) exploratory analysis. For presentation purposes, the facts to be presented are xed a priori, and the choice of the appropriate presentation technique depends largely on the user. The aim is to e ciently and e ectively communicate the results of an analysis. For con rmatory analysis, one or more hypotheses about the data serve as a starting point. The process can be described as a goal-oriented examination of these hypotheses. As a result, visualization either con rms these hypotheses or rejects them. Exploratory data analysis as the process of searching and analyzing databases to nd implicit but potentially useful information, is a di cult task.
At the beginning, the analyst has no hypothesis about the data. According to John Tuckey, tools as well as understanding are needed [3] for the interactive and usually undirected search for structures and trends.

Visual analytics is more than only visualization. It can rather be seen as an integral approach combining visualization, human factors and data analysis.
Figure 1 illustrates the detailed scope of visual analytics. Concerning the eld of visualization, visual analytics integrates methodology from information analytics, geospatial analytics, and scienti c analytics. Especially human factors (e.g., interaction, cognition, perception, collaboration, presentation, and dissemination) play a key role in the communication between human and computer, as well as in the decision-making process. In this context, production is de ned as the creation of materials that summarize the results of an analytical e ort, presentation as the packaging of those materials in a way that helps the audience understand the analytical results in context using terms that are meaningful to them, and dissemination as the process of sharing that information with the intended audience [4]. In matters of data analysis, visual analytics further more pro ts from methodologies developed in the elds of data management & knowledge representation, knowledge discovery and statistical analytics. Note that visual analytics, is not likely to become a separate eld of study [5], but its in uence will spread over the research areas it comprises.
According to Jarke J. van Wijk, \visualization is not 'good' by de nition, developers of new methods have to make clear why the information sought cannot be extracted automatically" [6]. From this statement, we immediately see the need for the visual analytics approach using automatic methods from statistics, mathematics and knowledge discovery in databases (KDD) wherever they are applicable. Visualization is used as a means to e ciently communicate and explore the information space when automatic methods fail. In this context, human background knowledge, intuition and decision-making either cannot be automated or serve as input for the future development of automated processes.
Overlooking a large information space is a typical visual analytics problem.
In many cases, the information at hand is con icting and needs to be integrated from heterogeneous data sources. Moreover, the system lacks knowledge that is still hidden in the expert's mind. By applying analytical reasoning, hypotheses about the data can be either a rmed or discarded and eventually lead to a better understanding of the data, thus supporting the analyst in his task to gain insight.
Contrary to this, a well-de ned problem where the optimum or a good estimation can be calculated by non-interactive analytical means would rather not be described as a visual analytics problem. In such a scenario, the non-interactive  analysis should be clearly preferred due to e ciency reasons. Likewise, visualization problems not involving methods for automatic data analysis do not fall into the eld of visual analytics.
The elds of visualization and visual analytics both build upon methods from scienti c analytics, geospatial analytics and information analytics. They both pro t from knowledge out of the eld of interaction as well as cognitive and perceptual science. They do di erentiate in so far as visual analytics furthermore integrates methodology from the elds of statistical analytics, knowledge discovery, data management & knowledge representation and presentation, production & dissemination.

In this section we provide a formal description of the visual analytics process. As described in the last section the input for the data sets used in the visual analytics process are heterogeneous data sources (i.e., the internet, newspapers, books, scienti c experiments, expert systems). From these rich sources, the data sets S = S1; : : : ; Sm are chosen, whereas each Si; i 2 (1; ::; n) consists of attributes Ai1; : : : ; Aik. The goal or output of the process is insight I. Insight is either directly obtained from the set of created visualizations V or through con rmation of hypotheses H as the results of automated analysis methods. We illustrated this formalization of the visual analytics process in Figure 2. Arrows represent the transitions from one set to another one.

More formal the visual analytics process is a transformation F : S ! I, whereas F is a concatenation of functions f 2 fDW ; VX ; HY ; UZ g de ned as follows:  DW describes the basic data pre-processing functionality with DW : S ! S and W 2 fT; C; SL; Ig including data transformation functions DT , data cleaning functions DC , data selection functions DSL and data integration functions DI that are needed to make analysis functions applicable to the data set.
VW ; W 2 fS; Hg symbolizes the visualization functions, which are either functions visualizing data VS : S ! V or functions visualizing hypotheses VH : H ! V .

HY ; Y 2 fS; V g represents the hypothesis generation process. We distinguish between functions that generate hypotheses from data HS : S ! H and functions that generate hypotheses from visualizations HV : V ! H.
Moreover, user interactions UZ ; Z 2 fV; H; CV; CH g are an integral part of the visual analytics process. User interactions can either e ect only visualizations UV : V ! V (i.e., selecting or zooming), or can e ect only hypotheses UH : H ! H by generating a new hypotheses from given ones. Furthermore, insight can be concluded from visualizations UCV : V ! I or from hypothesis UCH : H ! I The typical data pre-processing applying data cleaning, data integration and data transformation functions is de ned as DP = DT (DI (DC (S1; : : : ; Sn))). After the pre-processing step either automated analysis methods HS = ffs1; : : : ; fsqg (i.e., statistics, data mining, etc.) or visualization methods VS : S ! V; VS = ffv1; : : : ; fvsg are applied to the data, in order to reveal patterns as shown in Figure 2.
The application of visualization methods can hereby directly provide insight to the user, described by UCV ; the same applies to automatic analysis methods UCH . However, most application scenarios may require user interaction to re ne parameters in the analysis process and to steer the visualization process.
This means that after having obtained initial results from either the automatic analysis step or the visualization step, the user may re ne the achieved results by applying another data analysis step, expressed by UV and UH . Furthermore visualization methods can be applied to the results of the automated analysis step to transform a hypotheses into a visual representation VH or the ndings extracted from visualizations may be validated through an data analysis step to generated a hypotheses HV . F(S) is rather an iterative process than a single application of each provided function, as indicated by the feedback loop in Figure 2. The user may re ne input parameters or focus on di erent parts of the data in order to validate generated hypotheses or extracted insight.
We take a visual analytics application for monitoring network security as an example. Within the network system, four sensors measure the network tra c resulting in four data sets S1; : : : ; S4. While preprocessing, the data is cleaned from missing values and unnecessary data using the data cleaning function dc, integrated using di (each measurement system stores data slightly di erent), and transformed in a format suitable for our analysis using dt. We now select UDP and TCP tra c for our analysis with the function ds, resulting in S0 = ds(dt(di(dc(S1; : : : ; S4)))). For further analysis, we apply a data mining algorithm hs to search for security incidents within the tra c generating a hypothesis h0 = hs(S0). To better understand this hypothesis, we visualize it using  the function vh: v0 = vh(h0). Interactive adjustment of the parameters results in v00 = uv(v0), revealing a correlation of the incidents from two speci c source networks. By applying the function hv, we obtain a distribution of networks where similar incidents took place h00 = hv(v00). This leads to the insight that a speci c network worm tries to communicate with our network from 25 source networks i0 = uch(h00). Repeating the same process at a later date by using the feedback loop reveals a much higher spread of the virus, emphasizing the need to take countermeasures.
Unlike described in the information seeking mantra (\overview rst, zoom/ lter, details on demand") [7], the visual analytics process comprises the application of automatic analysis methods before and after the interactive visual representation is used like demonstrated in the example. This is primarily due to the fact that current and especially future data sets are complex on the one hand and too large to be visualized straightforward on the other hand. Therefore, we present the visual analytics mantra:  \Analyse First Show the Important Zoom, Filter and Analyse Further Details on Demand"  For the advancement of the research eld of visual analytics several application and technical challenges need to be mastered. In this section, we present the ten most signi cant application challenges and discuss them in the context of research projects trying to solve the challenges. Both the application (this section), as well as the technical challenges (next section) were identi ed by the panel discussion on the Workshop on Visual Analytics in 2005 [8].

One major eld in the area of visual analytics covers physics and astronomy, including applications like ow visualization, uid dynamics, molecular dynamics, nuclear science and astrophysics, to name just a few of them.
Especially the research eld of astrophysics o ers a wide variety of usage scenarios for visual analytics. Never before in history scientists had the ability to capture so much information about the universe. Massive volumes of unstructured data, originating from di erent directions of the orbit and covering the whole frequency spectrum, form continuous streams of terabytes of data that can be recorded and analysed. The amount of data is so high that it far exceeds the ability of humans to consider it all. By common data analysis techniques like knowledge discovery, astronomers can nd new phenomena, relationships and useful knowledge about the universe, but although a lot of the data only consists of noise, a visual analytics approach can help separating relevant data  from noise and help identifying unexpected phenomena inside the massive and dynamic data streams. One celebrated example is the Sloan Digital Sky Survey [10] and the COMPLETE project [11], generating terabytes of astrophysics data each day, or the Large Hadron Collider (LHC) at CERN which generates a volume of 1 petabyte of data per year.
One example for a visual analytics application is the simulation of a Supernova. The SciDAC program has brought together tremendous scienti c expertise and computing resources within the Terascale Supernova Initiative (TSI) project to realize the promise of terascale computing for attempting to answer some of the involved questions [9]. A complete understanding of core collapse supernovae requires 3D simulations of the turbulence, rotation, radiation, magnetic elds and gravitational forces, producing tens of terabytes of data per simulation. As an examination of this massive amount of data in a numeric format would simply exceed human capabilities and would therefore not give an insight into the processes involved, a visual approach (see Fig. 3) can help analyzing these processes on a higher aggregated level in order to draw conclusions and extract knowledge from it.

Another major eld in the area of visual analytics covers business applications.
The nancial market with its thousands of di erent stocks, bonds, futures, commodities, market indices and currencies generates a lot of data every second, which accumulates to high data volumes throughout the years. The main chal lenge in this area lies in analyzing the data under multiple perspectives and assumptions to understand historical and current situations, and then monitoring the market to forecast trends and to identify recurring situations. Visual analytics applications can help analysts obtaining insights and understanding into previous stock market development, as well as supporting the decision making progress by monitoring the stock market in real-time in order to take necessary actions for a competitive advantage, with powerful means that reach far beyond the numeric technical chart analysis indicators or traditional line charts. One popular application in this eld is the well-known Smartmoney [13], which gives an instant visual overview of the development of the stock market in particular sectors for a user-de nable time frame. A new application in this eld is the FinDEx system [12] (see Fig. 4), which allows a visual comparison of a fund's performance to the whole market for all possible time intervals at one glance.

Monitoring climate and weather is also a domain which involves huge amounts of data collected throughout the world or from satellites in short time intervals, easily accumulating to terabytes per day. Applications in this domain most often do not only visualize snapshots of a current situation, but also have to generate sequences of previous developments and forecasts for the future in order to analyse certain phenomena and to identify the factors responsible for a development, thus enabling the decision maker to take necessary countermeasures (like the global reduction of carbon dioxide emissions in order to reduce global  warming). The applications for climate modeling and climate visualization can cover all possible time intervals, from daily weather forecasts which operate in rather short time frames of several days, to more complex visualizations of climate changes that can expand to thousands of years. A visual approach can easily help to interpret these massive amounts of data and to gain insight into the dependencies of climate factors and climate change scenarios that would otherwise not be easily identi ed. Besides weather forecasts, existing applications for instance visualize the global warming, melting of the poles, the stratospheric ozone depletion, hurricane warnings or oceanography, to name just a few.

Despite the slowly arising environmental changes like global warming that have been mentioned above, environmental or other disasters can face us as sudden major catastrophes. In the domain of emergency management, visual analytics can help determining the on-going progress of an emergency and can help identifying the next countermeasures (construction of physical countermeasures or evacuation of the population) that must be taken to limit the damage. Such scenarios can include natural or meteorological catastrophes like ood or waves, volcanos, storm, re or epidemic growth of diseases (bird u), but also humanmade technological catastrophes like industrial accidents, transport accidents or pollution. Depending on the particular case, visual analytics can help to determine the amount of damage, to identify objectives, to assign priorities, and to provide e ective coordination for various organizations for more e cient help in the disaster zone.

Visual analytics for security is an important research topic and is strongly supported by the U.S. government. The application eld in this sector is wide, ranging from terrorism informatics over border protection to network security.
In these elds, the challenges lie in getting all the information together and linking numerous incidents to nd correlations.
A demonstrative example of work in the eld is the situational awareness display VisAware [14] which is built upon the w3 premise, assuming that every incident has at least the three attributes what, when, and where (see Fig. 5). In this display, the location attribute is placed on a map, the time attribute indicated on concentric circles around this map, and the classi cation of the incident is mapped to the angle around the circle. For each incident, the attributes are linked through lines. Other examples in the eld are [15] and [16].

Visual software analytics has become a popular research area, and as modern software packages often consist of millions of code lines it can support a faster understanding of the structure of a software package with its dependencies. Visual  Shaun Moon, ‡ College of Architecture+Planning University of Utah  Stefano Foresti§ Center for High Performance Computing University of Utah  Fig. 5. VisAware f(ocr)BBioWioaWtchat(cch 2005 IEEE) [14].

el visualization paradigm for situational awareness.

r situaerse set pproach ch leads ntuitive traction revious e use of  visualow that broader ion that at each Where , which correlaour apn a col analytics tools can not only help revealing the structure of a software package, but can also be used for various other tasks like debugging, maintenance, restructuring or optimization, therefore reducing software maintenance costs. Two 1 Introadppuliccattiioonns in this eld are CVSscan [17] for interactively tracking the changes of a software package over time, or the Voronoi treemaps [18] for visualization of software metrics.
Situational Awareness (SA) is the ability to identify, process, and comprehend the critical elements of information about what is h4a.p7peBniionlgog.y,TMheedticeirnmeaSnAdHceoamlthes from the world of military pilots, where achieving high levels of SA was found to be botThhecrrietsiecaarclh a neldds cinhabliolelonggyinangd [m5e]d.icTineheo iemrapvoerrtyawnicdee voafriety of appliSA as a focuantidonast.iAosncoomfdpuetceirsitoomn-omgraapkhiynagnd ultrasound imaging sinptahne medical area and performance for 3-dimensional digital reconstruction and visualization have been widely used many ﬁelds such as air traﬃc controllers, driving, for years, especially the emerging area of bio-informatics nopwowoeerrs a lot of posplant opesriabtleioanppsl,icmatiaoinnstfeonravinsucael, aannaldytimcs.ilFitroamrythoepeearrlaytbioegnins.ning of sequencing, There siscieantisgtrionwthinesge abreoads yfacoefunrperseecaedrecnhtedthvoaltumveasloidfdaatteas, litkheein the Human role of viGsuenaolmizeaPtiroonjectawsitah tmhreeeanbislliofonrbassoelpvaiinrsgpecrohmumpalenx.Odthaetranew areas like problems.ProVteiosmuiaclsi z(sattuidoiens oeflethveapterostetinhseincao mcelpl)r,eMheetnabsoiolonmics (systematic study of informationcoofbmuynbiifqnouastetocerhiraeilmncgihcearmlaipsntigrdyerpwcroiitnrhtrsetetlnhasattioosfpnmecaiillnicodncseploleufrlaccroempivrpoeocudenssdaesssseloveae-vneebnelahrigned)thoer ciations. Tamootuhntatofednadta, etvheeryddeasyi.gAn borfutteh-feordceiscpolmapyumtatuiosnt osfuapllppoorstsible combinathe decisitoionnsmisaokftienngnoptrpoocsesisbsle:, bidutenvitsiufaylianpgprpoarcohbeslecmansh,eclphtaoriadce-ntify the main terizing t hreegmion,saonf dintdereetsteramndi nexinclgudaeparperaos pthraitataere rneostpporonmseissin.g.ItAsistraditional viimperativseuatlhizaattioinntfeocrhmniqauteisocnanbneotpcroepseewnittehdthiense aammouanntsnoefrdtahtaa, tnew and more e ective visualizations are necessary to analyze this amount oafnddata ([19], [20]).
facilitates the user’s ability to process the information minimize any mental transformations that must be applied to the data.
In this work we focus on developing a visualization paradigm that takes advantage of human perceptive and cognitive facilities in order to enhance users’ situational awareness and support decision-making. We propose a novel visual correlation paradigm for SA and suggest its usage in a diverse set of SA applications.

The application eld in engineering analytics covers the whole range from engineering to construction, with a lot of parallels to physics (see above). The most important application is also ow visualization, regarding the automotive industry for example optimization of the air resistance of vehicles, optimization of the ows inside a catalytic converter or diesel particle lter, or computation of optimal air ows inside an engine [21]. Instead of only solving these problems algorithmically, visual analytics can help to understand the ows, and to interactively change construction parameters to optimize the ows. Another application in the automotive industry is the simulation of a car crash, where the frame of a car is represented as a grid of hundreds of thousands of points and the crash is simulated inside a computer. As an optimal car frame cannot be fully automatically computed, visual analytics can help engineers to understand the deformation of the frame during a crash step by step, and to identify the keypoints where optimization of the frame is necessary for a better overall stability.

The eld of personal information management has many facets and is already a ecting our everyday life through digital information devices such as PDAs, mobile phones, and laptop computers. However, there are many further possibilities where research might help to form our future. One example is the IBM Remail project [22], which tries to enhance human capabilities to cope with email overload. Concepts like \Thread Arcs", \Correspondents Map", and \Message Map" support the user to e ciently analyse his personal email communication. MIT's project Oxygen [23] goes one step further, by addressing the challenges of new systems to be pervasive, embedded, nomadic, adaptable, powerful, intentional and eternal. Many of those challenges re ect the visual analytics approach to combine human intelligence and intuition with computational resources.

As an example for tra c monitoring, we consider an ongoing project at University of Illinois-Chicago National Center for Data Mining [24]. In this project, tra c data from the tri-state region (Illinois, Indiana, and Wisconsin) are collected from hundreds of embedded sensors. The sensors are able to identify vehicle weights and tra c volumes. There are also cameras that capture live video feeds, Global Positioning System (GPS) information from selected vehicles, textual accident reports, and weather information. The research challenge is to integrate this massive information ow, provide visualizations that fuse this information to show the current state of the tra c network, and develop algorithms that will detect changes in the ows. Part of this project will involve characterizing normal and expected tra c patterns and developing models that will predict tra c activity when stimulus to the network occurs. The changes detected will include both changes in current congestion levels and di erences in congestion levels from what would be expected from normal tra c levels.

To complete the list of challenges of the previous section, we brie y list the 10 most important technical challenges.
The rst technical challenge lies in the eld of problem solving, decision science, and human information discourse. The process of problem solving supported by technology requires understanding of technology on the one hand, but also comprehension of logic, reasoning, and common sense on the other hand.
Intuitive displays and interaction devices should be constructed to communicate analytical results through meaningful visualizations and clear representations.
User acceptability is a further challenge; many novel visualization techniques have been presented, yet their wide-spread deployment has not taken place, primarily due to the users' refusal to change their working routines. Therefore, the advantages of visual analytics tools need to be communicated to the audience of future users to overcome usage barriers, and to eventually tap the full potential of the visual analytics approach. After having developed a system, its evaluation is crucial for future reference. Clear comparisons with previous systems to assess its adequacy and objective rules of thumbs to facilitate design decisions would be a great contribution to the community.
To automatically derive semantics out of large document collections is still a challenge. On the one hand, some expert systems have been successfully built for specialized elds, but on the other hand the researched methods only perform reasonably within a limited scope. Unlike human comprehension, automatic methods often fail to recognize complex coherences for which they have not been explicitly trained. Modeling of semantics to better deal with con icting and incomplete information is therefore a challenging eld.
Data quality and uncertainty is an issue in many domains, ranging from terrorism informatics to natural sciences, and needs to be taken into account when designing algorithms and visualization metaphors. Semiotic misinterpretations can occur easily. Data provenance as the science of understanding where data has come from and why it arrived in the user's database [25] is closely connected to the latter topic. In application elds such as biology, experimental data is made publicly available on the web, copied into other databases, and transformed several times (data curation). Seldom, this information about the transformations and the origins of the data under consideration is properly documented, although it is indispensable for the reproducibility of scienti c results. Another challenge lies in data streams producing new data at astonishing pace. In this eld, especially the timely analysis of the data streams plays an important role. In many cases, e.g. network tra c monitoring, detailed information is abundant and in the long term storage capacities do not su ce to log all data. Thus, e cient and e ective methods for compression and feature extraction are needed.
Due to improved measurement methods and decreasing costs of storage capacities, data sets keep on growing. Eventually, scalability becomes a major problem in both, automatic as well as visual analysis ([26], [27]), as it becomes more and more challenging to analyze these data sets. For more details see [1], page 24 \The Scalability Challenge".

Real-world applications often consist of a series of heterogeneous problems.
While solving one or more of these problems might still be accomplishable, their correlation make it very di cult to solve the overall problem, thus turning synthesis of problems into another challenge. It soon becomes apparent that integration with automated analysis, databases, statistics, design, perception, etc.
comprises the last of the technical challenges.

Visual analytics is an emerging eld of research combining strengths from information analytics, geospatial analytics, scienti c analytics, statistical analytics, knowledge discovery, data management & knowledge representation, presentation, production and dissemination, cognition, perception and interaction. Its goal is to gain insight into homogeneous, contradictory and incomplete data through the combination of automatic analysis methods with human background knowledge and intuition.
In this paper, we de ned the scope of this emerging eld and took a closer look at the visual analytics process. By presenting a formal model of the process, we identi ed the key concepts (data sets, hypotheses, visualizations and insight) and transition functions from one concept to another. To represent the iterative character of the process, a feedback-loop was introduced starting the process over again. To better understand the new analysis methodology, we presented the visual analytics mantra \analyse rst - show the important - zoom, lter and analyse further - details on demand". By means of the top 10 application challenges and the top 10 technical challenges, we gave an overview of the current state of the eld and its challenges.