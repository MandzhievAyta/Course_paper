{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from nltk.parse import stanford\n",
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tree import ParentedTree\n",
    "from IPython.display import Image, display\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score \n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RemoveShortStubs(paragraphs):\n",
    "    startStub = [u'it', u'there']\n",
    "    endStub = u'that'\n",
    "    for par in paragraphs:\n",
    "        for sent in par:\n",
    "            startidx = []\n",
    "            endidx = []\n",
    "            flag = 0\n",
    "            for i, word in enumerate(sent):\n",
    "                if word[0].lower() in startStub:\n",
    "                    flag = 1\n",
    "                    start_candidate = i\n",
    "                if (word[0] == endStub) and (flag == 1):\n",
    "                    flag = 0\n",
    "                    startidx.append(start_candidate)\n",
    "                    endidx.append(i)\n",
    "            for stubidx in reversed(range(len(startidx))):\n",
    "                del sent[startidx[stubidx]: endidx[stubidx] + 1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcess(text):\n",
    "    #Deleting literature references\n",
    "    #[([] -open bracket ( or [;  [^([]*? - lazy; \\d+ - number; [^])]*?; [])] - close bracket ) or ]\n",
    "    print \"regex:\"\n",
    "    time_beg = datetime.datetime.now()\n",
    "    pat = re.compile(r\"[([][^([]*?\\d+[^])]*?[])]\", re.IGNORECASE or re.DOTALL)\n",
    "    text = re.sub(pat, r\"\", text)\n",
    "    text = re.sub(r'([,.:;/)(\"])', r' \\g<1> ', text)\n",
    "    print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    \n",
    "    #Splitting into paragraphs\n",
    "    paragraphs = re.split(u\"\\n\", text)\n",
    "    \n",
    "    #Splitting into sentences\n",
    "    for i, par in enumerate(paragraphs):\n",
    "        paragraphs[i] = tokenize.sent_tokenize(par)\n",
    "    paragraphs_for_tag = []\n",
    "    for par in paragraphs:\n",
    "        paragraphs_for_tag.extend(par)\n",
    "        \n",
    "    #Tagging all words\n",
    "    print \"downloading stanford pos tagger:\"\n",
    "    time_beg = datetime.datetime.now()\n",
    "    st = StanfordPOSTagger('english-left3words-distsim.tagger')\n",
    "    print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    time_beg_tagging = datetime.datetime.now()\n",
    "\n",
    "    for i, par in enumerate(paragraphs_for_tag):\n",
    "        paragraphs_for_tag[i] = paragraphs_for_tag[i].split()\n",
    "    print (datetime.datetime.now() - time_beg_tagging).total_seconds()\n",
    "    paragraphs_for_tag = st.tag_sents(paragraphs_for_tag)\n",
    "    prev_par = 0\n",
    "    for i, par in enumerate(paragraphs):\n",
    "        paragraphs[i] = paragraphs_for_tag[prev_par:prev_par + len(paragraphs[i])]\n",
    "        prev_par += len(paragraphs[i])\n",
    "    for i, par in enumerate(paragraphs):\n",
    "        for sentence in range(len(par)):\n",
    "            paragraphs[i][sentence][:] = [x for x in paragraphs[i][sentence] if x[0] != u'(' and x[0] != u')']\n",
    "    print \"actual tagging:\"\n",
    "    print (datetime.datetime.now() - time_beg_tagging).total_seconds()\n",
    "    #print paragraphs\n",
    "    print \"remove short stubs:\"\n",
    "    time_beg = datetime.datetime.now()\n",
    "    RemoveShortStubs(paragraphs)\n",
    "    print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex:\n",
      "0.008271\n",
      "downloading stanford pos tagger:\n",
      "0.000618\n",
      "0.000807\n",
      "actual tagging:\n",
      "7.004806\n",
      "remove short stubs:\n",
      "0.003477\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(u\"./articles/parsed/1_b.txt\", 'r', 'utf-8') as fin:\n",
    "        text = fin.read()\n",
    "paragraphs = PreProcess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SourceText:\n",
    "    def __init__(self, paragraphs):\n",
    "        self._paragraphs = [x for x in paragraphs if x != []]\n",
    "        self._par_iter = 0\n",
    "        self._sent_iter = 0\n",
    "        \n",
    "    def nextPar(self):\n",
    "        self._par_iter += 1\n",
    "        if self._par_iter == len(self._paragraphs):\n",
    "            return 1\n",
    "        else:\n",
    "            self._sent_iter = 0\n",
    "            return 0\n",
    "        \n",
    "    def nextSent(self):\n",
    "        if self._sent_iter < len(self._paragraphs[self._par_iter]):\n",
    "            self._sent_iter += 1\n",
    "            return self._paragraphs[self._par_iter][self._sent_iter - 1], 0\n",
    "        else:\n",
    "            return [], 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OffsetTopicsAndStresses:\n",
    "    def __init__(self):\n",
    "        self.topic_strong_words = [[], [], []]\n",
    "        self.topic_weak_words = [[], [], []]\n",
    "        self.stress_strong_words = [[], [], []]\n",
    "        self.stress_weak_words = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NOT_APPLICABLE = 0\n",
    "UNKNOWN = 1\n",
    "FLUID = 2\n",
    "INVERTED_TOPIC_CANDIDATE = 3\n",
    "OUT_OF_SYNC = 4\n",
    "INVERTED_TOPIC = 5\n",
    "DISCONNECTED = 6\n",
    "\n",
    "class SentInfo:\n",
    "    _fluid_words = [u\"admittedly\", u\"all in all\", u\"as a result\", u\"because\", u\"conversely\", u\"equally\", u\"finally\",\\\n",
    "               u\"for example\", u\"in a similar\", u\"in contrast\", u\"in summary\", u\"initially\", u\"last\", u\"nevertheless\",\\\n",
    "               u\"once\", u\"so far\", u\"such\", u\"after\", u\"along these lines\", u\"as expected\", u\"before\", u\"curiously\",\\\n",
    "               u\"even though\", u\"first\", u\"for instance\", u\"in a way\", u\"in other words\", u\"in the first\",\\\n",
    "               u\"interestingly\", u\"lastly\", u\"next\", u\"regardless\", u\"specifically\", u\"surprisingly\", u\"afterward\",\\\n",
    "               u\"although\", u\"as soon as\", u\"but\", u\"despite\", u\"eventually\", u\"firstly\", u\"for this reason\",\\\n",
    "               u\"in comparison\", u\"in particular\", u\"in the same way\", u\"it follows\", u\"likewise\", u\"nonetheless\",\\\n",
    "               u\"similarly\", u\"still\", u\"that is why\", u\"again\", u\"as a consequence\", u\"be that as it may\",\\\n",
    "               u\"consequently\", u\"during\", u\"figure\", u\"following\", u\"in a certain sense\", u\"in conclusion\", u\"in short\",\\\n",
    "               u\"indeed\", u\"it is as if\", u\"meanwhile\", u\"now\", u\"so\", u\"subsequently\", u\"the first\", u\"the last\",\\\n",
    "               u\"this\", u\"to elaborate\", u\"the next\", u\"this is why\", u\"to explain\", u\"the reason\", u\"thus\",\\\n",
    "               u\"to illustrate\", u\"then\", u\"to conclude\", u\"to put it another way\", u\"to put it succinctly\",\\\n",
    "               u\"unexpectedly\", u\"while\", u\"to sum up\", u\"until\", u\"while\", u\"to summarize\", u\"up to now\", u\"yet\",\\\n",
    "               u\"ultimately\", u\"whereas\"]\n",
    "    def __init__(self, sentence, type, tree):\n",
    "        self.sent = sentence\n",
    "        self.type = type\n",
    "        self.topic_strong_words = []\n",
    "        self.topic_weak_words = []\n",
    "        self.stress_strong_words = []\n",
    "        self.stress_weak_words = []\n",
    "        self._begins_fluid_words = -1\n",
    "        self.current_offset = 0\n",
    "        #0 ~ Sn-1; 1 ~ Sn-2; 2 ~ Sn-3 \n",
    "        self.offset_wordset = OffsetTopicsAndStresses()\n",
    "        self._tree = next(tree)\n",
    "    \n",
    "    def _deleteNonMainClauses(self, tree):\n",
    "        idxs = tree.treepositions()\n",
    "        for s in list(tree.subtrees(lambda t: t.label() == u\"SBAR\" or t.label() == u\"SBARQ\")):\n",
    "            idx = -1\n",
    "            for x in idxs:\n",
    "                if s == tree[x]:\n",
    "                    idx = x\n",
    "                    break\n",
    "            if idx != -1:\n",
    "                del tree[idx]\n",
    "                idxs = tree.treepositions()\n",
    "        return tree\n",
    "    \n",
    "    def _recursive_search(self, tree, subj_list):\n",
    "        if tree.height() == 2 or (tree.height() == 3 and len(tree.leaves()) == 1):\n",
    "            subj_list.extend(tree.leaves())\n",
    "        else:\n",
    "            for np_idx in range(len(tree)):\n",
    "                l = tree[np_idx].label()\n",
    "                target_tags = [u\"NP\", u\"NN\", u\"NNS\", u\"PRP\", u\"CD\"]\n",
    "                if  l in target_tags:\n",
    "                    self._recursive_search(tree[np_idx], subj_list)    \n",
    "    \n",
    "    def _findSubjects(self, tree):\n",
    "        res = []\n",
    "        for s in list(tree.subtrees(lambda t: t.label() == u\"S\")):\n",
    "            child_labels_list = []\n",
    "            for i in range(len(s)):\n",
    "                child_labels_list.append(s[i].label())\n",
    "            if u\"NP\" in child_labels_list and u\"VP\" in child_labels_list:\n",
    "                for np_idx in range(len(s)):\n",
    "                    if s[np_idx].label() == u\"NP\":\n",
    "                        self._recursive_search(s[np_idx], res)\n",
    "        return res\n",
    "    \n",
    "    def _appearsBeforeFirstPunct(self, idx):\n",
    "        punctuation_marks = [tuple([u\",\", u\",\"]), tuple([u\":\", u\":\"]), tuple([u\";\", u\";\"]), tuple([r'\"', r'``'])]\n",
    "        if ([x for x in punctuation_marks if x in self.sent[:idx]] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _appearsAfterLastPunctOrConj(self, idx):\n",
    "        punctuation_marks = [tuple([u\",\", u\",\"]), tuple([u\":\", u\":\"]), tuple([u\";\", u\";\"]), tuple([r'\"', r'``'])]\n",
    "        conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "        if ([x for x in punctuation_marks if x in self.sent[idx:]] == []) or\\\n",
    "        ([x for x in self.sent[idx:] if x[1] in conj_tags] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isMainClauseContainsTopic(self, main_clause_words):\n",
    "        topic = list(self.topic_strong_words)\n",
    "        topic.extend(self.topic_weak_words)\n",
    "        if ([x for x in topic if not x in main_clause_words] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isNumber(self, word):\n",
    "        numbers = [\"one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen,\\\n",
    "        fifteen, sixteen, seventeen, eighteen, nineteen, twenty, thirty, fourty, fifty, sixty, seventy, eighty, ninety,\\\n",
    "        hundred, thousand, million, billion\"]\n",
    "        if (word in numbers) or (re.match(r\"^[-+]?[0-9]+$\", word) != None):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _appearsAfterConj(self, idx):\n",
    "        conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "        if ([x for x in self.sent[:idx] if x[1] in conj_tags] != []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isStrongStress(self, stress_word, main_clause_words):\n",
    "        noun_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\"]\n",
    "        if stress_word[1] in noun_tags:\n",
    "            idx = self.sent.index(stress_word)\n",
    "            if self._appearsBeforeFirstPunct(idx) or self._appearsAfterLastPunctOrConj(idx):\n",
    "                return True\n",
    "            if (stress_word in main_clause_words) and self._isMainClauseContainsTopic(main_clause_words) and\\\n",
    "            self._appearsAfterConj(idx):\n",
    "                return True\n",
    "            if self._isNumber(self.sent[idx - 1][0]):\n",
    "                return True\n",
    "        #verb derived; stress word in main clause\n",
    "        elif stress_word[0] in main_clause_words:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _addStressWords(self, stress_words, main_clause_words):\n",
    "        for stress_word in stress_words:\n",
    "            if self._isStrongStress(stress_word, main_clause_words):\n",
    "                self.stress_strong_words.append(stress_word[0])\n",
    "            else:\n",
    "                self.stress_weak_words.append(stress_word[0])\n",
    "    \n",
    "    def setDefaultWordSet(self):\n",
    "        self.topic_weak_words = []\n",
    "        self.topic_strong_words = []\n",
    "        self.stress_weak_words = []\n",
    "        self.stress_strong_words = []\n",
    "        tree = self._tree.copy(True)           \n",
    "        #display(tree)\n",
    "        \n",
    "        #Добавляем все слова с метками NN, NNS, NNP, NNPS\n",
    "        #Отличить verb derived nouns в VBG FIX\n",
    "        target_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\", u\"VBG\"]\n",
    "        nounsAndVerbDerivedNouns = []\n",
    "        for tup in self.sent:\n",
    "            if tup[1] in target_tags:\n",
    "                nounsAndVerbDerivedNouns.append(tup)\n",
    "        #print \"nounsAndVerbDerivedNouns: \", nounsAndVerbDerivedNouns, \"\\n\"\n",
    "        \n",
    "        #Удаляем non main clauses\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #Ищем subjects\n",
    "        self.topic_strong_words = self._findSubjects(tree)\n",
    "        stress_words = [x for x in nounsAndVerbDerivedNouns if x[0] not in self.topic_strong_words]\n",
    "        self._addStressWords(stress_words, tree.leaves())\n",
    "        #print \"topic_strong_words(main clause subj): \", self.topic_strong_words, \"\\n\"\n",
    "        #print \"strong stress: \", self.stress_strong_words, \"\\n\"\n",
    "        #print \"weak stress: \", self.stress_weak_words, \"\\n\"\n",
    "        #print \"current sentence: \", self.sent, \"\\n\"\n",
    "    \n",
    "    def addStressWords(self, stress_words, offset):\n",
    "        tree = self._tree.copy(True)\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #print \"stress_words:\"\n",
    "        #print stress_words\n",
    "        for stress_word in stress_words:\n",
    "            if self._isStrongStress(stress_word, tree.leaves()):\n",
    "                self.offset_wordset.stress_strong_words[offset - 1].append(stress_word[0])\n",
    "            else:\n",
    "                self.offset_wordset.stress_weak_words[offset - 1].append(stress_word[0])\n",
    "    \n",
    "    def getMainClauseSubjects(self):\n",
    "        tree = self._tree.copy(True)\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #display(tree)\n",
    "        return self._findSubjects(tree) \n",
    "    \n",
    "    def getMainClauseWords(self):\n",
    "        tree = self._tree.copy(True)\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        return tree.leaves()\n",
    "        \n",
    "    def beginsWithFluidWords(self):\n",
    "        if self._begins_fluid_words != -1:\n",
    "            return self._begins_fluid_words\n",
    "        verb_tags = [u\"VB\", u\"VBD\", u\"VBN\", u\"VBP\", u\"VBZ\"]\n",
    "        ordinal_numbers = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\",\\\n",
    "                           \"tenth\", \"eleventh\", \"twelfth\", \"thirteenth\", \"fourteenth\", \"fifteenth\", \"sixteenth\",\\\n",
    "                           \"seventeenth\", \"eighteenth\", \"nineteenth\", \"twentieth\", \"thirtieth\", \"fortieth\", \"fiftieth\",\\\n",
    "                           \"sixtieth\", \"seventieth\", \"eightieth\", \"ninetieth\", \"hundredth\", \"thousandth\"]\n",
    "        pronouns_tags = [u\"PRP$\", u\"PRP\"]\n",
    "        patt = re.compile(r\"([A-Z]+\\))|(\\([A-Z]+\\))|([1-9]+\\))|(\\([1-9]+\\))|([1-9]*1st)|([1-9]*2nd)|([1-9]*3rd)|([1-9]*[4-9]th)|([1-9]+0th)\")\n",
    "        for word in self.sent:\n",
    "            if word[1] in verb_tags:\n",
    "                break\n",
    "            #FIX некоторые FLUID WORDS состоят из двух и более слов, а я рассматриваю только по одному\n",
    "            if (word[0].lower() in self._fluid_words) or (word[1] in pronouns_tags) or\\\n",
    "            (re.search(patt, word[0]) != None) or (word[0].lower() in ordinal_numbers):\n",
    "                self.beginsWithFluidWords = 1\n",
    "            else:\n",
    "                self.beginsWithFluidWords = 0\n",
    "        return self.beginsWithFluidWords\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STRONG_TOPIC = 10\n",
    "WEAK_TOPIC = 11\n",
    "\n",
    "def BetweenFluidOrInverted(sent_list, offset):\n",
    "    #check Sn-1\n",
    "    if sent_list[-2].type != FLUID and sent_list[-2].type != INVERTED_TOPIC_CANDIDATE:\n",
    "        return False\n",
    "    #check Sn-2\n",
    "    if sent_list[-3].type != FLUID and sent_list[-3].type != INVERTED_TOPIC_CANDIDATE:\n",
    "        return False\n",
    "    if offset == 3:\n",
    "        if sent_list[-4].type != FLUID and sent_list[-4].type != INVERTED_TOPIC_CANDIDATE:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def TopicFound(topic_words, topic_type, sentence, previous_sentence, reached_verb, offset, sent_list):\n",
    "#     print \"trying to find topic!\", topic_words, topic_type\n",
    "    if offset == 1:\n",
    "        if not reached_verb:\n",
    "            sentence.type = FLUID\n",
    "        else:\n",
    "            sentence.type = INVERTED_TOPIC_CANDIDATE\n",
    "    else:\n",
    "        if not reached_verb:\n",
    "            if BetweenFluidOrInverted(sent_list, offset):\n",
    "                sentence.type = FLUID\n",
    "            else:\n",
    "                sentence.type = OUT_OF_SYNC\n",
    "    if topic_type == WEAK_TOPIC:\n",
    "        if not set(topic_words).issubset(set(sentence.offset_wordset.topic_strong_words[offset - 1])):\n",
    "            sentence.offset_wordset.topic_weak_words[offset - 1].extend(topic_words)\n",
    "    elif topic_type == STRONG_TOPIC:\n",
    "        sentence.offset_wordset.topic_strong_words[offset - 1].extend(topic_words)\n",
    "                 \n",
    "def CheckSentenceMainClauses(sentence, previous_sentence, sent_list):\n",
    "    offset = sentence.current_offset\n",
    "    main_clause_subjects = sentence.getMainClauseSubjects()\n",
    "    prev_sent_wordset = list(previous_sentence.topic_weak_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.topic_strong_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.stress_strong_words)\n",
    "#     print \"prev_sent_wordset: \", prev_sent_wordset\n",
    "#     print \"main clause subjects: \", main_clause_subjects\n",
    "    matched_words = [x for x in main_clause_subjects if x in prev_sent_wordset]\n",
    "#     print \"found strong topic! Give me sec to check: \", matched_words\n",
    "    if matched_words != []:\n",
    "        TopicFound(matched_words, STRONG_TOPIC, sentence, previous_sentence, False, offset, sent_list)\n",
    "    #FIX какие именно subjects имеются в виду?\n",
    "    stress_words = [x for x in main_clause_subjects if x not in matched_words]\n",
    "    sentence.addStressWords([(x, u\"\") for x in stress_words], offset)\n",
    "    \n",
    "def CheckWholeSentence(sentence, previous_sentence, sent_list):\n",
    "    offset = sentence.current_offset\n",
    "    reached_verb = False\n",
    "    reached_topic_or_main = False\n",
    "    prev_sent_wordset = list(previous_sentence.topic_weak_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.topic_strong_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.stress_strong_words)\n",
    "    main_clause_words = sentence.getMainClauseWords()\n",
    "#     print \"prev_sent_wordset: \", prev_sent_wordset\n",
    "#     print \"main clause words: \", main_clause_words\n",
    "    conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    noun_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\"]\n",
    "    \n",
    "    for word in sentence.sent:\n",
    "        reached_verb = word[1] in conj_tags or reached_verb\n",
    "        reached_topic_or_main = word[0] in main_clause_words or reached_topic_or_main\n",
    "#         print word, reached_verb, reached_topic_or_main\n",
    "        if not reached_verb:\n",
    "            matches = word[0] in prev_sent_wordset\n",
    "            if (matches and word[1] in noun_tags) or (matches and reached_topic_or_main and (word[1] == u\"VBG\")):\n",
    "                TopicFound([word[0]], WEAK_TOPIC, sentence, previous_sentence, False, offset, sent_list)\n",
    "            elif (word[1] == u\"VBG\"):\n",
    "                sentence.addStressWords([word], offset)\n",
    "        else:\n",
    "            if (sentence.offset_wordset.topic_strong_words[offset - 1] != []) or (sentence.offset_wordset.topic_weak_words[offset - 1] != []):\n",
    "                if word[1] in noun_tags:\n",
    "                    sentence.addStressWords([word], offset)\n",
    "            else:\n",
    "                matches = word[0] in prev_sent_wordset\n",
    "                if (word[1] in noun_tags) and matches:\n",
    "#                     print \"special for recognition!!!\", [word[0]]\n",
    "                    TopicFound([word[0]], WEAK_TOPIC, sentence, previous_sentence, True, offset, sent_list)\n",
    "                    reached_topic_or_main = True\n",
    "                elif (word[1] == u\"VBG\") and matches:\n",
    "                    sentence.addStressWords([word], offset)\n",
    "                    \n",
    "def CheckSentenceProgression(sentence, previous_sentence, sent_list):\n",
    "    sentence.current_offset += 1\n",
    "    CheckSentenceMainClauses(sentence, previous_sentence, sent_list)\n",
    "    CheckWholeSentence(sentence, previous_sentence, sent_list)\n",
    "    \n",
    "def DefineResults(sentence, total_amount):\n",
    "    if sentence.type == UNKNOWN:\n",
    "        sentence.type = DISCONNECTED\n",
    "        sentence.setDefaultWordSet()\n",
    "    else:\n",
    "        if sentence.type == INVERTED_TOPIC:\n",
    "            word_set_from_round = 1\n",
    "        elif sentence.type in [FLUID, OUT_OF_SYNC]:\n",
    "            word_set_from_round = sentence.current_offset\n",
    "        sentence.topic_strong_words = sentence.offset_wordset.topic_strong_words[word_set_from_round - 1]\n",
    "        sentence.topic_weak_words = sentence.offset_wordset.topic_weak_words[word_set_from_round - 1]\n",
    "        sentence.stress_strong_words = sentence.offset_wordset.stress_strong_words[word_set_from_round - 1]\n",
    "        sentence.stress_weak_words = sentence.offset_wordset.stress_weak_words[word_set_from_round - 1]\n",
    "    if sentence.type == FLUID:\n",
    "        total_amount[0] += 1\n",
    "    elif sentence.type == INVERTED_TOPIC:\n",
    "        total_amount[1] += 1\n",
    "    elif sentence.type == OUT_OF_SYNC:\n",
    "        total_amount[2] +=1\n",
    "    elif sentence.type == DISCONNECTED:\n",
    "        total_amount[3] += 1\n",
    "        \n",
    "def TypeToString(type):\n",
    "    if type == FLUID:\n",
    "        return \"Fluid\"\n",
    "    elif type == INVERTED_TOPIC:\n",
    "        return \"Inverted topic\"\n",
    "    elif type == OUT_OF_SYNC:\n",
    "        return \"Out if sync\"\n",
    "    elif type == DISCONNECTED:\n",
    "        return \"Disconnected\"\n",
    "    elif type == NOT_APPLICABLE:\n",
    "        return \"Not applicable\"\n",
    "    \n",
    "def PrintSentInfo(sent):\n",
    "    print \"--------------------------------------------------------------------------------------------------------\"\n",
    "    print \"\\nSentence: \", sent.sent\n",
    "    print \"Topic:\"\n",
    "    print \"Strong: \", sent.topic_strong_words\n",
    "    print \"Weak  : \", sent.topic_weak_words\n",
    "    print \"Stress:\"\n",
    "    print \"Strong: \", sent.stress_strong_words\n",
    "    print \"Weak  : \", sent.stress_weak_words\n",
    "    print \"\\nType: \", TypeToString(sent_list[-1].type)\n",
    "    print \"\\nWordsets from checkups with Sn-1, Sn-2, Sn-3:\"\n",
    "    print \"Topic:\"\n",
    "    print \"Strong: \", sent.offset_wordset.topic_strong_words\n",
    "    print \"Weak: \", sent.offset_wordset.topic_weak_words\n",
    "    print \"Stress:\"\n",
    "    print \"Strong: \", sent.offset_wordset.stress_strong_words\n",
    "    print \"Weak: \", sent.offset_wordset.stress_weak_words\n",
    "    print \"--------------------------------------------------------------------------------------------------------\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build trees time:\n",
      "317.023608\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.00063\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000895\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.008863\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000309\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000539\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.001576\n",
      "setDefaultWordSet:\n",
      "0.002332\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000869\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000287\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000295\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.005189\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.00133\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.0012\n",
      "############################################################################################################\n",
      "\n",
      "Next Paragraph:\n",
      "setDefaultWordSet:\n",
      "0.000859\n",
      "############################################################################################################\n",
      "Total amount:\n",
      "Fluid sentences:  35\n",
      "Inverted topic sentences:  21\n",
      "Out of sync sentences:  0\n",
      "Disconnected sentences:  78\n"
     ]
    }
   ],
   "source": [
    "total_amount = [0, 0, 0, 0] #fluid, inverted_topic, out_of_sync, disconnected\n",
    "state = 0\n",
    "text = SourceText(paragraphs)\n",
    "\n",
    "plain_paragraphs = []\n",
    "for par in paragraphs:\n",
    "    plain_paragraphs.extend(par)\n",
    "parser = stanford.StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "print \"build trees time:\"\n",
    "time_beg = datetime.datetime.now()\n",
    "all_trees = parser.tagged_parse_sents(plain_paragraphs)\n",
    "print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "\n",
    "while state == 0:\n",
    "    print \"\\nNext Paragraph:\"\n",
    "    sent_list = []\n",
    "    sent, p = text.nextSent()\n",
    "    if sent == []:\n",
    "        text.nextPar()\n",
    "        continue\n",
    "    sent_list.append(SentInfo(sent, NOT_APPLICABLE, next(all_trees)))\n",
    "    sent_list[-1].setDefaultWordSet()\n",
    "    #PrintSentInfo(sent_list[-1])\n",
    "    sent, p = text.nextSent()\n",
    "    while p == 0:\n",
    "        sent_list.append(SentInfo(sent, UNKNOWN, next(all_trees)))\n",
    "        if sent_list[-1].beginsWithFluidWords():\n",
    "            sent_list[-1].type = FLUID\n",
    "            print \"setDefaultWordSet:\"\n",
    "            time_beg = datetime.datetime.now()\n",
    "            sent_list[-1].setDefaultWordSet()\n",
    "            print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "        else:\n",
    "            CheckSentenceProgression(sent_list[-1], sent_list[-2], sent_list)\n",
    "            if sent_list[-1].type == INVERTED_TOPIC_CANDIDATE:\n",
    "                sent_list[-1].type = INVERTED_TOPIC\n",
    "            elif sent_list[-1].type == UNKNOWN:\n",
    "                if len(sent_list) > 2:\n",
    "                    for offset in range(2):\n",
    "                        CheckSentenceProgression(sent_list[-1], sent_list[-2], sent_list)\n",
    "                        if sent_list[-1].type != UNKNOWN or len(sent_list) < 4:\n",
    "                            break\n",
    "            DefineResults(sent_list[-1], total_amount)\n",
    "        #PrintSentInfo(sent_list[-1])\n",
    "        sent, p = text.nextSent()\n",
    "    state = text.nextPar()\n",
    "    print \"############################################################################################################\"\n",
    "print \"Total amount:\"\n",
    "print \"Fluid sentences: \", total_amount[0]\n",
    "print \"Inverted topic sentences: \", total_amount[1]\n",
    "print \"Out of sync sentences: \", total_amount[2]\n",
    "print \"Disconnected sentences: \", total_amount[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "with open('./result') as fresult:\n",
    "    for line in fresult:\n",
    "        features = line.strip().split()\n",
    "        name_inctance = features[0]\n",
    "        name = name_inctance.split('_')[0]\n",
    "        label = name_inctance.split('_')[1][0]\n",
    "        y.append(label)\n",
    "        X.append([features[1], features[2], features[4]])\n",
    "        \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16814159  0.21238938  0.61946903]\n",
      " [ 0.16091954  0.13793103  0.70114943]\n",
      " [ 0.31932773  0.24369748  0.43697479]\n",
      " [ 0.2962963   0.30555556  0.39814815]\n",
      " [ 0.14705882  0.10294118  0.75      ]\n",
      " [ 0.25        0.14705882  0.60294118]\n",
      " [ 0.1328125   0.1328125   0.734375  ]\n",
      " [ 0.08433735  0.22891566  0.68674699]\n",
      " [ 0.30656934  0.16788321  0.52554745]\n",
      " [ 0.23958333  0.13541667  0.625     ]\n",
      " [ 0.18531469  0.2027972   0.61188811]\n",
      " [ 0.35087719  0.1754386   0.47368421]\n",
      " [ 0.18719212  0.20689655  0.60591133]\n",
      " [ 0.10091743  0.01834862  0.88073394]\n",
      " [ 0.2605042   0.17647059  0.56302521]\n",
      " [ 0.17553191  0.10638298  0.71808511]\n",
      " [ 0.12380952  0.1047619   0.77142857]\n",
      " [ 0.18461538  0.26923077  0.54615385]\n",
      " [ 0.25242718  0.13592233  0.61165049]\n",
      " [ 0.26119403  0.15671642  0.58208955]\n",
      " [ 0.25252525  0.2020202   0.54545455]\n",
      " [ 0.18888889  0.12222222  0.68888889]\n",
      " [ 0.15044248  0.16814159  0.68141593]\n",
      " [ 0.12676056  0.21126761  0.66197183]\n",
      " [ 0.19327731  0.14285714  0.66386555]\n",
      " [ 0.26829268  0.09756098  0.63414634]\n",
      " [ 0.3025641   0.17948718  0.51794872]\n",
      " [ 0.2037037   0.13888889  0.65740741]\n",
      " [ 0.30693069  0.21782178  0.47524752]\n",
      " [ 0.24731183  0.20430108  0.5483871 ]\n",
      " [ 0.10344828  0.05172414  0.84482759]\n",
      " [ 0.15517241  0.14655172  0.69827586]\n",
      " [ 0.20930233  0.20930233  0.58139535]\n",
      " [ 0.2244898   0.24489796  0.53061224]\n",
      " [ 0.2625      0.2125      0.525     ]\n",
      " [ 0.28409091  0.18181818  0.53409091]\n",
      " [ 0.20289855  0.05797101  0.73913043]\n",
      " [ 0.28571429  0.20535714  0.50892857]\n",
      " [ 0.14285714  0.14285714  0.71428571]\n",
      " [ 0.168       0.152       0.68      ]\n",
      " [ 0.26666667  0.19047619  0.54285714]\n",
      " [ 0.23770492  0.17213115  0.59016393]\n",
      " [ 0.23106061  0.15530303  0.61363636]\n",
      " [ 0.23129252  0.17006803  0.59863946]\n",
      " [ 0.20603015  0.15075377  0.64321608]\n",
      " [ 0.14655172  0.15517241  0.69827586]\n",
      " [ 0.18333333  0.25        0.56666667]\n",
      " [ 0.23255814  0.11627907  0.65116279]\n",
      " [ 0.17857143  0.20535714  0.61607143]\n",
      " [ 0.13043478  0.14229249  0.72727273]\n",
      " [ 0.28571429  0.26785714  0.44642857]\n",
      " [ 0.27884615  0.17307692  0.54807692]\n",
      " [ 0.2173913   0.23913043  0.54347826]\n",
      " [ 0.13559322  0.23728814  0.62711864]\n",
      " [ 0.14705882  0.21323529  0.63970588]\n",
      " [ 0.22580645  0.09677419  0.67741935]\n",
      " [ 0.26548673  0.16814159  0.56637168]\n",
      " [ 0.21621622  0.19369369  0.59009009]\n",
      " [ 0.18518519  0.03703704  0.77777778]\n",
      " [ 0.13718412  0.08303249  0.77978339]\n",
      " [ 0.23076923  0.13846154  0.63076923]\n",
      " [ 0.22522523  0.13513514  0.63963964]\n",
      " [ 0.2890625   0.25        0.4609375 ]\n",
      " [ 0.15789474  0.16842105  0.67368421]\n",
      " [ 0.28731343  0.18283582  0.52985075]\n",
      " [ 0.20879121  0.17582418  0.61538462]\n",
      " [ 0.26666667  0.18518519  0.54814815]\n",
      " [ 0.17592593  0.18518519  0.63888889]\n",
      " [ 0.18617021  0.20212766  0.61170213]\n",
      " [ 0.18461538  0.10769231  0.70769231]\n",
      " [ 0.22        0.2         0.58      ]\n",
      " [ 0.15384615  0.09401709  0.75213675]\n",
      " [ 0.26219512  0.26829268  0.4695122 ]\n",
      " [ 0.3030303   0.16161616  0.53535354]\n",
      " [ 0.36893204  0.09708738  0.53398058]\n",
      " [ 0.25        0.0625      0.6875    ]\n",
      " [ 0.20095694  0.12440191  0.67464115]\n",
      " [ 0.13821138  0.17886179  0.68292683]\n",
      " [ 0.22164948  0.19587629  0.58247423]\n",
      " [ 0.13333333  0.18333333  0.68333333]\n",
      " [ 0.29230769  0.23076923  0.47692308]\n",
      " [ 0.06060606  0.06060606  0.87878788]\n",
      " [ 0.29126214  0.22330097  0.48543689]\n",
      " [ 0.17977528  0.20224719  0.61797753]\n",
      " [ 0.24468085  0.29787234  0.45744681]\n",
      " [ 0.24691358  0.12345679  0.62962963]\n",
      " [ 0.40449438  0.20224719  0.39325843]\n",
      " [ 0.12857143  0.07142857  0.8       ]\n",
      " [ 0.19512195  0.17073171  0.63414634]\n",
      " [ 0.20338983  0.16949153  0.62711864]\n",
      " [ 0.17073171  0.08710801  0.74216028]\n",
      " [ 0.17241379  0.2183908   0.6091954 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype |S3 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#normalize our results\n",
    "normed_matrix = normalize(X, axis=1, norm='l1')\n",
    "print normed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.61290323  0.61290323  0.73333333]\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(GradientBoostingClassifier(), X, y, cv=3)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5483871   0.58064516  0.3       ]\n"
     ]
    }
   ],
   "source": [
    "#все очень плохо\n",
    "score = cross_val_score(GradientBoostingClassifier(), normed_matrix, y, cv=3)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653046596667\n"
     ]
    }
   ],
   "source": [
    "print (0.61290323 + 0.61290323 + 0.73333333) / 3.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
