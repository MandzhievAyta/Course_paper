of bias in inductive concept way that directly relates this quantitative theory design of effective learning measuring some common to conjunctive concepts internal disjunction, and, develop learning algorithms have provably good conver The theme of this pa er is that inductive concept learning IpU86] [R86/ way that enables us to rove meaningful ties for learning algorit i ms. We measure binatorial parameter defined on classes the Vapnik-Chervonenkis dimension [VC71/, [P78j’, JBEHW86/. The lower class of concepts considered by the stronger the bias. In /BEHW86,‘? shown to be strongly correlated with defined in the learning performance ant jV84j, [VSS]. Th is model can  the notion of bias in can be quantified in a convergence properbias with a comof concepts known as (or simply d+ensiorr ) the dlmenslon of the learning algorithm, the this parameter has been learning performance, as model introduced by Valibe outlined as follows.

A concept is defined by its set of instances in some instance s ace. A sample of a concept is sequence of observations, eat rl of which is an instance of the concept ( positive observation ) or a non-instance of the concept negatrve observation ). Samples are assumed to be create 6 from inde endent, random observations, chosen according to some Kxed probability distribution on the instance space. Given a samle of a target concept to be learned, a learning algorithm f!orms a hypothesis, which is itself a concept. The algorithm is consistent if its hypothesis is always consistent with the given sample, i.e. includes all observed positive instances and no observed negative instances. A consistent hypothesis may still disagree with the target concept by failing to include unobserved instances of the target concept or including unobserved non-instances of the target concept. The error of a hypothesis is the combined probability of such instances, i.e.
the probability that the hypothesis will disa ree with a random observation of the target concept, se ‘iected from the instance space according to the fixed probability distribution.

Two performance in this setting.
convergence of the with The  rithms 1. The rate of the learning algorithm is measured in terms sample size that is required for the algorithm to produce, high probability, a hypothesis that has small error. qualification “with high probability” required because the creation of the sample is a probabilistic event. Even the best learning algorithm cannot succeed in the unlikely event that the sample is not indicative of typical observations. However, while the model is probabilistic, specific assumptions are made about the probability distribution that governs the observations. This distinguishes this approach from usual statistical methods employed in pattern recognition, where the object of learning is usually reduced to the estimation of certain parameters of a classical distribution. The distribution-free formulation of convergence rate  obtained by upper bounding the worst case convergence rate of the learning algorithm over all probability distributions on the instance space. This provides an extremely robust performance guarantee.
2. The >omputational efficiency of the learning algorithm is measured in terms of the (worst case) computation time required to pass from a sample of a given size to a hypothesis.
Our results for conjunctive concepts indicate the possibility of a trade-off between convergence rate and computational efficiency., in which the fastest converging learning methods require significantly more computation time than their slower converging counterparts. In order to optimize this trade-off, applying the general method developed in JBEHW86/, we em loy heuristic techniques based on the greedy method for fin x ine: a small set cover iN69l lJ741 that trade off a small decrezse in the convergence rate’ for’s very large increase in computational efficiency. This general idea forms a secondary theme of the paper.

In the simplest type of inductive concept learnin , each instance of a concept is defined by the values of a fixe li set of attributes, not all of which are necessarily relevant. For example, an instance of the concept “red triangle” might be characterized by the fact that its color is red, its shape is triangular and its size is 5. Following [MCLBS], we consider three types of attributes. A nominal attribute is one that takes on a finite, unordered set of mutually exclusive values, e.g. the attribute color, restricted to the six primary and secondary colors. A linear attribute is one with a linearly ordered set of mutually exclusive values, e.g. a real-valued or integer-valued attribute. A tree-structured attribute is one with a finite set of hierarchically ordered values, e.g. the attribute shape with values triangle, square, hezagonll circle, polygon and any-shape, arranged in the usual “is-a hierarchy. Only the leaf values triangle, square, hexagon and circle are directly observed. Since a nominal attribute can be converted to a tree-structured attribute by addition of the special value any-value, we will restrict our discussion to treestructured and linear attributes.

relating attributes to values either elementary or compound.
terms are as follows.

For tree-structured attributes: = red, shape = polygon.

linear attributes: 12. Strict inequalities open on one side.
as size = 5.

Compound terms /MC83/ For tree-structured attributes: attribute = value. or value, e.g. shape = square 0: circle, aid disjunction of intervals e.g. 0 5 junctive operators within compound disjunctions.

We consider the following pure conjunctive: term1 each term. is = red and 5 1. .&e 5  * * . or value, for linear at&ibutes: any age 2 21 or age 2 65. Disterms are called internal  These concept the context of rule Pure conjunctive: clause rule (PROLOG  types have the following interpretations based knowledge representations.
antecedent of a single, variable-free rule), e.g.

type = pos t color = red and 5 < size 5 Pure disjunctive: antecedents of severafrules, le term and all with a common consequent.
f nternal disjunctive: antecedent of a single junctive “helper rules” for the compound internal disjunctive concept given above, “primary” for color and form the rules color = primary t color = red color = primary t color = blue color = primary t color = yellow type = pos t color = primary and 5 5 site 2  In Section nal disjunctive samples. But algorithms can  2 we will see how collections concepts can be generated first, we describe how these be evaluated,  To quantify the inductive bias of a learning al orithm, we use the following notion from ‘VC71]. Let 2 be an instance s ace and let H be a class o i concepts defined on X, e.g. the cfass of pure conjunctive concepts over an instance space determined by a fixed set of attributes. For any finite set S C X of instances, El,(S) = {S n h : h E H}, i.e. the set of 21 subsets of S that can be obtained by intersecting S with a concept in H, or equivalently, the set of all ways the instances of S can be divided into positive and negative instances so as to be consistent with some concept in H. If n,(S) is the set of all subsets of S then we say that S is shattered by H. The Vapnik-Chervonenkis dimension of H (or simply the dimension of H) is the smallest integer d such that no S C X of cardinality d + 1 is shattered by H. If no such d existsrthe dimension of H is infinite.

As an example, suppose X is the instance space defined by one linearly ordered attribute size and H is the set of pure conjunctive concepts over X. Thus H is just the set of elementary terms involving site, i.e. size intervals. For any three distinct instances, i.e. instances where size = z, size = y and sire = z, with x < y < Z, there is no concept in H for which the first and third instances are positive but the second instance is negative because there is no interval that contains z and z without containin y. Hence no set of three instances in X can be shattered by fI implying that the dimension of H is at most 2. Since any two ‘out of three distinct instances can be shattered by H, this upper bound is tight, at least when size has three or more distinct values.
cpper bounds on the dimensions of the more general concept classes introduced above are as follows: k term pure conjunctive concepts on n attributes, each treestructured or linear: 1) d 5 4klog(4k”/n).
F(or k of size roughly n/2 or larger,  i!‘i) bette: u’,,‘e”r bound .
k term pure*disjunctive concepts (2) d < 4klog(l6n)(log(2k) k term internal disJunctlve total of j internal disjunctions: d < 5(k+j)log(5(k+j for these  on n attributes: + loglog( 16n)).
concepts on n attributes,  Let C be a class of target level of complexity, e.g. p-term an instance space defined by concept in C and some number  concepts of some type and pure conjunctive concepts over n-attributes. Given a target m of observations of this con [WSl], [AA831 and [BEHWM] of finite dimeoslon. When H a Vapalk-Chervonenkis nttmber of this class, denoted  g’lve a variety of other examples of H is of finite dimension, Wenocur and Class (VCC) iwD8li. The VapntkV(H), corresponds to the dimension of H  ‘/VC71), concept classes Dudley call Chervonenkis plus one.

observations, Algorithm 2 produces, with probability 1 - 6, a hypothesis with error at most e, independent target concept and independent of the underlying distrigoverning the generation of observations. By a different using bound (1’) and (6) with (Y = 0, we can also the upper bound  %This is derived ditional measurability they wll not be relevant  from Theorem 11 of [BEHW86]. We are suppressing some adassumptions required in the general form of the theorem since in our intended applications (see appendix of [BEHW86/3.

cept! a learning algorithm will explore some space of possible hypotheses. This will be called the eflective hypothesis space of the learning algorithm for target concepts in C and sample size m. The numerical bias of the learning algorithm is defined as the Vapnik-Chervonenkis dimension of its effective hypothesis space. A lower bias is a stronger bias. For example, in the next section we will present an algorithm (Algorithm 2) for learning pure conjunctive concepts that has the following property: presented with m observations of an unknown p-term pure conjunctive target concept over an instance space of it attributes, it always produces a consistent pure conjunctive hypothesis with at most Dlnm + 1 terms. Hence the-effective hypothesis space of the a’lgorithm for target concepts of this type with samde size m is the class of at most plnm + l-term pure con’unct*ive concepts over an instance space of n attrlbutes. + hese limitations on the hypothesis space are due to the fact that the algorithm only considers pure conjunctive hypotheses and prefers concepts with fewer terms, two of the informal types of bias identified in jV86/.
Using formula (1) with k = plnm, we can approximately upper bound the numerical bias of the algorithm for p-term pure conjunctive target concepts over an instance space of n ~~ibute~~~d~~~~~~~~~,y  e can now use the following theorem to relate this cal bias with the convergence rate of the algorithm target concepts.
Theorem 1. [BEHWSS,p Given any consistent learning algorithm with numerical bias for targetaconcepts in a class and sample size m of at most rm , where r 2 2 0 5 N < 1, then for any probability distribution on instance space, any target concept in C and any E and between 0 and 1, given a random sample of the target concept of size at least  probability at least e. If the numerical to have sample size l-cl  (5 above into (7) but ignoring 1 = 1 and r = 4plog(4pVn)), a p-term pure conjunctive and approximately  formulas (8 and (9) is that the convergence rate does not depend at a 11 on the size or complexity of the trees that define the values of the tree-structured attributes, nor on the number of values of the linearly ordered attributes. It also shows that the convergence rate depends only logarithmically on the number of attributes and the confidence facltor S. The strongest dependence is on the inverse error - and the  In fact, the argument used in proving Theorem 1 shows the following stronger result: given any p-term pure conjunctive target concept over n attributes and a sample of approximately the size given in 8) or (9), with probability at least 1 - 6 any consistent hypot 6 esis within the effective hypothesis space of Algorithm 2, i.e. any consistent conjunct with at most plnm + 1 terms, will have error at most e, independent of the underlying probability distribution that governs the observations. Thus no matter what our method is, if we happen to find a conjunct with at most plnm + 1 terms consistent with a sample of this size, then we can use this conjunct as our hypothesis and have confidence at least 1 that its error is at most E. This kind of a posteriori is what lead Pearl to call the  projection of the minimal dominating dominating terms  The effective class of all pure attributes and number of terms of pure con’unctive formula (1’3 above, iven by formula 9), Algorithm Kypothesis with get concept and  hypothesis space of this algorithm is the conjunctive concepts over some fixed set of doesn’t depend on the sample size or the in the target concept. Since the dimension concepts on n attributes is at most 2n by the convergence rate of this algorithm is (9) above, i.e. given a random sample of size 1 produces, with probability at least 1 - 6, a error at most e for any pure conjunctive tarany distribution on the instance space.

While significant in its generality, this upper bound suffers from the fact that the number of observations required grows at least linearly in the number of attributes. In many AI learning situations where conjunctive concepts are used, the task is to learn relatively simple conjuncts from samples over instance spaces with many attributes. In this case a better algorithm would be to find the simplest conjunct (i.e.
the conjunct with the least number of terms) that is consistent with the data, rather than the most specific conjunct.
With this strategy, given a sample of any p-term pure conjunctive concept on n attributes, we always find a consistent ure conjunctive hypothesis that has at most p terms. Thus Ky the same analysis (i.e. using (6) with a = 0) and using formula (1) instead of (1 ) (with k = p)! the upper bound on the sample size required for convergence 1s reduced to  which is logarithmic the optimal algorithm.
The following shows that  Minimum Set Cover: T find a subcollection mmimum number of sets.

There is, however, an obvious heuristic for approximating the minimum cover of T: First choose a largest set. Then remove the elements of this set from T and choose another set that includes the maximum number of the remaining elements, continuing in this manner until T is exhausted. This is called the greedy method. Applying it to the problem of finding pure conjunctive concepts, we get the following.

Algorithm 2. (greedy algorithm for learning tive concepts) 1. For each attribute, calculate the projection onto this attribute and find the minimal dominating 2. Starting with the empty expression E, while there tive observations in the sample do: a. Among all attributes, find the term that eliminates the most negative add it to E, breaking out of the loop inating term eliminates any negative b. Remove from the sample the that are eliminated and update the attributes accordingly.
3. If there are no negative observations left return else report that the sample is not consistent with conjunctive concept.

minimal dominating observations if no minimal examples.
negative observations projections onto  It can be shown that if the set T to be covered has m elements and p is the size of the minimum cover, then the greedy method is uaranteed to find a cover of size at most p logm + 1 [N69] fJ7.41. H ence given a sample of an p-term pure conjunctive concept with m negative observations, Algorithm 2 is guaranteed to find a consistent pure conjunctive hypothesis with at most approximately plogm terms. Using  Theorem convergence previous consistent the bound as well.
gir;z:~,  1, this ives the approximate upper bound on the rate 7or Algorithm 2 given by formula (8) in the section. Since Algorithm 2 is, like Algorithm 1, a algorithm for arbitrary pure .conjunctTve concepts, on the convergence rate given in formula (9) holds Note that the b&nd on theconvergence rate for the method is not much worse than the bound (10) for the algorithm! yet the greedy method 1s slgmficantly computationally.

The compliments of pure conjunctive concepts can be represented as pure disjunctive concepts. Hence this is the dual form of pure conjunctive concepts. A variant of Algorithm 2 can be used to learn oure disiunctive concerts. In the dual form, each term must eliminate811 negative observat,ions and need only imply some subset of positive observations, and all terms together must imply all positive observations. The dual greedy method is to repeatedly choose the term that implies the most positive observations and add it to the disjunct, removing the positive observations that are implied, until all ositive observations are accounted for. This is a variant o P the “star” method in MCL891. Since k term nure disjunctive concepts have a Vannik-Chervonenkis dimension similar to that of k term pure cbnjunctive concepts formula (2)), the analysis of the convergence rate of this a fgorithm goes through as above.

We now tackle internal disjunctive concepts. The calculation of the Vannik-Chervonenkis dimension of these concerts given in the pretious section indicates that the strongest bias yn learning them is to minimize the total number ‘;f terms plus internal disiunctions. i.e. to minimize the total size of all the terms, where the size ‘of a compound term is defined as the number of internal disjunctions it contains plus one. Let E be an internal disjunctive concept that is consistent with a given sample. As with pure conjunctive concepts, each term in E implies all positive observations and eliminates some set of negative observations. A compound term with this pro erty will be called a dominating compound term. We would liKe to eliminate all the negative observations using a set of terms with the smallest total size. This leads to the following.

Minimum Set Cover given a collection of sets associated with it a positive whose union is T that has the  Since it generalizes clearly NP-hard. However, by a generalized greedy remainine to be covered.
the gain,&ost ratio of this it contains divided by method is to always choose the ratio. As with the basic Minimum be shown that if the original set ments and p is the minimum cost eralized greedy method is guaranteed most plogm L 1.

Minimum Set Cover, this problem is approximate solutions can found method. Let T’ be a set of elements For each set in the collection. define set as the number of elements of T’ its cost. The generalized greedy set with the highest gain/cost Set Cover problem, it can T to be covered has m eleof any cover, then the gento find a cover of size at  method in learning internal of a dominating compound observations it eliminates  Algorithm 3. (greedy algorithm for learning junctive concepts) 1. For each attribute, calculate the projection onto this attribute.
2. Starting with the empty expression E, while tive observations in the samnle do: a. Among all attributei, find the term t with the highest gain/cost the loop if none have ositive gains.
for the attribute of t a Pready in E, add replace the old term in E for the attribute b. Remove from the sample the negative eliminates and update the projections onto accordingly.
3. If’there are no negative else report that the sample disjunctive concept.  dominating compound ratio, breaking out If there is no term t to E. Otherwise of t with t.
observations all attributes  ratio for a given attribute from the projection of the sample onto that attribute. Since there are in general exponentially many distinct dominating compound terms with respect to the number of leaves of a tree-structured attribute or the number of values of a linear attribute, this cannot be done by exhaustive search. However, there is a reasonably efficient recursive procedure that does this for tree-structured attributes, and simple iterative procedure fsr linear attributes. Each of these procedures takes time O(q ), where q is the number of distinct values of the attribute that ap ear in the observations.
Space limitations preclude a detaile cr discussion of these procedures.

By formula (3) and the above result on the of the generalized greedy method. the numerical rithm g for k-terminteinal disju’nctive target a total of j internal disjunctions (i.e. of size ple size most S(k+j)fn(m)log S(k?j ln(rr$‘&) atIgnoring term, formula (i ) of 4 heorem 1 gives an upper convergence rate similar to that of Algorithm equation (8), with k+j substituted for p.

performance bias of Algoconcepts using k + j) and sam handle “don’t care” values (see e.g. [QSS], [V84/).

A “don’t care” value for attribute A corresponds to an observation in rule form having the term A = any-value. In fact, we can go one step further and let observations be arbitrary pure conjunctive expressions, where, for example, the positive observation shape = polygon and color = blue means that the concept contains all blue polygons, and the corresponding negative observation means that no blue olygons are contained in the concept. In this form, the probPem of learning from examples is seen to be a special. case of the more general problem of knowledge refinement /MICS,Y,I, wherein we start with a collection of rules that are already known and try to derive from them a simpler, more general (and hopefully more comprehensible) set of rules. This extension can be accomplished by modifying the notion of the projection of the samples onto the attributes to allow terms of the observations to project to internal nodes of the treestructured attributes or intervals in the linear attributes.
Other parts of the algorithm are changed accordingly.

Once we have learned to recognize the attribute.in terms of the primitive attributes, to the set of primitive attributes and used reco nize the values of other attributes.
attri f utes are always nominal. However, tree structure as they are used to define following manner (see also [US61 /BSPSS/): nal disjunctive concept is formed using A =vloru20t a*- otvk, check to see pound term is required by other concepts.
often enough, check the tree for the attribute v can be added without ~&tlZZ~~~~ “d:, ‘.Z. ff a new node is added, terms it reprdsents can be replaced by an using the value of the new node. Thus the given in Section 1 for the internal disjunctive  values of the new it can be added later in learning to In this scheme new they could acquire later concepts in the whenever an intera compound term if this same comIf it is required A. If a node destroying the the compound elementary term collection of rules concept involv ing the primary colors the higher level value this way a useful attributes evolves higher level concepts, cepts.

might be created by the “discovery” of primary for the attribute color.
vocabulary of more abstract values under the pressure to find simple forms creating a synergy between learned  Another type of synergy is achieved by using the algorithm for pure conjunctive concepts along with the dual algorithm for pure disjunctive concepts. If new Boolean attributes are defined for often-used pure conjuncts or disjuncts, then these can allow the recognition of higher level concepts in DNF and CNF respectively by effectively reducing these expressions to pure disjunctive or conjunctive form. Often used internal disiunctive concents could be used as well. The creation of thesk new attribites can greatly increase the number of attributes that are considered in- later learning tasks, which argues strongly for learning methods whose performance does not degrade badly as the number of attributes grows, such as those we have presented.

We have presented a methodology for the quantitative analysis of learning performance based on a relatively simple combinatorial property of the space of hypotheses explored by the learning algorithm. Applications of this methodology have been presented in the development and analysis of learning algorithms for pure conjunctive, pure disjunctive and internal disjunctive concepts. Several open problems remain, in addition to those mentioned above. Some are: 1. Can we develop the proper analytic tools to deal with algorithms that a. attempt to handle the problem of noisy data jQSS/ or b. attemnt to learn “fuzzv” concepts that are defin’ed probabilisticaliy with respect to-the instance space? 2. What power is gained by allowing the learnin algorithm to form queries during the learning process [SASS] b’vG86/? 3. Can we find provably efficient incremental learning algorithms (i.e. ones that modify an evolving hypothesis after each observation) to replace the “batch processing” learning algorithms we have given here? 4. To what extent can we extend these results to concepts that involve internal structure, expressed with the use of variables, quantifiers and binary relations (e.g. the c-expressions of [MCL 83/)?  Acknowledgements. I would like for suggestin the relationship Chervonenkis i imension and Utgoff’s and Ryszard Michalski for suggesting learning internal disjunctive concepts.
ant, Leonard Pitt, Phil Laird, Ivan gleton and Andrzej Ehrenfeucht these ideas, and an anonymous improving the presentation.

to thank between notion I look I also Bratko and for helpful referee for  Larry the of inductive at the problem thank Les Stephan discussions suggestions  a basis for pattern recogin Advances in Computers,