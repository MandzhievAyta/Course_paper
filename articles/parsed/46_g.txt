Respellings are a widely employed method of conveying the pronunciation of English and foreign words, both in print and on the Web. For example, Huatulco, the name of a Mexican resort, is respelled as ‘wah-tool-koh’ in a travel guide (Noble, 2012).
The advantage of using respellings lies in removing the need for a separately deﬁned phonetic transcription system. Since they contain only the letters of the Latin alphabet, their phonetic interpretation relies exclusively on orthographic intuitions of readers. For this reason, respellings are widely used in travel phrase books, medical compendia, and drug name pronunciation guides, among others.
Despite their utility, good respellings are not easy to create. Respellings found on the Web often contain errors or ambiguities. For example, HenochSchoenlein purpura, a skin disease, is respelled both  as ‘heh-nok shoon-line purr-puh-ruh’ and ‘hen-awk sher-line purr-purr-ah’. Does ‘heh’ rhyme with eh [e] or with Nineveh [@], or is it the same vowel as in hen [E]? Clearly, if both respellings refer to the same pronunciation, at least one of them must be wrong. In addition, converting the pronunciation of a foreign name to English phonemes is in itself a non-trivial task.
In this paper, we focus on the task of generating respellings from the intended pronunciation given as a sequence of phonemes. We develop a stand-alone system that combines linguistic knowledge and resources with machine learning models trained on data mined from the Web and electronic dictionaries. One of our ultimate objectives is to aid writers by evaluating their respellings, improving them, or generating new candidates. Accordingly, we endeavour to maintain the generation and the evaluation stages as separate modules in our system.
The evaluation of respellings is a challenging problem. Since English spelling conventions are notoriously inconsistent, there is no algorithm for accurately predicting the pronunciation of an out-ofvocabulary word. The current state-of-the-art letterto-phoneme (L2P) converters are typically reported with 10-30% error rates on dictionary words (Bisani and Ney, 2008). On the other hand, human readers often disagree on the details of the pronunciation implied by a respelling. In this paper, we conduct two kinds of evaluations: an automated veriﬁcation with an independent L2P system, and an experiment with human participants that pass judgments on different respellings of the same word. We interpret the results as evidence that the output of our system compares favourably with typical respellings found on the Web.

Although Chomsky and Halle (1968) characterize English orthography as close to optimal, Kominek and Black (2006) estimate that it is about 3 times more complex than German, and 40 times more complex than Spanish. This is conﬁrmed by lower accuracy of letter-to-phoneme systems on English (Bisani and Ney, 2008). A survey of English spelling (Carney, 1994) devotes 120 pages to describe phoneme-to-letter correspondences, and lists 226 letter-to-phoneme rules, almost all of which admit exceptions.
There is no consensus on how to best convey the pronunciation of an uncommon word in English.
Most dictionaries employ either the International Phonetic Alphabet (IPA), or their own transcription schemes that incorporate special symbols and diacritics. Unfortunately, many readers are unfamiliar with phonetic transcription. Instead, respellings are often preferred by writers in the news and on the Web. In this section, we deﬁne the respelling task in detail.

A respelling is a non-standard spelling of a word, that is intended to better convey its pronunciation.
We assume that the pronunciation is deﬁned as a sequence of English phonemes, and that the respelling contains only the 26 letters of the alphabet, with optional hyphenation. Some transcription schemes combine respellings with special symbols for representing certain phonemes. For example, an otherwise purely alphabetic Wikipedia scheme employs the symbol @ for the vowel schwa. In our opinion, such devices destroy the main advantage of respellings, which is their universality, without attaining the precision of a true phonetic transcription. In fact, Fraser (1997) identiﬁes the schwa symbol as the cause of many pronunciation errors.
In our system, we consistently use hyphens to segment multi-syllable respellings. Each syllablesize segment contains the representation of exactly one vowel phoneme, so that the number of segments matches the number of syllables.1 However, the hyphenation need not correspond exactly to the actual  1Henceforth, we refer to “syllable-size segments” simply as “syllables”.

syllable breaks. This approach has several advantages. First, individual syllables are easier to pronounce than an entire unfamiliar word. Second, hyphens limit the context that affects the pronunciation of a given letter (e.g. th in Beethoven ‘baythoe-ven’). Finally, hyphens indicate whether adjacent vowel letters, such as oe in ‘hoe’, represent one vowel phoneme or two.
Some respellings explicitly indicate the stressed syllable by expressing it in a different font. This is potentially helpful because unstressed vowels tend to be reduced, which changes their pronunciation.
However, since the vowel reduction phenomenon is by no means universal, the readers may be unsure whether to apply it to, e.g. the ﬁnal o in ‘KWATro’. In this paper, we make no distinction between stressed and unstressed syllables; instead, we follow the principle that each syllable is to be pronounced as if it was a separate word. Nonetheless, it would be straightforward to project the stress indicators onto the appropriate syllables in the respellings generated by our system.

There is no clear-cut distinction between good and bad respellings. The quality of a respelling is more of a subjective opinion rather than a veriﬁable fact.
We propose to evaluate it according to the following three criteria: ambiguity, correctness, and preference.
A respelling is ambiguous if it is perceived as compatible with more than one pronunciation. Because most of the rules of English spelling have exceptions, it is rarely possible to demonstrate that a respelling is completely unambiguous. However, some respellings are clearly more ambiguous than others. For example, the digraph ee almost always represents the vowel [i], whereas the letter sequence ough can represent several different phonemes.2 Respellings that contain highly ambiguous letter-phoneme mappings can be expected to be ambiguous themselves. Ambiguity is a property of a respelling itself, regardless of the intended pronunciation.
A respelling is correct if it accurately conveys the intended pronunciation to the reader. Unlike the am 2Compare bough, cough, dough, tough, lough, through.

biguity, correctness can be veriﬁed objectively for a particular reader, by comparing the intended pronunciation with the pronunciation inferred by the reader. A respelling that is judged correct with respect to one pronunciation cannot be judged correct with respect to a different pronunciation. Nevertheless, it is entirely possible that different readers will derive different pronunciations from the same respelling.
A respelling can be classiﬁed as unambiguous and yet incorrect by a given reader, but it cannot be judged as simultaneously ambiguous and correct. Indeed, an ambiguous respelling is compatible with at least two pronunciations, only one of which can be the intended pronunciation. Therefore, for a given reader, unambiguity is a necessary but not sufﬁcient condition for correctness.
Given two unambiguous and correct respellings, a reader may prefer one over the other, perhaps because of the ease of inferring the intended pronunciation. For example, ‘rode-ease-yew’ may be preferred to ‘roh-dee-zyoo’ because the former is entirely composed of actual English words with unique pronunciation, whereas the latter contains an unusual consonant cluster zy. Preference is also expressed implicitly if only one of the alternative respellings is judged as unambiguous (or correct),  Fraser (1997) describes an experiment in which 15 human subjects were asked to pronounce uncommon words after being shown a representation of their pronunciation. The respellings designed by the author were much more effective for that purpose than either the IPA phonetic transcription or phonemic respelling (Section 4.3). However, the creation of respellings was described as labour-intensive, and at least one of them was found to be sub-optimal during the experiment.
Williams and Jones (2008) propose respellings as a way of extending pronunciation lexicons by informants who lack linguistic training. Galescu (2009) reports that the addition of respellings of medical terms from an on-line dictionary improves the accuracy of an L2P system. The author identiﬁes an automatic pronunciation-to-respelling system as future work.

Ghoshal et al. (2009) extract a large number of respellings from the Web, and show that they can be exploited to improve the accuracy of the L2P conversion by supplementing the data in pronunciation dictionaries. Can et al. (2009) further analyze the effect of using respellings on the accuracy of spokenterm detection (STD) systems.

In this section, we discuss three direct methods of generating respellings: manual design, dictionary lookup, and phonemic respelling.

Respellings found on the Web and in news articles are usually ad-hoc creations of the authors of those texts. Respellings designed by different writers for the same word are rarely identical.3 The quality of Web respellings vary.
The respellings found in specialized lexicons are more likely to be designed by experts, and are often guided by a set of respelling rules. Nevertheless, such respelling guides may also be ambiguous.4 Regardless of the source, since respellings are often used for names and foreign words, no lexicon can be expected to provide a complete coverage.

Pronunciation dictionaries can be helpful in generating respellings. Assuming that we have a method of dividing pronunciations into syllables, a complete respelling of an out-of-dictionary word can in some cases be automatically derived from the list of syllable pronunciations. For example, hyphy can be respelled as ‘high-fee’ by following such a procedure.
If each of the syllables has a unique pronunciation, such respellings are arguably both unambiguous and correct.
Unfortunately, only a subset of potential phonemic syllables actually occur in a lexicon. Considering only the syllables of the CVC type (consonantvowel-consonant), there are over ten thousand distinct possibilities (e.g., [bEb], [bES], etc.), of which  3For example, the word capoeira is represented by 99 different respellings in the corpus of Ghoshal et al. (2009).
4For an example of a confusing respelling guide see http: //www.ama-assn.org/go/usan.

fewer than three thousand can be found in the Combilex pronunciation dictionary (Richmond et al., 2009). While the dictionary lookup may produce attractive respellings, it is not sufﬁcient for a standalone use.

A simple method that can produce a respelling for any word is to directly map each phoneme to a particular letter or a letter sequence that is frequently used to represent that phoneme. Phonemes such as [m], [d] and [f] are indeed closely associated with individual letters. This is not surprising since the Roman letters were originally created to represent single phonemes in Latin, and some of those phonemes also exist in English. However, many phonemes, especially vowels, have no obvious orthographic representation. One solution is to use digraphs such as ee and aw, but a number of phonemes, such as [aU] as in loud, have no mappings that work in all contexts.
The principal weakness of a phonemic respelling is its inﬂexibility, which often results in counterintuitive respellings. For example, many readers are bafﬂed by respelling such as ‘gee’ for ghee or ‘john’ for Joan. Phonemic respelling tends to fail in cases where it generates a sequence of letters that is inherently ambiguous, or which pronunciation changes because of the context. On the other hand, mappings such as uu for [U] and ahy for [aI], which never occur in real English words, are difﬁcult to interpret for some readers.
In this paper, we adopt a context-free phonemic respelling scheme as the baseline, with the mappings from the online dictionary Dictionary.com, which differs from the system used in Wikipedia only in a few details.

In this section, we present our syllabiﬁcation approach, as well as two generation modules: a trained phoneme-to-letter (P2L) model and a rule-based respeller.

Our respelling generation process is for the most part performed on the level of individual syllables.

Correct syllabiﬁcation is by itself a non-trivial problem, but even if it was provided by an oracle, it might not correspond to the optimal segmentation of a respelling. For example, the word trigonal [trIg@n@l] is usually syllabiﬁed as tri-go-nal, but a better segmentation for the purposes of respelling is trig-onal. We adopt an overgenerate-and-rank approach, whereby instead of committing to a speciﬁc word segmentation at the start of the process, we process multiple syllabiﬁcation alternatives in parallel, one of which is ultimately selected at the respelling evaluation stage.
Ideally, syllabiﬁcation should conform to the phonotactic constraints of English, so that the resulting respellings are easy to pronounce. The consonant sonority should be rising in onsets, and falling in codas (Kenstowicz, 1994). We verify that syllables follow the sonority principle by following the formulation of Bartlett et al. (2009). The sonority constraints are not tested at the boundaries of the word, which are independent of the syllabiﬁcation choice. We also incorporate another important principle of English phonotactics that asserts that lax vowels do not occur in open syllables (Rogers, 2000).
In our implementation, each candidate syllable is tested with respect to the following sequence of four violable constraints, ordered from the strongest to the weakest: (1) the syllable contains exactly one vowel phoneme; (2) the onset satisﬁes the sonority principle; (3) if the nucleus contains a lax vowel (except @), the coda is non-empty; (4) the coda satisﬁes the sonority principle. For a syllabiﬁcation to be accepted, all its syllables must satisfy the four constraints. However, if this results in rejection of all possible syllabiﬁcations, the constraints are gradually relaxed starting from the weakest.

As an example, consider the word abandonment [@bænd @nm@nt], which has 18 different syllabiﬁcations satisfying the VOWEL constraint (Table 1). 8 of the 18 satisfy the ONSET constraint as well, but only two syllabiﬁcations satisfy all four constraints: [@b-æn-d @n-m@nt] and [@-bæn-d @n-m@nt].

The respelling problem can be viewed as a string transduction problem, with the transduction occurring between phonemes and letters. As such, it is directly related to the well-studied letter-to-phoneme conversion task. The difference is that the letters may not conform to the standard orthography of English. If we had a sufﬁciently large training set of pronunciation-respelling pairs, we could train a machine learning algorithm to directly generate respellings for any strings of English phonemes. However, such a training set is not readily available. The respellings in the corpus collected by Ghoshal et al.
(2009) are not easily matched to the phonetic transcriptions, and few of them can be found in electronic pronunciation dictionaries. In addition, the quality of Web respellings vary greatly.
In place of a direct pronunciation-to-respelling model, we aim to model the orthographic intuitions of readers by deriving a phoneme-to-letter (P2L) transduction model from an English pronunciation dictionary. A possible criticism of such an approach is that our model may create ambiguous respellings, which abound in English orthography. However, we rely on a separate evaluation module to identify and ﬁlter ambiguous respellings at a later stage.
Our systems utilizes the DIRECTL+ program (Jiampojamarn et al., 2008), which was originally designed for L2P conversion. Since our basic unit is the syllable, rather than the word, we train our P2L model on a set of of 4215 pairs of monosyllabic words and their pronunciations extracted from the Combilex dictionary. We exclude syllables in multisyllabic words from training because their pronunciation is often affected by context. This is consistent with our expectation that the reader will pronounce each hyphen-delimited segment of the respelling as if it was an individual word.
Since the P2L training data consists of a relatively small set of syllables, we ensure that the phonemeletter alignment is highly accurate. As a preprocess ing step, we replace the letter x with ks, and we convert digraphs, such as ch and th, to single symbols.
The alignment is performed by M2M-ALIGNER (Jiampojamarn et al., 2007), under the restriction that each phoneme is matched to either one or two letter symbols.

A hand-crafted context-sensitive respeller is intended to complement the trained P2L model described in the previous section. It is similar to to the phonemic respelling approach described in Section 4.3 in that it converts each phoneme to a letter sequence. However, the mappings depend on adjacent phonemes, as well as on the CV pattern of the current syllable. In addition, more than one mapping for a phoneme can be proposed. We designed the mappings by analyzing their frequency and consistency in pronunciation dictionaries.
The process of candidate generation involves establishing the pattern of consonants in the input syllable. The consonant mappings are the same as in the baseline, except for [¥] and [T], while the vowels yield up to three different letter sequences. For example, [o] is mapped to oh as a default, but also to o if both onset and coda are empty, or to o followed by a consonant and a silent e if the coda is composed of a single consonant. So, given the syllable [tok] as input, the respeller produces two candidates: tohk and toke.
We make no claims about the completeness or optimality of the mappings, but in our development experiments we observed that the context-sensitive respeller contributes to the robustness of our system, and in some cases produces more attractive respellings that the P2L model.

We aim at developing a stand-alone method for the assessment of respellings that could be applied regardless of their origin. We consider two criteria: correctness, which is evaluated against the intended pronunciation, and ambiguity, which is a property of the respelling itself. As was the case in the generation stage, the evaluation is performed at the level of syllables.

The principal method of verifying the correctness of a respelling involves the application of a letterto-phoneme (L2P) model trained on the wordpronunciation pairs extracted from an English dictionary. The generated pronunciation of each syllable is compared against its intended pronunciation; if any of the syllables fail the test, the entire respelling is rejected.
The L2P model is derived using the DIRECTL+ system. The main difference between the L2P model described in this section and the P2L model from Section 5.2 is that the input and output data are reversed. However, the L2P model is not simply a mirror image of the P2L model. Often the phonemic output of the composition of the two models is different from the initial phonemic input; e.g., [ro] → row → [raU]. This is because the intermediate orthographic string may be ambiguous. Furthermore, the L2P model is also intended to test the correctness of respellings that were generated with other methods.
Other differences between the two models pertain to the preprocessing of the training data, and the letter-to-phoneme alignment. As with the P2L model, the training data consists of a set of monosyllabic words from the Combilex dictionary. However, in order to make our correctness ﬁlter more conservative, we also remove all words that contain diacritics (e.g., crêpe), non-English phonemes (e.g., avant), or silent consonants (e.g., limn). The alignment is restricted to matching each letter symbol to at most one phoneme, and is derived with the ALINE phonetic aligner (Kondrak, 2000), which has been shown to outperform other 1-1 alignment methods (Jiampojamarn and Kondrak, 2010).

Syllables that contain multiple vowel groups may be confusing to readers even if they correctly represent the intended pronunciation. For example, readers might be unsure whether takess represents one or two syllables. A simple vowel counter is provided to ﬁlter out such syllables. The vowel ﬁlter accepts a syllable only if (a) it contains exactly one vowel group (e.g., moe), or (b) the second vowel group consists of a single e at the end of the syllable (e.g., zake).

This module is designed to compute a score that reﬂects the ambiguity of an orthographic syllable. The ambiguity score of a respelling is deﬁned as the average of scores assigned to each of its syllables. The score can then be used to select the best respelling from a number of candidates generated by our system, or to rate a respelling from another source.
Since we have no explicit ambiguity annotations for respellings, we attempt instead to exploit ambiguity judgments that are implicitly made when respellings are created by human authors. We approach ambiguity as a binary classiﬁcation task. For any given syllable, we wish to determine whether it is ambiguous (a negative instance), or unambiguous (a positive instance). Our assumption is that a syllable will not be respelled unless it is necessary due to ambiguity. For each observed word-respelling pair, we take all syllables from the respelling as positive instances, and all syllables in the original word that are not preserved in the respelling as negative instances. For example, the pair consisting of the word cec-il-y respelled as ‘sehs-il-ee’ provides three positive instances: sehs, il and ee; and two negative instances: cec and y.
We extracted word-respelling pairs from the Webderived corpora of Ghoshal et al. (2009). The syllable breaks in the respellings were mapped onto the original words using ALINE. In order to improve the quality of the data, we applied a letterto-phoneme model to both the original words and their respellings, and removed pairs with divergent pronunciations (computed as normalized edit distance ≤ 0.8). After the ﬁltering, we were left a set of 25067 word-respelling pairs containing 78411 training syllables, which yielded 47270 positive and 31141 negative instances.
For the classiﬁcation task we utilize the SVMlight software package (Joachims, 1999). Each instance is represented by a set of binary indicator features. The features correspond to character n-grams (including syllable boundary markers) with the values of n ranging from 1 to 5. For example, the syllable -il- turns on the following features: i, l, -i, il, l-, -il, il-, -il-. The model learns which n-grams are characteristic of ambiguous or unambiguous syllables. For example, it classiﬁes bothle and li as am biguous, and lee as unambiguous. Apart from the binary classiﬁcation, the classiﬁer also provides a real-valued score for each syllable.

In this section, after describing our test sets, we present the results of two evaluation experiments: direct human judgment, and indirect validation with an L2P system.

Since the use of familiar English letter sequences makes the respellings easier to interpret (Fraser, Our two test sets were deﬁned after the development 1997), we incorporate dictionary lookup (Section of our system had been completed. There is no over4.2) into our system. When the pronunciation of a lap between the test sets and any of our training sets.
syllable happens to correspond to the pronunciation The ﬁrst test set consists of 27 out of 30 words coman actual dictionary word, the syllable may be re- piled by Fraser (1997) — 3 words from the origispelled using that word. This is done as the ﬁnal step nal set were excluded because the corresponding rein the generation process because dictionary words spellings assume a non-rhotic variety of English. We often receive poor scores from the SVM classiﬁer on refer to Fraser’s respellings as expert, and consider the account of their n-gram composition. The lexi- them as the upper bound in terms of quality.
cal reviser is restricted to optionally improving the The second test set of 231 words (henceforth retop-ranked word respelling candidate as determined ferred to as the Web set) was extracted from the by the SVM classiﬁer without altering its syllabiﬁ- corpus of Ghoshal et al. (2009) after performing cation. For example, the respelling ‘surr-sin-uss’ of additional data clean-up described in Section 6.3.
circinus is modiﬁed to‘sir-sin-us’. If more than one We identiﬁed a subset of words for which we word can be used, we let the SVM classiﬁer select could ﬁnd phonetic transcriptions composed of Enthe least ambiguous one. glish phonemes on Wikipedia. In order to ensure that the respellings and the corresponding transcriptions reﬂect the same pronunciation, we adapted the 7 System Overview Soundex algorithm to apply to phonetic transcriptions, and retained only the respelling/transcription pairs that yielded identical Soundex codes. We removed words that are found in the Combilex dictionary as those could be familiar to human judges.
Since longer words are more challenging to respell, and more likely to exhibit variation in respellings from different sources, we retained only words containing at least eight phonemes.

Our respelling generation system is a multi-stage process. The input is a sequence of phonemes representing the pronunciation of the word. We start by identifying acceptable syllabiﬁcations of phonemes as described in Section 5.1. For each syllable, we take up to ﬁve respelling candidates produced by the P2L model (Section 5.2), and between one and three candidates proposed by the context-sensitive respeller (Section 5.3). The next stage involves ﬁltering the candidate respellings with the L2P model (Section 6.1), and the vowel counter (Section 6.2).
If all candidates happen to be rejected, we retain the ﬁrst output of the context-sensitive respeller as the default. The candidate respellings are then scored by the SVM model (Section 6.3). At this point the syllables are combined into word respellings, which are ranked according to their syllable score average.
Finally, the lexical reviser described in Section 6.4 is applied to the top candidate in an attempt to further improve the result.

We conducted an experiment with human evaluators using a specially developed graphical annotation program with synthesized word pronunciations.
The evaluators were students enrolled in an introductory linguistic course, who were not involved in our project. 13 out of 20 evaluators declared themselves as native speakers of English.
The evaluation process involves 40 randomly selected words: 10 from Fraser’s set, and 30 from the Web set. For each word, the program displays in a random sequence three respellings, which are from  the following sources: (1) the Baseline approach described in Section 4.3, (2) our system, and (3) either expert design (for Fraser’s set) or the Web (for the Web set). In order to reduce bias, the original spelling of the word is not shown. Each respelling is judged separately with regards to ambiguity, and those that are judged ambiguous are removed from further consideration. Next, an audio clip synthesized from the phonemic sequence representing the intended pronunciation is played through headphones. For each of the remaining respellings, the evaluators decide whether it is correct with respect to the recorded pronunciation. Finally, if more than one respelling have been judged both unambiguous and correct, the evaluators are asked to identify the one that they prefer.
The results of the experiment are shown in Table 2. Our system signiﬁcantly outperforms both Web respellings and the Baseline approach in terms of unambiguity and correctness. In addition, the respellings produced by our system are more likely to be preferred over the Web respellings, and more than twice as likely to be preferred over the baseline respellings than vice versa. The results on the small Fraser’s set are less conclusive, but suggest that in terms of overall quality our system is much closer to the upper bound than to the baseline.

Human evaluation is expensive and limited in terms of the number of variant respellings. Moreover, human judgements may be biased by previously seen respellings or by the familiarity with the standard spelling of a word. An automated evaluation is much less constrained, and facilitates an ablation study to determine the relative importance of various components of our system.

eSpeak is a publicly available speech synthesizer5 that can also convert text into phonemic sequences.
The letter-to-phoneme component for English utilizes about ﬁve thousand rules, and a dictionary of about three thousand words, names, and abbreviations. In our evaluation, we treat eSpeak as a “black box” which translates a respelling into its most likely pronunciation. By determining if there is a match between the output of eSpeak and the intended pronunciation, we directly test the correctness of the respelling, and indirectly also its ambiguity.
The results of the automated evaluation are shown in Table 4. The accuracy on the original orthography is low, which is unsurprising since the test sets contain mostly rare, unusually spelled words. Neither the baseline nor the Web respellings are signiﬁcantly easier foreSpeak than the original words.
On the other hand, respellings generated by our system make a massive difference, boosting phoneme accuracy to well over 90% on both sets. They are also signiﬁcantly more effective than the expert respellings.
Table 5 shows the results of our system on the  No. Spelling IPA Web/HF respelling 1 Incirlik [inÃirlik] injirlik 2 Captopril [kæpt @prIl] kap-toh-pril 3 Coquitlam [kokwItl@m] ko-kwit-lam 4 Karolina [kArOlinA] karo-leena 5 subluxation [s@bl@kseS@n] sub-luck-say-shun 6 swingle [swINg@l] swing-gl 7 cockatrice [kAk@traIs] kok-a-trice 8 recalesce [rik@lEs] ree-ka-less 9 jongleur [ZANgl@r] jong-gler 10 ylang-ylang [ilæ Nilæ N] ee-lang-ee-lang  System respelling een-jeer-leek cap-tuh-prill koh-quit-lumb car-awl-ee-nah suh-bluck-say-shun swing-gull cock-uh-trice re-cull-ess zhahng-gler eel-ang-eel-ang  Web set with various modules disabled, which provides an estimate of their importance. Neither the context-sensitive respeller nor dictionary lookup seem to contribute much to eSpeak’s performance.
On the other hand, disabling the P2L generator produces a signiﬁcant drop in word accuracy, while removing the L2P correctness ﬁlter almost doubles the phoneme error rate. Interestingly, removing syllable breaks from the output of the full system has an even greater negative impact.

Each of 20 evaluators judged 3 variant respellings of 40 different words. The average number of judgments per word was 7.4 for the 27 words in Fraser’s set, and 2.8 for the 212 words in the Web set (due to random selection, 19 words from the Web set were not judged). Table 3 shows examples of respellings that were judged by at least ﬁve evaluators. The score columns indicate the proportion of the evaluators that judged a particular respellings as unambiguous and correct. The baseline respellings are not included as their scores were rarely higher than the scores of the other respellings for a given word.
An interesting exception is palimpsest, for which the baseline respelling is identical to the actual spelling of the word.
Examples 1-5 in Table 3 come from the Web set, while examples 6-10 are from Fraser’s set. The low scores of the ﬁrst three Web respellings can be attributed to speciﬁc letter-to-phoneme mappings: [i]→i, [@]→oh, and [@]→a. Each of the examples 3-5 indicate the evaluators’ acceptance of a particular respelling device: silent letters, multi-syllable  units, and dictionary words. In examples 6-8, the syllables immediately after the ﬁrst hyphen in Helen Fraser’s respellings seem to be problematic. The expert respelling of jongleur is considered correct even though the initial j suggests [Ã], not [Z]. Finally, the last example demonstrates that the hyphenation choice can result in very different judgments.

In this paper, we introduced the task of automatically generating respellings from the given pronunciation. We investigated the characteristics of good respellings, and discussed three direct methods of their creation. We proposed a system that combines supervised and unsupervised learning with phonetic and orthographic principles. The evaluation experiment involving human participants indicates that the respellings produced by our system are better on average than those found on the Web. The automated veriﬁcation demonstrates that they are also much easier to interpret for a rule-based text-to-speech converter. In the future we plan to address the related tasks of improving existing respellings, and assisting writers in creating respellings without direct access to the phonemic representations.

We thank Aditya Bhargava and Clarke Chomyc for their contribution to the creation of data sets, and Ben Tucker for advice on the complexities of the human evaluation experiment. This research was partially funded by the Natural Sciences and Engineering Research Council of Canada.