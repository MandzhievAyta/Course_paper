{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from nltk.parse import stanford\n",
    "import os\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tree import ParentedTree\n",
    "from IPython.display import Image, display\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RemoveShortStubs(paragraphs):\n",
    "    startStub = [u'it', u'there']\n",
    "    endStub = u'that'\n",
    "    for par in paragraphs:\n",
    "        for sent in par:\n",
    "            startidx = []\n",
    "            endidx = []\n",
    "            flag = 0\n",
    "            for i, word in enumerate(sent):\n",
    "                if word[0].lower() in startStub:\n",
    "                    flag = 1\n",
    "                    start_candidate = i\n",
    "                if (word[0] == endStub) and (flag == 1):\n",
    "                    flag = 0\n",
    "                    startidx.append(start_candidate)\n",
    "                    endidx.append(i)\n",
    "            for stubidx in reversed(range(len(startidx))):\n",
    "                del sent[startidx[stubidx]: endidx[stubidx] + 1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcess(text):\n",
    "    #Deleting literature references\n",
    "    #[([] -open bracket ( or [;  [^([]*? - lazy; \\d+ - number; [^])]*?; [])] - close bracket ) or ]\n",
    "    print \"regex:\"\n",
    "    time_beg = datetime.datetime.now()\n",
    "    pat = re.compile(r\"[([][^([]*?\\d+[^])]*?[])]\", re.IGNORECASE or re.DOTALL)\n",
    "    text = re.sub(pat, r\"\", text)\n",
    "    text = re.sub(r'([,.:;/)(\"])', r' \\g<1> ', text)\n",
    "    print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    \n",
    "    paragraphs = tokenize.sent_tokenize(text)\n",
    "    print paragraphs\n",
    "    #Splitting into paragraphs\n",
    "    #paragraphs = re.split(u\"\\n\", text)\n",
    "    #Splitting into sentences\n",
    "#     for i, par in enumerate(paragraphs):\n",
    "#         paragraphs[i] = tokenize.sent_tokenize(par)\n",
    "    #Tagging all words\n",
    "    print \"downloading stanford pos tagger:\"\n",
    "    time_beg = datetime.datetime.now()\n",
    "    st = StanfordPOSTagger('english-left3words-distsim.tagger')\n",
    "    print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    time_beg_tagging = datetime.datetime.now()\n",
    "#     for i, par in enumerate(paragraphs):\n",
    "#         for sentence in range(len(par)):\n",
    "#             paragraphs[i][sentence] = paragraphs[i][sentence].split()\n",
    "\n",
    "    for i, par in enumerate(paragraphs):\n",
    "        paragraphs[i] = paragraphs[i].split()\n",
    "    print (datetime.datetime.now() - time_beg_tagging).total_seconds()\n",
    "#     for i, par in enumerate(paragraphs):\n",
    "#         #for sentence in range(len(par)):\n",
    "#         print \"tagging of one sent:\"\n",
    "#         time_beg = datetime.datetime.now()\n",
    "#         paragraphs[i] = st.tag_sents(paragraphs[i])\n",
    "#         print paragraphs[i]\n",
    "#         print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    paragraphs = st.tag_sents(paragraphs)        \n",
    "#     for i, par in enumerate(paragraphs):\n",
    "#         for sentence in range(len(par)):\n",
    "#             paragraphs[i][sentence][:] = [x for x in paragraphs[i][sentence] if x[0] != u'(' and x[0] != u')']\n",
    "    print \"actual tagging:\"\n",
    "    print (datetime.datetime.now() - time_beg_tagging).total_seconds()\n",
    "    #print paragraphs\n",
    "    print \"remove short stubs:\"\n",
    "    time_beg = datetime.datetime.now()\n",
    "    RemoveShortStubs(paragraphs)\n",
    "    print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Information filtering agents and collaborative filtering both attempt to alleviate information overload by identifying which items a user will find worthwhile.', u'Information filtering (IF) focuses on the analysis of item content and the development of a personal user interest profile.', u'Collaborative filtering (CF) focuses on identification of other users with similar tastes and the use of their opinions to recommend items.', u'Each technique has advantages and limitations that suggest that the two could be beneficially combined.\\r'], [u'This paper shows that a CF framework can be used to combine personal IF agents and the opinions of a community of users to produce better  recommendations than either agents or users can produce  alone.', u'It  also shows that using CF to create a personal combination of a set of agents produces better results than either individual agents or  other combination mechanisms.', u'One  key implication of these results is that users can avoid having to select among agents; they can use them all and let the CF framework select the best ones for them.\\r'], [u'\\r'], [u'\\r'], [u'Recommender systems help individuals and communities address the challenges of information overload.', u'Information filtering recommenders look at the syntactic and semantic content of items to determine which are likely to be of interest or value to a user.', u'Collaborative filtering recommenders use the opinions of other users to predict the value of items for each user in the community.', u'For example, in the domain of movie selection, content filtering would allow recommendation based on the movie genre (horror, comedy, romance, etc.)', u'and cast/credits (Woody Allen, Steven Spielberg, Bette Midler).', u'Collaborative  filtering,  by  contrast,  might  be completely unaware of genre and cast, but would know that a group of like-minded people recommends \"Hoop Dreams\" and suggests avoiding \"Dumb and Dumber.\"'], [u'In this work, we examine collaborative filtering, personal information filtering agents, and mechanisms for combining them to produce a better recommender system.', u'The next section reviews existing approaches to alleviating information overload, including a variety of content-based and collaborative approaches, and presents our model for how these approaches can be more effective when combined.', u'The following sections present our experimental design and results.', u'We conclude with observations about the implications of these results.\\r'], [u'\\r'], [u'Each day, more and more books, journal articles, web pages, and movies are created.', u'As each new piece of information competes for our attention,  we  quickly become overwhelmed and seek assistance in identifying the most interesting, worthwhile, valuable, or entertaining items on which we should expend our scarce money and time.', u'Historically, humans have adapted well to gluts of information.', u'Our senses are tuned to notice change and the unusual.', u'Our ability to communicate allows us to collaboratively address large problems.', u'And, we have developed an astonishingly good ability to make quick judgements-indeed, we often can judge a book by its cover, an article by its title or abstract, or a movie by its trailer or advertisement.', u'Today we are also finding that it is becoming easier and easier to produce and publish content.', u'As computers, communication, and the Internet make it easier for anyone and everyone to speak to a large audience, we find that even our well-developed filtering skills may be inadequate.\\r'], [u'In response to the challenge of information overload, we have sought to develop useful recommender systems- systems that people can use to quickly identify content that will likely interest them.', u\"This project draws from work in creating recommender systems for movies - film fans tell the MovieLens system (movielens.umn.edu) how much they like or dislike movies they've already seen, and MovieLens recommends other movies they would likely enjoy.\\r\"], [u'There are three different technologies that are commonly used to address information overload challenges.', u'Each technology focuses primarily on a particular set of tasks or questions.', u'Information retrieval focuses on tasks involving fulfilling ephemeral interest queries such as finding the movies directed by Woody Allen.', u'Information filtering focuses on tasks involving classifying streams of new content into categories, such as finding any newly released movies directed by Steven Spielberg (to consider watching) or any newly released movies without an English-language soundtrack or subtitles (to reject).', u'Collaborative filtering focuses on answering two questions:\\r'], [u'Information retrieval (IR) systems focus on allowing users to express queries to select items that match a topic of interest to fulfill a particular information need.', u'They may index a collection of documents using either the full text of the document or document abstracts.', u'For non- textual items such as movies, IR systems index genres, keywords, actors, directors, etc.', u'IR systems are generally optimized for ephemeral interest queries, such as looking up a topic in the library.', u'(Belkin and Croft 1992) Internet search engines are popular IR systems, and the Internet Movie Database (www.imdb.com) provides extensive support for IR queries on movies.\\r'], [u'An IR front-end is useful in a recommender system both as a mechanism for users to identify specific  movies about which they would like to express an opinion and for narrowing the scope of recommendation.', u'For example, MovieLens allows users to specifically request recommendations for newer movies, for movies released in particular time periods, for particular movie  genres such as comedy and documentary, and for various combinations of movie.', u'Information retrieval techniques are less valuable in the actual recommendation process, since they capture no information about user preferences other than the specific query.', u'For that reason, we do not consider IR further in this paper.\\r'], [u'Information filtering (IF) systems require a profile of user needs or preferences.', u'The simplest systems require the user to create this profile manually or with limited assistance.', u'Examples of these  systems  include:  \"kill files\" that are used to filter out advertising, e-mail filtering software that sorts e-mail into categories based on the sender, and new-product notification services that request notification when a new book or album by a favorite author or artist is released.', u\"More advanced IF systems may build a profile by learning the user's preferences.\", u\"A wide range of agents including Maes' agents for e-mail and Usenet news filtering (Maes 1995) and Lieberman's Letizia (Lieberman 1997) employ learning techniques to classify, dispose of, or recommend documents based on the user's prior actions.\", u\"Similarly, Cohen's Ripper system has been used to classify e-mail (Cohen 1996); alternative approaches use other learning techniques and term frequency (Boone 1998)\\r\"], [u'Information filtering techniques have a central role in recommender systems.', u'IF techniques build a profile of user preferences that is particularly valuable when a user encounters new content that has not been rated before.', u\"An avid Woody Allen fan doesn't need to wait for reviews to decide to see a new Woody Allen film, and a person who hates horror films can as quickly dismiss a  new horror film without regret.\", u'IF techniques also have an important property that they do not depend on having other users in the system, let alone users with  similar tastes.', u'IF techniques can be effective, as we shall see, but they suffer certain drawbacks, including requiring a source of content information, and not providing much in the way of serendipitous discovery; indeed, a Woody Allen-seeking agent would likely never discover a non- Woody Allen drama that just happens to appeal greatly to most Woody Allen fans.\\r'], [u'Collaborative filtering (CF) systems build a database of user opinions of available items.', u'They use the database to find users whose opinions are similar (i.e., those that are highly correlated) and make predictions of user opinion on an item by combining the opinions of other like- minded individuals.', u\"For example, if Sue and Jerry have liked many of the same movies, and Sue liked Titanic, which Jerry hasn't seen yet, then the system may recommend Titanic to Jerry.\", u'While Tapestry (Goldberg et al.', u'1992), the earliest CF system, required explicit user action to retrieve and evaluate ratings, automatic CF systems such as GroupLens (Resnick et al.', u'1994) (Konstan et al.', u'1997) provide predictions with little or no user effort.', u\"Later systems such as Ringo (Shardanand and Maes 1995) and Bellcore's Video Recommender (Hill et al.\", u'1995) became widely used sources of advice on music and movies respectively.', u'More recently, a number of systems have begun to use observational ratings; the system infers user preferences from actions rather than requiring the user to explicitly rate an item (Terveen et al.', u'1997).', u'In the past year, a wide range of web sites have begun to use CF recommendations in a diverse set of domains including books, grocery products, art, entertainment, and information.\\r'], [u'Collaborative filtering techniques can be an  important part of a recommender system.', u'One key advantage of CF is that it does not consider the content of the items being recommended.', u'Rather than map users to items through \"content attributes\" or \"demographics,\" CF treats each item and user individually.', u'Accordingly, it becomes possible to discover new items of interest simply because other people liked them; it is also easier to provide good recommendations even when the attributes of greatest interest to users are unknown or hidden.', u'For example, many movie viewers may not want to see a particular actor or genre so much as \"a movie that makes me feel good\" or \"a smart, funny movie.\"', u\"At the same time, CF's dependence on human ratings can be a significant drawback.\", u'For a CF system to work well, several users must evaluate each item; even then, new items cannot be recommended until some users have taken the time to evaluate them.', u'These limitations, often referred to as the sparsity and first-rater problems, cause trouble for users seeking obscure movies (since nobody may have rated them) or advice on movies about to be released (since nobody has had a chance to evaluate them).\\r'], [u'\\r'], [u'Several systems have tried to combine  information filtering and collaborative filtering techniques in an effort to overcome the limitations of each.', u'Fab (Balabanovic and Shoham 1997) maintains user profiles of interest in web pages using information filtering techniques, but uses collaborative filtering techniques to identify profiles with similar tastes.', u'It then can recommend documents across user profiles.', u'(Basu, Hirsh, and Cohen 1998) trained the Ripper machine learning system with a combination of content data and training data in an effort  to  produce better   recommendations.', u'Researchers working in collaborative filtering have proposed techniques for using IF profiles as a fall-back, e.g., by requesting predictions for a director or actor when there is no information on the specific movie, or by having dual systems and using the IF profile when the CF system cannot produce a high-quality recommendation.\\r'], [u'In earlier work, Sarwar, et al.', u'(1998) showed that a simple but consistent rating agent, such as one that assesses the quality of spelling in a Usenet news article, could be a valuable participant in a collaborative filtering community.', u'In that work, they showed how these  filterbots-ratings robots that participate as members of a collaborative filtering system -  helped users who agreed with them by providing more ratings upon which recommendations could be made.', u'For users who  did  not agree with the filterbot, the CF framework would notice a low preference correlation and not make use of its ratings.\\r'], [u'This work extends the filterbot concept in three key ways.', u'First, we use a more intelligent set of filterbots, including learning agents that are personalized to an individual user.', u'Second, we apply this work to small communities, including using CF to serve a single human user.', u'Third, we evaluate the simultaneous use of multiple filterbots.', u'In addition, we explore other combination mechanisms as alternatives to CF.', u'We demonstrate that CF is a useful framework both for integrating agents and for combining agents and humans.\\r'], [u'\\r'], [u'\\r'], [u'The context in which these hypotheses are tested is a small, anonymous community of movie fans.', u'The combination of small size and non-textual content cause disadvantages for both collaborative filtering and information filtering; it provides a middle-ground between the common contexts for collaborative filtering (many users, little content information) and information filtering (one user, much content information).\\r'], [u'\\r'], [u'Data Set\\r'], [u'The user ratings for this experiment were drawn from the MovieLens system (http://movielens.umn.edu) which has more than 3 million ratings from over 80,000 users.', u'Fifty users were selected at random from the set of users with more than 120 movie ratings.', u'For each user, three sets of movies/ratings were selected at random without replacement.', u'The first set of 50 ratings, termed the training set, was set aside for use in training the personalized information filtering agents.', u'The second set of 50 ratings, termed the correlation set was used when combining users, agents, or both together.', u'The final set of 20 ratings served as the test set.', u'In each experiment, the test ratings of the target user were withheld and compared against the recommendation value produced by the system.\\r'], [u'\\r'], [u'Metrics\\r'], [u'Recommender systems researchers use several different measures for the quality of recommendations produced.\\r'], [u'Coverage metrics evaluate the number of items for which the system could provide recommendations.', u'In many systems, coverage decreases as a function of accuracy-the system can produce fewer accurate recommendations or more inaccurate ones.', u'Because our information filtering systems provide total coverage, we do not report coverage except as part of the analysis of the standard CF system.\\r'], [u'\\r'], [u'Statistical accuracy metrics evaluate the accuracy of a filtering system by comparing the numerical prediction values against user ratings for the items that have both predictions and ratings.', u'(Shardanand and Maes, 1995) and (Sarwar et al, 1998) have both used mean absolute error (MAE) to measure the performance of a prediction engine.', u'Other metrics used include root mean squared error (Sarwar et al.', u'1998) and correlation between ratings and predictions (Hill et al.', u'1995) (Konstan et al.', u'1997) (Sarwar et al.', u'1998).', u'Our experience has shown that these metrics typically track each other closely.', u'We have chosen to report mean absolute error, therefore, because it is the most commonly used and the easiest to interpret directly.\\r'], [u'Decision-support accuracy metrics evaluate how effective a prediction engine is at helping a user select high-quality items from the item set.', u'These metrics are based on the observation that, for the majority of users, filtering is a binary operation - they will either view the item, or they will not.', u'If this is true, then whether an item has a rating of 1.5 or 2.5 on a five-point scale is irrelevant if the user only views items with a rating of 4 or higher.', u'The most common decision-support accuracy measures are reversal rate, weighted errors, and ROC sensitivity.', u'Reversal rate is the frequency with which the system makes recommendations that are extremely wrong.', u'On a five point scale, it is commonly defined as the percentage of recommendations where the recommendation was off by 3 points or more.', u'Weighted error metrics give extra weight to large errors that occur when the user  has  a strong opinion about the item.', u'For example, errors might count double or more when the user considers the item a favorite (5 out of 5).', u'ROC sensitivity is a signal processing measure of the decision making power of a filtering system.', u'Operationally, it is the area under the receiver operating characteristic curve (ROC) - a curve that plots the sensitivity vs. 1 - specificity of the  test (Swets 1988).', u'Sensitivity refers to the probability of a randomly selected good item being accepted by the filter.', u'Specificity is the probability of a randomly selected bad item being rejected by the filter.', u'Points on the ROC curve represent trade-offs supported by the filter.', u'A good filter might allow the user to choose between receiving 90% of the good items while accepting only 10% of the bad ones, or receiving 95% of the good ones with 20% of the bad ones.', u'A random filter always accepts the same percentage of the good and the bad items.', u'The  ROC  sensitivity ranges from 0 to 1 where 1 is perfect and 0.5 is random.\\r'], [u'We use ROC sensitivity as our decision support accuracy measure.', u'To operationalize ROC, we must determine which items are \"good\" and which are \"bad.\"', u\"We use the user's own rating, with a mapping that 4 and 5 are good and 1,2, and 3 are bad.\", u'Our experience has shown that this reflects user behavior on MovieLens.', u'We found that one user had no movies rated below 4; we eliminated that user from the statistics compiled for each experiment.\\r'], [u'\\r'], [u'Evaluating the hypotheses in the face of multiple metrics can be a challenge.', u'We considered it important to consider both statistical and decision-support accuracy in evaluating different recommender systems.', u'When several agents, for example, provide different but incomparable trade-offs among the two metrics, we consider each one to be a possible \"best agent\" and compare each of them against the alternative recommender.', u'We consider one alternative to dominate another, however, if there is a significant improvement in one metric and no significant difference in the other.\\r'], [u'Statistical significance is assessed for mean absolute errors using the Wilcoxan test on paired errors.', u'Differences reported as significant are based on a significance level of p<0.05.', u'Statistical significance assessment for ROC sensitivity is less clear;* from experience we therefore assert that changes of 0.01 or more are \"meaningful\" and smaller differences are \"not meaningful.\"'], [u'\\r'], [u'User Opinions Only.', u'Extensive  research  has  already been performed on the problem of generating recommendations from a set of user opinions.', u'Nearest- neighbor collaborative filtering is already generally accepted to be the most effective mechanism for performing this task, and we therefore use it (Breese, 1998).', u'In particular, we use the DBLens research collaborative filtering engine developed by the GroupLens Research project for exploration of collaborative filtering algorithms.', u'DBLens allows experimenters to control several parameters that trade among performance, coverage, and accuracy.', u'For our experiments, we set each of these to prefer maximum coverage and to use all data regardless of performance.\\r'], [u'The CF result set was computed for each user by loading the correlation data set (50 ratings per user) into the engine, then loading the test set (20 ratings per user) for each user, and requesting a prediction for each test set\\r'], [u'We modified traditional TFIDF by counting each keyword as either occurring (1) or not occurring (0) in any given movie.', u'Accordingly, the TF vector for a movie is produced by inserting a 1 for each keyword and 0 elsewhere.\\r'], [u'Building the user profile requires a balanced set of user ratings, so we subtract 3 from each rating to transform them to a -2 to +2 scale.', u\"For each movie in the 50-rating training set, we produce a keyword preference vector that is the product of the transformed rating, the movie's TF vector, and the IDF vector.\", u'We then normalize the keyword preference vector to length 1.', u\"The mean of the user's 50 keyword preference vectors is the user profile.\\r\"], [u'The DGBot produces ratings for all movies at once.', u'For each  movie,  it  computes  the  dot  product  of  the  user profile vector and the TF vector.', u'Those scores are then ranked and broken into rating levels with a distribution matching the MovieLens overall rating distribution.', u'The top 21% of movies received a rating of 5, the next 34% a rating of 4, the next 28% a 3, the next 12% a 2, and the bottom 5% a 1.', u'While each user has a separate user profile vector and set of recommendations, the TF and IDF vectors could be re-used from user to user.\\r'], [u'RipperBot was created using Ripper, an inductive logic program created by William Cohen (Cohen, 1995).', u'We found that Ripper performed best when trained on a set of data limited to genre identifiers and the 200 most frequent keywords.', u'Ripper also works best when asked to make binary decisions, so for each user we trained four Ripper instances, tuned to distinguish between 5/4321, 54/321, 543/21, and 5432/1 respectively.', u'Each instance was trained on the 50-rating training set along with the identifiers and keywords for those 50 movies.', u'After training, we asked each instance to classify the entire set of movies and summed the number of Ripper instances that indicated the higher value and added one to create a recommendation value.\\r'], [u'Ripper requires substantial tuning; we experimented with several parameters and also relied on advice from (Basu Hirsh and Cohen, 1998).', u'In particular, we adjusted default settings to allow negative tests in set value attributes and experimented by varying the loss ratio.', u'We found a loss ration of 1.9 to give us the best results.\\r'], [u\"The GenreBots consisted of 19 simple bots  that  rated each movie a 5 if the movie matched the bot's genre and a\\r\"], [u'3 otherwise.', u\"For example, Toy Story, which is a children's animated comedy would receive a 5 from the ChildrensBot, the AnimatedBot and the ComedyBot, and a 3 from each of the remaining bots.\", u'Genre data was obtained from IMDB.\\r'], [u'A Mega-GenreBot was created for each user.', u\"This was done by using linear regression and training the bot on each user's training set.\", u\"A user's known rating  was treated as a dependent variable of the 19 individual GenreBots.\", u'The regression coefficients formed  an equation that could then be used to generate predictions for each other movie from the genre identifiers.\\r'], [u'Combinations of IF Agents.', u'We identified four different strategies for combining agents: selecting one agent for each person, averaging the agents together, using regression to create a personal combination, and using CF to create a personal combination.', u'For all but the first of these, we found it valuable to create two combinations: one that used all 19 GenreBots and one that used the Mega-GenreBot.', u'Adding the 3 DGBots and RipperBot, we refer to these as 23-agent and 5-agent versions, respectively.\\r'], [u'BestBot.', u'The best agent per user was selected by testing each bot on the correlation data set (50 ratings) and selecting the bot with the lowest MAE.', u'BestBot then used the ratings generated by that bot for the test data set to produce statistics for evaluation.\\r'], [u'Regression.', u'We used linear regression to produce a \"best fit\" combination for a given user.', u\"To do this we used the predictions on the correlation sets for the 23 and 5 agents respectively as the independent variables and the known user's rating as the dependent variable.\", u\"Using the resultant weights, we could generate predictions for the movies in the test sets by creating linear combinations of the agents' recommendations.\\r\"], [u'CF Combination.', u'We used the DBLens CF engine to create a CF combination of agents.', u\"For this purpose, we loaded all ratings from the 5 or 23 agents into the engine, along with the user's 50 ratings from the correlation set.\", u\"We generated predictions for the user's 20 test movies.\", u'The ratings database was cleared after each user.', u'The parameters used were the same as for the simple CF case.\\r'], [u'Combination of Users and IF Agents.', u'Because user ratings were incomplete, and because CF with 23 agents proved to be the most effective combination of IF agents, we used CF to combine the 23 agents and all 50 users.', u'The method is identical to the CF combination of agents except that we also loaded the ratings for the other 49 users.', u'Again, the database was cleared after each user.\\r'], [u'\\r'], [u'\\r'], [u'8H1:  Collaborative Filtering better than Single Agents  We  hypothesized  that  collaborative  filtering  using  the opinions of the 50-user community would provide better results than any individual agent.', u'To compare these, we first identified the best individual agent.', u'We evaluated the   three   DGBots,    RipperBot,   the   19   individual genreBots,  and  the  personalized  Mega-GenreBot  (see table 1).', u'Of these, only RipperBot, Mega-GenreBot, and the DGBot that used both cast and keywords were not dominated by other agents.', u'RipperBot had the highest accuracy (lowest MAE) by far, but low ROC sensitivity (poor decision support).', u'The combined DGBot has the\\r'], [u'\\r'], [u'\\t\\r'], [u'\\r'], [u'highest ROC sensitivity, but relatively low accuracy.', u'The Mega-GenreBot has the second-best accuracy and second- best decision-support.', u'We compare these three against the results of collaborative filtering using user opinions.\\r'], [u'Collaborative filtering is significantly less accurate than RipperBot, but has a meaningfully higher ROC sensitivity value.', u'In effect, while RipperBot avoids making large errors, it performs little better than random at helping people find good movies and avoid bad ones.', u'If accuracy were paramount, H1 would be rejected.\\r'], [u'Collaborative filtering is significantly more accurate than the   combined   DGBot   and   has   comparable   ROC\\r'], [u'\\r'], [u'there was no significant difference in MAE, and the ROC value for CF23 was dramatically better than for RipperBot.', u'Accordingly we accept H2.', u\"We also observe that H2 depended on using collaborative filtering technology; no other combination method was close to dominating RipperBot's accuracy.\\r\"], [u'8H3: CF of Users better than Combination of Agents\\r'], [u'At this stage, it is clear that we must reject H3.', u'Table 3 summarizes the results, but we recognize that collaborative filtering with a group of 50 users is indeed not as accurate or valuable as we had hypothesized.\\r'], [u'\\r'], [u'The Mega-GenreBot is slightly worse than collaborative filtering of user opinions on both MAE and ROC, but the differences were not statistically significant.', u'We would consider Mega-GenreBot to be a good pragmatic substitute for user-based collaborative filtering for a small community.', u'Furthermore, the collaborative filtering result was only able to provide coverage of 83% (other desired recommendations could not be made due to a lack of ratings for those movies).\\r'], [u'We hypothesized that the combination of the opinions of a community of users and the personalized agents for a given user will provide that user with better results than either users alone or agents alone.', u\"From both H2 and H3 we found that collaborative filtering of a single user and that user's 23 agents provides the best accuracy and decision support of all agent-only or user-only methods tested.\", u'Table 4 shows a small, but statistically significant improvement in accuracy resulting from including the other users in the collaborative filtering mix.', u'ROC also improves, but not by a meaningful amount.\\r'], [u'We  hypothesized  that  combining  several  agents  would\\r'], [u'yield better results than any single personalized agent.', u'In testing H1, we found that for single agents, RipperBot had the best accuracy value (MAE), DGBot Combo had the best decision support value (ROC) and the  Mega- GenreBot was competitive with both values.', u'In table 2 we compare these values to those obtained from the seven methods of combining the agents - regression, agent average, collaborative filtering of a single user and its bots, and manually selecting the \"best bot.\"'], [u'The most important results we found were the value of combining agents with CF and of combining agents and users with CF.', u'In essence, these results suggest that an effective mechanism for producing high-quality recommendations is to throw in any available data and allow the CF engine to sort out which information  is useful to each user.', u'In effect, it becomes less important to invent a brilliant agent, instead we can simply invent a collection of useful ones.', u'We should point out that these experiments tested the quality of the resulting recommender system, not the performance or economics of such a system.', u'Current CF recommendation engines cannot efficiently handle \"users\" who rate all items and re-rate them frequently as they \"learn.\"', u'To  take advantage of learning agents, these engines must be redesigned to accommodate \"users\" with dynamic rating habits.', u'We are examining several different CF engine designs that could efficiently use filterbots.\\r'], [u'We were also pleased, though somewhat surprised, to find that CF outperformed linear regression as a combining mechanism for agents.', u\"While linear regression should provide an optimal linear fit, it appears that CF's non- optimal mechanism actually does a better job avoiding overfitting the data when the number of columns approaches the number of rows.\", u'CF also has the advantage of functioning on incomplete (and indeed very sparse) data sets, suggesting that it retains its value as a useful combination tool whenever human or agents are unlikely to rate each item.\\r'], [u\"We were surprised by several of the results that  we'd found, and sought to explain them.\", u'Foremost, we clearly overestimated the value of collaborative filtering for a small community of 50 users.', u'In retrospect, our expectations may have been built from our own positive experiences when starting CF systems with a small group of researchers and friends.', u'Those successes may have been due in part to close ties among the users; we often had seen the same movies and many had similar tastes.', u'Using real users resulted in real diversity which may explain the lower, and more realistic, value.', u'Future work should both incorporate larger user sets (other experiments have consistently shown MAE values in the range of 0.71-0.73 and ROC sensitivity values near 0.72 for MovieLens communities with thousands of users) and look explicitly at closer-knit communities to see whether a smaller but more homogeneous community would have greater benefits from collaborative filtering.\\r'], [u'We also were surprised by the results we achieved using Ripper.', u'We were impressed by its accuracy, after extensive tuning, but dismayed by how close to random it was in distinguishing good from bad movies.', u'We are still uncertain as to why RipperBot performs as it does, and believe further work is needed to understand why it behaves as it does and whether it would be possible to train it to perform differently.\\r'], [u'In the future, we plan to examine further combinations of users and agents in recommender systems.', u'In particular, we are interested in developing a combined community where large numbers of users and agents co-exist.', u\"One question we hope to answer is whether users who agree with each other would also benefit from the opinions of each other's trained agents.\\r\"], []]\n"
     ]
    }
   ],
   "source": [
    "paragraphs = re.split(u\"\\n\", text)\n",
    "for i, par in enumerate(paragraphs):\n",
    "    paragraphs[i] = tokenize.sent_tokenize(par)\n",
    "print paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex:\n",
      "0.007292\n",
      "[u'Information filtering agents and collaborative filtering both attempt to alleviate information overload by identifying which items a user will find worthwhile .', u'Information filtering  ( IF )  focuses on the analysis of item content and the development of a personal user interest profile .', u'Collaborative filtering  ( CF )  focuses on identification of other users with similar tastes and the use of their opinions to recommend items .', u'Each technique has advantages and limitations that suggest that the two could be beneficially combined .', u'This paper shows that a CF framework can be used to combine personal IF agents and the opinions of a community of users to produce better  recommendations than either agents or users can produce  alone .', u'It  also shows that using CF to create a personal combination of a set of agents produces better results than either individual agents or  other combination mechanisms .', u'One  key implication of these results is that users can avoid having to select among agents ;  they can use them all and let the CF framework select the best ones for them .', u'Recommender systems help individuals and communities address the challenges of information overload .', u'Information filtering recommenders look at the syntactic and semantic content of items to determine which are likely to be of interest or value to a user .', u'Collaborative filtering recommenders use the opinions of other users to predict the value of items for each user in the community .', u'For example ,  in the domain of movie selection ,  content filtering would allow recommendation based on the movie genre  ( horror ,  comedy ,  romance ,  etc .  )', u'and cast / credits  ( Woody Allen ,  Steven Spielberg ,  Bette Midler )  .', u'Collaborative  filtering ,   by  contrast ,   might  be completely unaware of genre and cast ,  but would know that a group of like-minded people recommends  \" Hoop Dreams \"  and suggests avoiding  \" Dumb and Dumber .  \"', u'In this work ,  we examine collaborative filtering ,  personal information filtering agents ,  and mechanisms for combining them to produce a better recommender system .', u'The next section reviews existing approaches to alleviating information overload ,  including a variety of content-based and collaborative approaches ,  and presents our model for how these approaches can be more effective when combined .', u'The following sections present our experimental design and results .', u'We conclude with observations about the implications of these results .', u'Each day ,  more and more books ,  journal articles ,  web pages ,  and movies are created .', u'As each new piece of information competes for our attention ,   we  quickly become overwhelmed and seek assistance in identifying the most interesting ,  worthwhile ,  valuable ,  or entertaining items on which we should expend our scarce money and time .', u'Historically ,  humans have adapted well to gluts of information .', u'Our senses are tuned to notice change and the unusual .', u'Our ability to communicate allows us to collaboratively address large problems .', u'And ,  we have developed an astonishingly good ability to make quick judgements-indeed ,  we often can judge a book by its cover ,  an article by its title or abstract ,  or a movie by its trailer or advertisement .', u'Today we are also finding that it is becoming easier and easier to produce and publish content .', u'As computers ,  communication ,  and the Internet make it easier for anyone and everyone to speak to a large audience ,  we find that even our well-developed filtering skills may be inadequate .', u'In response to the challenge of information overload ,  we have sought to develop useful recommender systems- systems that people can use to quickly identify content that will likely interest them .', u'This project draws from work in creating recommender systems for movies - film fans tell the MovieLens system  ( movielens .', u'umn .', u\"edu )  how much they like or dislike movies they've already seen ,  and MovieLens recommends other movies they would likely enjoy .\", u'There are three different technologies that are commonly used to address information overload challenges .', u'Each technology focuses primarily on a particular set of tasks or questions .', u'Information retrieval focuses on tasks involving fulfilling ephemeral interest queries such as finding the movies directed by Woody Allen .', u'Information filtering focuses on tasks involving classifying streams of new content into categories ,  such as finding any newly released movies directed by Steven Spielberg  ( to consider watching )  or any newly released movies without an English-language soundtrack or subtitles  ( to reject )  .', u'Collaborative filtering focuses on answering two questions : \\r\\nInformation retrieval  ( IR )  systems focus on allowing users to express queries to select items that match a topic of interest to fulfill a particular information need .', u'They may index a collection of documents using either the full text of the document or document abstracts .', u'For non- textual items such as movies ,  IR systems index genres ,  keywords ,  actors ,  directors ,  etc .', u'IR systems are generally optimized for ephemeral interest queries ,  such as looking up a topic in the library .', u'Internet search engines are popular IR systems ,  and the Internet Movie Database  ( www .', u'imdb .', u'com )  provides extensive support for IR queries on movies .', u'An IR front-end is useful in a recommender system both as a mechanism for users to identify specific  movies about which they would like to express an opinion and for narrowing the scope of recommendation .', u'For example ,  MovieLens allows users to specifically request recommendations for newer movies ,  for movies released in particular time periods ,  for particular movie  genres such as comedy and documentary ,  and for various combinations of movie .', u'Information retrieval techniques are less valuable in the actual recommendation process ,  since they capture no information about user preferences other than the specific query .', u'For that reason ,  we do not consider IR further in this paper .', u'Information filtering  ( IF )  systems require a profile of user needs or preferences .', u'The simplest systems require the user to create this profile manually or with limited assistance .', u'Examples of these  systems  include :    \" kill files \"  that are used to filter out advertising ,  e-mail filtering software that sorts e-mail into categories based on the sender ,  and new-product notification services that request notification when a new book or album by a favorite author or artist is released .', u\"More advanced IF systems may build a profile by learning the user's preferences .\", u\"A wide range of agents including Maes' agents for e-mail and Usenet news filtering  and Lieberman's Letizia  employ learning techniques to classify ,  dispose of ,  or recommend documents based on the user's prior actions .\", u\"Similarly ,  Cohen's Ripper system has been used to classify e-mail  ;  alternative approaches use other learning techniques and term frequency \\r\\nInformation filtering techniques have a central role in recommender systems .\", u'IF techniques build a profile of user preferences that is particularly valuable when a user encounters new content that has not been rated before .', u\"An avid Woody Allen fan doesn't need to wait for reviews to decide to see a new Woody Allen film ,  and a person who hates horror films can as quickly dismiss a  new horror film without regret .\", u'IF techniques also have an important property that they do not depend on having other users in the system ,  let alone users with  similar tastes .', u'IF techniques can be effective ,  as we shall see ,  but they suffer certain drawbacks ,  including requiring a source of content information ,  and not providing much in the way of serendipitous discovery ;  indeed ,  a Woody Allen-seeking agent would likely never discover a non- Woody Allen drama that just happens to appeal greatly to most Woody Allen fans .', u'Collaborative filtering  ( CF )  systems build a database of user opinions of available items .', u'They use the database to find users whose opinions are similar  ( i .', u'e .', u',  those that are highly correlated )  and make predictions of user opinion on an item by combining the opinions of other like- minded individuals .', u\"For example ,  if Sue and Jerry have liked many of the same movies ,  and Sue liked Titanic ,  which Jerry hasn't seen yet ,  then the system may recommend Titanic to Jerry .\", u'While Tapestry  ,  the earliest CF system ,  required explicit user action to retrieve and evaluate ratings ,  automatic CF systems such as GroupLens   provide predictions with little or no user effort .', u\"Later systems such as Ringo  and Bellcore's Video Recommender  became widely used sources of advice on music and movies respectively .\", u'More recently ,  a number of systems have begun to use observational ratings ;  the system infers user preferences from actions rather than requiring the user to explicitly rate an item  .', u'In the past year ,  a wide range of web sites have begun to use CF recommendations in a diverse set of domains including books ,  grocery products ,  art ,  entertainment ,  and information .', u'Collaborative filtering techniques can be an  important part of a recommender system .', u'One key advantage of CF is that it does not consider the content of the items being recommended .', u'Rather than map users to items through  \" content attributes \"  or  \" demographics ,  \"  CF treats each item and user individually .', u'Accordingly ,  it becomes possible to discover new items of interest simply because other people liked them ;  it is also easier to provide good recommendations even when the attributes of greatest interest to users are unknown or hidden .', u'For example ,  many movie viewers may not want to see a particular actor or genre so much as  \" a movie that makes me feel good \"  or  \" a smart ,  funny movie .  \"', u\"At the same time ,  CF's dependence on human ratings can be a significant drawback .\", u'For a CF system to work well ,  several users must evaluate each item ;  even then ,  new items cannot be recommended until some users have taken the time to evaluate them .', u'These limitations ,  often referred to as the sparsity and first-rater problems ,  cause trouble for users seeking obscure movies  ( since nobody may have rated them )  or advice on movies about to be released  ( since nobody has had a chance to evaluate them )  .', u'Several systems have tried to combine  information filtering and collaborative filtering techniques in an effort to overcome the limitations of each .', u'Fab  maintains user profiles of interest in web pages using information filtering techniques ,  but uses collaborative filtering techniques to identify profiles with similar tastes .', u'It then can recommend documents across user profiles .', u'trained the Ripper machine learning system with a combination of content data and training data in an effort  to  produce better   recommendations .', u'Researchers working in collaborative filtering have proposed techniques for using IF profiles as a fall-back ,  e .', u'g .', u',  by requesting predictions for a director or actor when there is no information on the specific movie ,  or by having dual systems and using the IF profile when the CF system cannot produce a high-quality recommendation .', u'In earlier work ,  Sarwar ,  et al .', u'showed that a simple but consistent rating agent ,  such as one that assesses the quality of spelling in a Usenet news article ,  could be a valuable participant in a collaborative filtering community .', u'In that work ,  they showed how these  filterbots-ratings robots that participate as members of a collaborative filtering system -  helped users who agreed with them by providing more ratings upon which recommendations could be made .', u'For users who  did  not agree with the filterbot ,  the CF framework would notice a low preference correlation and not make use of its ratings .', u'This work extends the filterbot concept in three key ways .', u'First ,  we use a more intelligent set of filterbots ,  including learning agents that are personalized to an individual user .', u'Second ,  we apply this work to small communities ,  including using CF to serve a single human user .', u'Third ,  we evaluate the simultaneous use of multiple filterbots .', u'In addition ,  we explore other combination mechanisms as alternatives to CF .', u'We demonstrate that CF is a useful framework both for integrating agents and for combining agents and humans .', u'The context in which these hypotheses are tested is a small ,  anonymous community of movie fans .', u'The combination of small size and non-textual content cause disadvantages for both collaborative filtering and information filtering ;  it provides a middle-ground between the common contexts for collaborative filtering  ( many users ,  little content information )  and information filtering  ( one user ,  much content information )  .', u'Data Set\\r\\nThe user ratings for this experiment were drawn from the MovieLens system  and  have both used mean absolute error  ( MAE )  to measure the performance of a prediction engine .', u'Other metrics used include root mean squared error  and correlation between ratings and predictions    .', u'Our experience has shown that these metrics typically track each other closely .', u'We have chosen to report mean absolute error ,  therefore ,  because it is the most commonly used and the easiest to interpret directly .', u'Decision-support accuracy metrics evaluate how effective a prediction engine is at helping a user select high-quality items from the item set .', u'These metrics are based on the observation that ,  for the majority of users ,  filtering is a binary operation - they will either view the item ,  or they will not .', u'If this is true ,  then whether an item has a rating of 1 .', u'5 or 2 .', u'5 on a five-point scale is irrelevant if the user only views items with a rating of 4 or higher .', u'The most common decision-support accuracy measures are reversal rate ,  weighted errors ,  and ROC sensitivity .', u'Reversal rate is the frequency with which the system makes recommendations that are extremely wrong .', u'On a five point scale ,  it is commonly defined as the percentage of recommendations where the recommendation was off by 3 points or more .', u'Weighted error metrics give extra weight to large errors that occur when the user  has  a strong opinion about the item .', u'For example ,  errors might count double or more when the user considers the item a favorite  .', u'ROC sensitivity is a signal processing measure of the decision making power of a filtering system .', u'Operationally ,  it is the area under the receiver operating characteristic curve  .', u'Sensitivity refers to the probability of a randomly selected good item being accepted by the filter .', u'Specificity is the probability of a randomly selected bad item being rejected by the filter .', u'Points on the ROC curve represent trade-offs supported by the filter .', u'A good filter might allow the user to choose between receiving 90% of the good items while accepting only 10% of the bad ones ,  or receiving 95% of the good ones with 20% of the bad ones .', u'A random filter always accepts the same percentage of the good and the bad items .', u'The  ROC  sensitivity ranges from 0 to 1 where 1 is perfect and 0 .', u'5 is random .', u'We use ROC sensitivity as our decision support accuracy measure .', u'To operationalize ROC ,  we must determine which items are  \" good \"  and which are  \" bad .  \"', u\"We use the user's own rating ,  with a mapping that 4 and 5 are good and 1 , 2 ,  and 3 are bad .\", u'Our experience has shown that this reflects user behavior on MovieLens .', u'We found that one user had no movies rated below 4 ;  we eliminated that user from the statistics compiled for each experiment .', u'Evaluating the hypotheses in the face of multiple metrics can be a challenge .', u'We considered it important to consider both statistical and decision-support accuracy in evaluating different recommender systems .', u'When several agents ,  for example ,  provide different but incomparable trade-offs among the two metrics ,  we consider each one to be a possible  \" best agent \"  and compare each of them against the alternative recommender .', u'We consider one alternative to dominate another ,  however ,  if there is a significant improvement in one metric and no significant difference in the other .', u'Statistical significance is assessed for mean absolute errors using the Wilcoxan test on paired errors .', u'Differences reported as significant are based on a significance level of p<0 .', u'05 .', u'Statistical significance assessment for ROC sensitivity is less clear ; * from experience we therefore assert that changes of 0 .', u'01 or more are  \" meaningful \"  and smaller differences are  \" not meaningful .  \"', u'User Opinions Only .', u'Extensive  research  has  already been performed on the problem of generating recommendations from a set of user opinions .', u'Nearest- neighbor collaborative filtering is already generally accepted to be the most effective mechanism for performing this task ,  and we therefore use it  .', u'In particular ,  we use the DBLens research collaborative filtering engine developed by the GroupLens Research project for exploration of collaborative filtering algorithms .', u'DBLens allows experimenters to control several parameters that trade among performance ,  coverage ,  and accuracy .', u'For our experiments ,  we set each of these to prefer maximum coverage and to use all data regardless of performance .', u'The CF result set was computed for each user by loading the correlation data set  into the engine ,  then loading the test set  for each user ,  and requesting a prediction for each test set\\r\\nWe modified traditional TFIDF by counting each keyword as either occurring  or not occurring  in any given movie .', u'Accordingly ,  the TF vector for a movie is produced by inserting a 1 for each keyword and 0 elsewhere .', u'Building the user profile requires a balanced set of user ratings ,  so we subtract 3 from each rating to transform them to a -2 to +2 scale .', u\"For each movie in the 50-rating training set ,  we produce a keyword preference vector that is the product of the transformed rating ,  the movie's TF vector ,  and the IDF vector .\", u'We then normalize the keyword preference vector to length 1 .', u\"The mean of the user's 50 keyword preference vectors is the user profile .\", u'The DGBot produces ratings for all movies at once .', u'For each  movie ,   it  computes  the  dot  product  of  the  user profile vector and the TF vector .', u'Those scores are then ranked and broken into rating levels with a distribution matching the MovieLens overall rating distribution .', u'The top 21% of movies received a rating of 5 ,  the next 34% a rating of 4 ,  the next 28% a 3 ,  the next 12% a 2 ,  and the bottom 5% a 1 .', u'While each user has a separate user profile vector and set of recommendations ,  the TF and IDF vectors could be re-used from user to user .', u'RipperBot was created using Ripper ,  an inductive logic program created by William Cohen  .', u'We found that Ripper performed best when trained on a set of data limited to genre identifiers and the 200 most frequent keywords .', u'Ripper also works best when asked to make binary decisions ,  so for each user we trained four Ripper instances ,  tuned to distinguish between 5 / 4321 ,  54 / 321 ,  543 / 21 ,  and 5432 / 1 respectively .', u'Each instance was trained on the 50-rating training set along with the identifiers and keywords for those 50 movies .', u'After training ,  we asked each instance to classify the entire set of movies and summed the number of Ripper instances that indicated the higher value and added one to create a recommendation value .', u'Ripper requires substantial tuning ;  we experimented with several parameters and also relied on advice from  .', u'In particular ,  we adjusted default settings to allow negative tests in set value attributes and experimented by varying the loss ratio .', u'We found a loss ration of 1 .', u'9 to give us the best results .', u\"The GenreBots consisted of 19 simple bots  that  rated each movie a 5 if the movie matched the bot's genre and a\\r\\n3 otherwise .\", u\"For example ,  Toy Story ,  which is a children's animated comedy would receive a 5 from the ChildrensBot ,  the AnimatedBot and the ComedyBot ,  and a 3 from each of the remaining bots .\", u'Genre data was obtained from IMDB .', u'A Mega-GenreBot was created for each user .', u\"This was done by using linear regression and training the bot on each user's training set .\", u\"A user's known rating  was treated as a dependent variable of the 19 individual GenreBots .\", u'The regression coefficients formed  an equation that could then be used to generate predictions for each other movie from the genre identifiers .', u'Combinations of IF Agents .', u'We identified four different strategies for combining agents :  selecting one agent for each person ,  averaging the agents together ,  using regression to create a personal combination ,  and using CF to create a personal combination .', u'For all but the first of these ,  we found it valuable to create two combinations :  one that used all 19 GenreBots and one that used the Mega-GenreBot .', u'Adding the 3 DGBots and RipperBot ,  we refer to these as 23-agent and 5-agent versions ,  respectively .', u'BestBot .', u'The best agent per user was selected by testing each bot on the correlation data set  and selecting the bot with the lowest MAE .', u'BestBot then used the ratings generated by that bot for the test data set to produce statistics for evaluation .', u'Regression .', u'We used linear regression to produce a  \" best fit \"  combination for a given user .', u\"To do this we used the predictions on the correlation sets for the 23 and 5 agents respectively as the independent variables and the known user's rating as the dependent variable .\", u\"Using the resultant weights ,  we could generate predictions for the movies in the test sets by creating linear combinations of the agents' recommendations .\", u'CF Combination .', u'We used the DBLens CF engine to create a CF combination of agents .', u\"For this purpose ,  we loaded all ratings from the 5 or 23 agents into the engine ,  along with the user's 50 ratings from the correlation set .\", u\"We generated predictions for the user's 20 test movies .\", u'The ratings database was cleared after each user .', u'The parameters used were the same as for the simple CF case .', u'Combination of Users and IF Agents .', u'Because user ratings were incomplete ,  and because CF with 23 agents proved to be the most effective combination of IF agents ,  we used CF to combine the 23 agents and all 50 users .', u'The method is identical to the CF combination of agents except that we also loaded the ratings for the other 49 users .', u'Again ,  the database was cleared after each user .', u'8H1 :   Collaborative Filtering better than Single Agents  We  hypothesized  that  collaborative  filtering  using  the opinions of the 50-user community would provide better results than any individual agent .', u'To compare these ,  we first identified the best individual agent .', u'We evaluated the   three   DGBots ,     RipperBot ,    the   19   individual genreBots ,   and  the  personalized  Mega-GenreBot   .', u'Of these ,  only RipperBot ,  Mega-GenreBot ,  and the DGBot that used both cast and keywords were not dominated by other agents .', u'RipperBot had the highest accuracy  ( lowest MAE )  by far ,  but low ROC sensitivity  .', u'We hypothesized that the combination of the opinions of a community of users and the personalized agents for a given user will provide that user with better results than either users alone or agents alone .', u\"From both H2 and H3 we found that collaborative filtering of a single user and that user's 23 agents provides the best accuracy and decision support of all agent-only or user-only methods tested .\", u'Table 4 shows a small ,  but statistically significant improvement in accuracy resulting from including the other users in the collaborative filtering mix .', u'ROC also improves ,  but not by a meaningful amount .', u'We  hypothesized  that  combining  several  agents  would\\r\\nyield better results than any single personalized agent .', u'In testing H1 ,  we found that for single agents ,  RipperBot had the best accuracy value  ( MAE )  ,  DGBot Combo had the best decision support value  data sets ,  suggesting that it retains its value as a useful combination tool whenever human or agents are unlikely to rate each item .', u\"We were surprised by several of the results that  we'd found ,  and sought to explain them .\", u'Foremost ,  we clearly overestimated the value of collaborative filtering for a small community of 50 users .', u'In retrospect ,  our expectations may have been built from our own positive experiences when starting CF systems with a small group of researchers and friends .', u'Those successes may have been due in part to close ties among the users ;  we often had seen the same movies and many had similar tastes .', u'Using real users resulted in real diversity which may explain the lower ,  and more realistic ,  value .', u'Future work should both incorporate larger user sets  and look explicitly at closer-knit communities to see whether a smaller but more homogeneous community would have greater benefits from collaborative filtering .', u'We also were surprised by the results we achieved using Ripper .', u'We were impressed by its accuracy ,  after extensive tuning ,  but dismayed by how close to random it was in distinguishing good from bad movies .', u'We are still uncertain as to why RipperBot performs as it does ,  and believe further work is needed to understand why it behaves as it does and whether it would be possible to train it to perform differently .', u'In the future ,  we plan to examine further combinations of users and agents in recommender systems .', u'In particular ,  we are interested in developing a combined community where large numbers of users and agents co-exist .', u\"One question we hope to answer is whether users who agree with each other would also benefit from the opinions of each other's trained agents . \\r\\n\"]\n",
      "downloading stanford pos tagger:\n",
      "0.000755\n",
      "0.000873\n",
      "actual tagging:\n",
      "4.707954\n",
      "remove short stubs:\n",
      "0.010996\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(u\"1_g.txt\", 'r', 'utf-8') as fin:\n",
    "        text = fin.read()\n",
    "paragraphs = PreProcess(text)\n",
    "paragraphs = [paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SourceText:\n",
    "    def __init__(self, paragraphs):\n",
    "        self._paragraphs = [x for x in paragraphs if x != []]\n",
    "        self._par_iter = 0\n",
    "        self._sent_iter = 0\n",
    "        \n",
    "    def nextPar(self):\n",
    "        self._par_iter += 1\n",
    "        if self._par_iter == len(self._paragraphs):\n",
    "            return 1\n",
    "        else:\n",
    "            self._sent_iter = 0\n",
    "            return 0\n",
    "        \n",
    "    def nextSent(self):\n",
    "        if self._sent_iter < len(self._paragraphs[self._par_iter]):\n",
    "            self._sent_iter += 1\n",
    "            return self._paragraphs[self._par_iter][self._sent_iter - 1], 0\n",
    "        else:\n",
    "            return [], 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OffsetTopicsAndStresses:\n",
    "    def __init__(self):\n",
    "        self.topic_strong_words = [[], [], []]\n",
    "        self.topic_weak_words = [[], [], []]\n",
    "        self.stress_strong_words = [[], [], []]\n",
    "        self.stress_weak_words = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NOT_APPLICABLE = 0\n",
    "UNKNOWN = 1\n",
    "FLUID = 2\n",
    "INVERTED_TOPIC_CANDIDATE = 3\n",
    "OUT_OF_SYNC = 4\n",
    "INVERTED_TOPIC = 5\n",
    "DISCONNECTED = 6\n",
    "\n",
    "class SentInfo:\n",
    "    _fluid_words = [u\"admittedly\", u\"all in all\", u\"as a result\", u\"because\", u\"conversely\", u\"equally\", u\"finally\",\\\n",
    "               u\"for example\", u\"in a similar\", u\"in contrast\", u\"in summary\", u\"initially\", u\"last\", u\"nevertheless\",\\\n",
    "               u\"once\", u\"so far\", u\"such\", u\"after\", u\"along these lines\", u\"as expected\", u\"before\", u\"curiously\",\\\n",
    "               u\"even though\", u\"first\", u\"for instance\", u\"in a way\", u\"in other words\", u\"in the first\",\\\n",
    "               u\"interestingly\", u\"lastly\", u\"next\", u\"regardless\", u\"specifically\", u\"surprisingly\", u\"afterward\",\\\n",
    "               u\"although\", u\"as soon as\", u\"but\", u\"despite\", u\"eventually\", u\"firstly\", u\"for this reason\",\\\n",
    "               u\"in comparison\", u\"in particular\", u\"in the same way\", u\"it follows\", u\"likewise\", u\"nonetheless\",\\\n",
    "               u\"similarly\", u\"still\", u\"that is why\", u\"again\", u\"as a consequence\", u\"be that as it may\",\\\n",
    "               u\"consequently\", u\"during\", u\"figure\", u\"following\", u\"in a certain sense\", u\"in conclusion\", u\"in short\",\\\n",
    "               u\"indeed\", u\"it is as if\", u\"meanwhile\", u\"now\", u\"so\", u\"subsequently\", u\"the first\", u\"the last\",\\\n",
    "               u\"this\", u\"to elaborate\", u\"the next\", u\"this is why\", u\"to explain\", u\"the reason\", u\"thus\",\\\n",
    "               u\"to illustrate\", u\"then\", u\"to conclude\", u\"to put it another way\", u\"to put it succinctly\",\\\n",
    "               u\"unexpectedly\", u\"while\", u\"to sum up\", u\"until\", u\"while\", u\"to summarize\", u\"up to now\", u\"yet\",\\\n",
    "               u\"ultimately\", u\"whereas\"]\n",
    "    _parser = stanford.StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "    def __init__(self, sentence, type):\n",
    "        self.sent = sentence\n",
    "        self.type = type\n",
    "        self.topic_strong_words = []\n",
    "        self.topic_weak_words = []\n",
    "        self.stress_strong_words = []\n",
    "        self.stress_weak_words = []\n",
    "        self._begins_fluid_words = -1\n",
    "        self.current_offset = 0\n",
    "        #0 ~ Sn-1; 1 ~ Sn-2; 2 ~ Sn-3 \n",
    "        self.offset_wordset = OffsetTopicsAndStresses()\n",
    "    \n",
    "    def _deleteNonMainClauses(self, tree):\n",
    "        idxs = tree.treepositions()\n",
    "        for s in list(tree.subtrees(lambda t: t.label() == u\"SBAR\" or t.label() == u\"SBARQ\")):\n",
    "            idx = -1\n",
    "            for x in idxs:\n",
    "                if s == tree[x]:\n",
    "                    idx = x\n",
    "                    break\n",
    "            if idx != -1:\n",
    "                del tree[idx]\n",
    "                idxs = tree.treepositions()\n",
    "        return tree\n",
    "    \n",
    "    def _recursive_search(self, tree, subj_list):\n",
    "        if tree.height() == 2 or (tree.height() == 3 and len(tree.leaves()) == 1):\n",
    "            subj_list.extend(tree.leaves())\n",
    "        else:\n",
    "            for np_idx in range(len(tree)):\n",
    "                l = tree[np_idx].label()\n",
    "                target_tags = [u\"NP\", u\"NN\", u\"NNS\", u\"PRP\", u\"CD\"]\n",
    "                if  l in target_tags:\n",
    "                    self._recursive_search(tree[np_idx], subj_list)    \n",
    "    \n",
    "    def _findSubjects(self, tree):\n",
    "        res = []\n",
    "        for s in list(tree.subtrees(lambda t: t.label() == u\"S\")):\n",
    "            child_labels_list = []\n",
    "            for i in range(len(s)):\n",
    "                child_labels_list.append(s[i].label())\n",
    "            if u\"NP\" in child_labels_list and u\"VP\" in child_labels_list:\n",
    "                for np_idx in range(len(s)):\n",
    "                    if s[np_idx].label() == u\"NP\":\n",
    "                        self._recursive_search(s[np_idx], res)\n",
    "        return res\n",
    "    \n",
    "    def _appearsBeforeFirstPunct(self, idx):\n",
    "        punctuation_marks = [tuple([u\",\", u\",\"]), tuple([u\":\", u\":\"]), tuple([u\";\", u\";\"]), tuple([r'\"', r'``'])]\n",
    "        if ([x for x in punctuation_marks if x in self.sent[:idx]] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _appearsAfterLastPunctOrConj(self, idx):\n",
    "        punctuation_marks = [tuple([u\",\", u\",\"]), tuple([u\":\", u\":\"]), tuple([u\";\", u\";\"]), tuple([r'\"', r'``'])]\n",
    "        conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "        if ([x for x in punctuation_marks if x in self.sent[idx:]] == []) or\\\n",
    "        ([x for x in self.sent[idx:] if x[1] in conj_tags] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isMainClauseContainsTopic(self, main_clause_words):\n",
    "        topic = list(self.topic_strong_words)\n",
    "        topic.extend(self.topic_weak_words)\n",
    "        if ([x for x in topic if not x in main_clause_words] == []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isNumber(self, word):\n",
    "        numbers = [\"one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen,\\\n",
    "        fifteen, sixteen, seventeen, eighteen, nineteen, twenty, thirty, fourty, fifty, sixty, seventy, eighty, ninety,\\\n",
    "        hundred, thousand, million, billion\"]\n",
    "        if (word in numbers) or (re.match(r\"^[-+]?[0-9]+$\", word) != None):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _appearsAfterConj(self, idx):\n",
    "        conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "        if ([x for x in self.sent[:idx] if x[1] in conj_tags] != []):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _isStrongStress(self, stress_word, main_clause_words):\n",
    "        noun_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\"]\n",
    "        if stress_word[1] in noun_tags:\n",
    "            idx = self.sent.index(stress_word)\n",
    "            if self._appearsBeforeFirstPunct(idx) or self._appearsAfterLastPunctOrConj(idx):\n",
    "                return True\n",
    "            if (stress_word in main_clause_words) and self._isMainClauseContainsTopic(main_clause_words) and\\\n",
    "            self._appearsAfterConj(idx):\n",
    "                return True\n",
    "            if self._isNumber(self.sent[idx - 1][0]):\n",
    "                return True\n",
    "        #verb derived; stress word in main clause\n",
    "        elif stress_word[0] in main_clause_words:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _addStressWords(self, stress_words, main_clause_words):\n",
    "        for stress_word in stress_words:\n",
    "            if self._isStrongStress(stress_word, main_clause_words):\n",
    "                self.stress_strong_words.append(stress_word[0])\n",
    "            else:\n",
    "                self.stress_weak_words.append(stress_word[0])\n",
    "    \n",
    "    def setDefaultWordSet(self):\n",
    "        self.topic_weak_words = []\n",
    "        self.topic_strong_words = []\n",
    "        self.stress_weak_words = []\n",
    "        self.stress_strong_words = []\n",
    "        tree = next(self._parser.tagged_parse(self.sent))            \n",
    "        #display(tree)\n",
    "        \n",
    "        #Добавляем все слова с метками NN, NNS, NNP, NNPS\n",
    "        #Отличить verb derived nouns в VBG FIX\n",
    "        target_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\", u\"VBG\"]\n",
    "        nounsAndVerbDerivedNouns = []\n",
    "        for tup in self.sent:\n",
    "            if tup[1] in target_tags:\n",
    "                nounsAndVerbDerivedNouns.append(tup)\n",
    "        #print \"nounsAndVerbDerivedNouns: \", nounsAndVerbDerivedNouns, \"\\n\"\n",
    "        \n",
    "        #Удаляем non main clauses\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #Ищем subjects\n",
    "        self.topic_strong_words = self._findSubjects(tree)\n",
    "        stress_words = [x for x in nounsAndVerbDerivedNouns if x[0] not in self.topic_strong_words]\n",
    "        self._addStressWords(stress_words, tree.leaves())\n",
    "        #print \"topic_strong_words(main clause subj): \", self.topic_strong_words, \"\\n\"\n",
    "        #print \"strong stress: \", self.stress_strong_words, \"\\n\"\n",
    "        #print \"weak stress: \", self.stress_weak_words, \"\\n\"\n",
    "        #print \"current sentence: \", self.sent, \"\\n\"\n",
    "    \n",
    "    def addStressWords(self, stress_words, offset):\n",
    "        tree = next(self._parser.tagged_parse(self.sent)) \n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #print \"stress_words:\"\n",
    "        #print stress_words\n",
    "        for stress_word in stress_words:\n",
    "            if self._isStrongStress(stress_word, tree.leaves()):\n",
    "                self.offset_wordset.stress_strong_words[offset - 1].append(stress_word[0])\n",
    "            else:\n",
    "                self.offset_wordset.stress_weak_words[offset - 1].append(stress_word[0])\n",
    "    \n",
    "    def getMainClauseSubjects(self):\n",
    "        tree = next(self._parser.tagged_parse(self.sent))\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        #display(tree)\n",
    "        return self._findSubjects(tree) \n",
    "    \n",
    "    def getMainClauseWords(self):\n",
    "        tree = next(self._parser.tagged_parse(self.sent))\n",
    "        tree = self._deleteNonMainClauses(tree)\n",
    "        return tree.leaves()\n",
    "        \n",
    "    def beginsWithFluidWords(self):\n",
    "        if self._begins_fluid_words != -1:\n",
    "            return self._begins_fluid_words\n",
    "        verb_tags = [u\"VB\", u\"VBD\", u\"VBN\", u\"VBP\", u\"VBZ\"]\n",
    "        ordinal_numbers = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\",\\\n",
    "                           \"tenth\", \"eleventh\", \"twelfth\", \"thirteenth\", \"fourteenth\", \"fifteenth\", \"sixteenth\",\\\n",
    "                           \"seventeenth\", \"eighteenth\", \"nineteenth\", \"twentieth\", \"thirtieth\", \"fortieth\", \"fiftieth\",\\\n",
    "                           \"sixtieth\", \"seventieth\", \"eightieth\", \"ninetieth\", \"hundredth\", \"thousandth\"]\n",
    "        pronouns_tags = [u\"PRP$\", u\"PRP\"]\n",
    "        patt = re.compile(r\"([A-Z]+\\))|(\\([A-Z]+\\))|([1-9]+\\))|(\\([1-9]+\\))|([1-9]*1st)|([1-9]*2nd)|([1-9]*3rd)|([1-9]*[4-9]th)|([1-9]+0th)\")\n",
    "        for word in self.sent:\n",
    "            if word[1] in verb_tags:\n",
    "                break\n",
    "            #FIX некоторые FLUID WORDS состоят из двух и более слов, а я рассматриваю только по одному\n",
    "            if (word[0].lower() in self._fluid_words) or (word[1] in pronouns_tags) or\\\n",
    "            (re.search(patt, word[0]) != None) or (word[0].lower() in ordinal_numbers):\n",
    "                self.beginsWithFluidWords = 1\n",
    "            else:\n",
    "                self.beginsWithFluidWords = 0\n",
    "        return self.beginsWithFluidWords\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#проверка алгоритма(поиск defaultWordSet для каждого предложения)\n",
    "# state = 0\n",
    "# text = SourceText(paragraphs)\n",
    "# sent_list = []\n",
    "\n",
    "# while state == 0:\n",
    "#     print state\n",
    "#     sent, p = text.nextSent()\n",
    "#     while p != 1:\n",
    "#         sent_list.append(SentInfo(sent, UNKNOWN))\n",
    "#         sent_list[-1].setDefaultWordSet()\n",
    "#         sent, p = text.nextSent()\n",
    "#     state = text.nextPar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STRONG_TOPIC = 10\n",
    "WEAK_TOPIC = 11\n",
    "\n",
    "def BetweenFluidOrInverted(sent_list, offset):\n",
    "    #check Sn-1\n",
    "    if sent_list[-2].type != FLUID and sent_list[-2].type != INVERTED_TOPIC_CANDIDATE:\n",
    "        return False\n",
    "    #check Sn-2\n",
    "    if sent_list[-3].type != FLUID and sent_list[-3].type != INVERTED_TOPIC_CANDIDATE:\n",
    "        return False\n",
    "    if offset == 3:\n",
    "        if sent_list[-4].type != FLUID and sent_list[-4].type != INVERTED_TOPIC_CANDIDATE:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def TopicFound(topic_words, topic_type, sentence, previous_sentence, reached_verb, offset, sent_list):\n",
    "#     print \"trying to find topic!\", topic_words, topic_type\n",
    "    if offset == 1:\n",
    "        if not reached_verb:\n",
    "            sentence.type = FLUID\n",
    "        else:\n",
    "            sentence.type = INVERTED_TOPIC_CANDIDATE\n",
    "    else:\n",
    "        if not reached_verb:\n",
    "            if BetweenFluidOrInverted(sent_list, offset):\n",
    "                sentence.type = FLUID\n",
    "            else:\n",
    "                sentence.type = OUT_OF_SYNC\n",
    "    if topic_type == WEAK_TOPIC:\n",
    "        if not set(topic_words).issubset(set(sentence.offset_wordset.topic_strong_words[offset - 1])):\n",
    "            sentence.offset_wordset.topic_weak_words[offset - 1].extend(topic_words)\n",
    "    elif topic_type == STRONG_TOPIC:\n",
    "        sentence.offset_wordset.topic_strong_words[offset - 1].extend(topic_words)\n",
    "                 \n",
    "def CheckSentenceMainClauses(sentence, previous_sentence, sent_list):\n",
    "    offset = sentence.current_offset\n",
    "    main_clause_subjects = sentence.getMainClauseSubjects()\n",
    "    prev_sent_wordset = list(previous_sentence.topic_weak_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.topic_strong_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.stress_strong_words)\n",
    "#     print \"prev_sent_wordset: \", prev_sent_wordset\n",
    "#     print \"main clause subjects: \", main_clause_subjects\n",
    "    matched_words = [x for x in main_clause_subjects if x in prev_sent_wordset]\n",
    "#     print \"found strong topic! Give me sec to check: \", matched_words\n",
    "    if matched_words != []:\n",
    "        TopicFound(matched_words, STRONG_TOPIC, sentence, previous_sentence, False, offset, sent_list)\n",
    "    #FIX какие именно subjects имеются в виду?\n",
    "    stress_words = [x for x in main_clause_subjects if x not in matched_words]\n",
    "    sentence.addStressWords([(x, u\"\") for x in stress_words], offset)\n",
    "    \n",
    "def CheckWholeSentence(sentence, previous_sentence, sent_list):\n",
    "    offset = sentence.current_offset\n",
    "    reached_verb = False\n",
    "    reached_topic_or_main = False\n",
    "    prev_sent_wordset = list(previous_sentence.topic_weak_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.topic_strong_words)\n",
    "    prev_sent_wordset.extend(previous_sentence.stress_strong_words)\n",
    "    main_clause_words = sentence.getMainClauseWords()\n",
    "#     print \"prev_sent_wordset: \", prev_sent_wordset\n",
    "#     print \"main clause words: \", main_clause_words\n",
    "    conj_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    noun_tags = [u\"NN\", u\"NNP\", u\"NNS\", u\"NNPS\"]\n",
    "    \n",
    "    for word in sentence.sent:\n",
    "        reached_verb = word[1] in conj_tags or reached_verb\n",
    "        reached_topic_or_main = word[0] in main_clause_words or reached_topic_or_main\n",
    "#         print word, reached_verb, reached_topic_or_main\n",
    "        if not reached_verb:\n",
    "            matches = word[0] in prev_sent_wordset\n",
    "            if (matches and word[1] in noun_tags) or (matches and reached_topic_or_main and (word[1] == u\"VBG\")):\n",
    "                TopicFound([word[0]], WEAK_TOPIC, sentence, previous_sentence, False, offset, sent_list)\n",
    "            elif (word[1] == u\"VBG\"):\n",
    "                sentence.addStressWords([word], offset)\n",
    "        else:\n",
    "            if (sentence.offset_wordset.topic_strong_words[offset - 1] != []) or (sentence.offset_wordset.topic_weak_words[offset - 1] != []):\n",
    "                if word[1] in noun_tags:\n",
    "                    sentence.addStressWords([word], offset)\n",
    "            else:\n",
    "                matches = word[0] in prev_sent_wordset\n",
    "                if (word[1] in noun_tags) and matches:\n",
    "#                     print \"special for recognition!!!\", [word[0]]\n",
    "                    TopicFound([word[0]], WEAK_TOPIC, sentence, previous_sentence, True, offset, sent_list)\n",
    "                    reached_topic_or_main = True\n",
    "                elif (word[1] == u\"VBG\") and matches:\n",
    "                    sentence.addStressWords([word], offset)\n",
    "                    \n",
    "def CheckSentenceProgression(sentence, previous_sentence, sent_list):\n",
    "    sentence.current_offset += 1\n",
    "    CheckSentenceMainClauses(sentence, previous_sentence, sent_list)\n",
    "    CheckWholeSentence(sentence, previous_sentence, sent_list)\n",
    "    \n",
    "def DefineResults(sentence, total_amount):\n",
    "    if sentence.type == UNKNOWN:\n",
    "        sentence.type = DISCONNECTED\n",
    "        sentence.setDefaultWordSet()\n",
    "    else:\n",
    "        if sentence.type == INVERTED_TOPIC:\n",
    "            word_set_from_round = 1\n",
    "        elif sentence.type in [FLUID, OUT_OF_SYNC]:\n",
    "            word_set_from_round = sentence.current_offset\n",
    "        sentence.topic_strong_words = sentence.offset_wordset.topic_strong_words[word_set_from_round - 1]\n",
    "        sentence.topic_weak_words = sentence.offset_wordset.topic_weak_words[word_set_from_round - 1]\n",
    "        sentence.stress_strong_words = sentence.offset_wordset.stress_strong_words[word_set_from_round - 1]\n",
    "        sentence.stress_weak_words = sentence.offset_wordset.stress_weak_words[word_set_from_round - 1]\n",
    "    if sentence.type == FLUID:\n",
    "        total_amount[0] += 1\n",
    "    elif sentence.type == INVERTED_TOPIC:\n",
    "        total_amount[1] += 1\n",
    "    elif sentence.type == OUT_OF_SYNC:\n",
    "        total_amount[2] +=1\n",
    "    elif sentence.type == DISCONNECTED:\n",
    "        total_amount[3] += 1\n",
    "        \n",
    "def TypeToString(type):\n",
    "    if type == FLUID:\n",
    "        return \"Fluid\"\n",
    "    elif type == INVERTED_TOPIC:\n",
    "        return \"Inverted topic\"\n",
    "    elif type == OUT_OF_SYNC:\n",
    "        return \"Out if sync\"\n",
    "    elif type == DISCONNECTED:\n",
    "        return \"Disconnected\"\n",
    "    elif type == NOT_APPLICABLE:\n",
    "        return \"Not applicable\"\n",
    "    \n",
    "def PrintSentInfo(sent):\n",
    "    print \"--------------------------------------------------------------------------------------------------------\"\n",
    "    print \"\\nSentence: \", sent.sent\n",
    "    print \"Topic:\"\n",
    "    print \"Strong: \", sent.topic_strong_words\n",
    "    print \"Weak  : \", sent.topic_weak_words\n",
    "    print \"Stress:\"\n",
    "    print \"Strong: \", sent.stress_strong_words\n",
    "    print \"Weak  : \", sent.stress_weak_words\n",
    "    print \"\\nType: \", TypeToString(sent_list[-1].type)\n",
    "    print \"\\nWordsets from checkups with Sn-1, Sn-2, Sn-3:\"\n",
    "    print \"Topic:\"\n",
    "    print \"Strong: \", sent.offset_wordset.topic_strong_words\n",
    "    print \"Weak: \", sent.offset_wordset.topic_weak_words\n",
    "    print \"Stress:\"\n",
    "    print \"Strong: \", sent.offset_wordset.stress_strong_words\n",
    "    print \"Weak: \", sent.offset_wordset.stress_weak_words\n",
    "    print \"--------------------------------------------------------------------------------------------------------\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next Paragraph:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2a8a0f650526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msent_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOT_APPLICABLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msent_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetDefaultWordSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#PrintSentInfo(sent_list[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextSent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-4525dfc4e1fa>\u001b[0m in \u001b[0;36msetDefaultWordSet\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstress_weak_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstress_strong_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m#display(tree)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36mtagged_parse\u001b[0;34m(self, sentence, verbose)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_parse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtagged_parse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36mtagged_parse_sents\u001b[0;34m(self, sentences, verbose)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# We don't need to escape slashes as \"splitting is done on the last instance of the character in the token\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         return self._parse_trees_output(self._execute(\n\u001b[0;32m--> 187\u001b[0;31m             cmd, '\\n'.join(' '.join(tag_separator.join(tagged) for tagged in sentence) for sentence in sentences), verbose))\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, cmd, input_, verbose)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 stdout, stderr = java(cmd, classpath=self._classpath,\n\u001b[0;32m--> 216\u001b[0;31m                                       stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'\\xc2\\xa0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mb' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Check the return code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_poll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate_with_poll\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mfd2file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_amount = [0, 0, 0, 0] #fluid, inverted_topic, out_of_sync, disconnected\n",
    "state = 0\n",
    "text = SourceText(paragraphs)\n",
    "while state == 0:\n",
    "    print \"\\nNext Paragraph:\"\n",
    "    sent_list = []\n",
    "    sent, p = text.nextSent()\n",
    "    if sent == []:\n",
    "        text.nextPar()\n",
    "        continue\n",
    "    sent_list.append(SentInfo(sent, NOT_APPLICABLE))\n",
    "    sent_list[-1].setDefaultWordSet()\n",
    "    #PrintSentInfo(sent_list[-1])\n",
    "    sent, p = text.nextSent()\n",
    "    while p == 0:\n",
    "        sent_list.append(SentInfo(sent, UNKNOWN))\n",
    "        if sent_list[-1].beginsWithFluidWords():\n",
    "            sent_list[-1].type = FLUID\n",
    "            print \"setDefaultWordSet:\"\n",
    "            time_beg = datetime.datetime.now()\n",
    "            sent_list[-1].setDefaultWordSet()\n",
    "            print (datetime.datetime.now() - time_beg).total_seconds()\n",
    "        else:\n",
    "            CheckSentenceProgression(sent_list[-1], sent_list[-2], sent_list)\n",
    "            if sent_list[-1].type == INVERTED_TOPIC_CANDIDATE:\n",
    "                sent_list[-1].type = INVERTED_TOPIC\n",
    "            elif sent_list[-1].type == UNKNOWN:\n",
    "                if len(sent_list) > 2:\n",
    "                    for offset in range(2):\n",
    "                        CheckSentenceProgression(sent_list[-1], sent_list[-2], sent_list)\n",
    "                        if sent_list[-1].type != UNKNOWN or len(sent_list) < 4:\n",
    "                            break\n",
    "            DefineResults(sent_list[-1], total_amount)\n",
    "        #PrintSentInfo(sent_list[-1])\n",
    "        sent, p = text.nextSent()\n",
    "    state = text.nextPar()\n",
    "    print \"############################################################################################################\"\n",
    "print \"Total amount:\"\n",
    "print \"Fluid sentences: \", total_amount[0]\n",
    "print \"Inverted topic sentences: \", total_amount[1]\n",
    "print \"Out of sync sentences: \", total_amount[2]\n",
    "print \"Disconnected sentences: \", total_amount[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print sent_list[-1].offset_wordset.topic_strong_words\n",
    "# print sent_list[-1].offset_wordset.topic_weak_words\n",
    "# print sent_list[-1].offset_wordset.stress_strong_words\n",
    "# print sent_list[-1].offset_wordset.stress_weak_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
